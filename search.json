[
  {
    "objectID": "tutorials-templates/06_S6_OPeNDAP_Access_Gridding_template.html",
    "href": "tutorials-templates/06_S6_OPeNDAP_Access_Gridding_template.html",
    "title": "06. Sentinel-6 MF L2 Altimetry Data Access (OPeNDAP) & Gridding",
    "section": "",
    "text": "In this tutorial you will learn…\n\nabout level 2 radar altimetry data from the Sentinel-6 Michael Freilich mission;\nhow to efficiently download variable subsets using OPeNDAP;\nhow to grid the along-track altimetry observations produced by S6 at level 2.;\n\n\nAbout Ocean Surface Topography (OST)\n\nThe primary contribution of satellite altimetry to satellite oceanography has been to:\n\nImprove the knowledge of ocean tides and develop global tide models.\nMonitor the variation of global mean sea level and its relationship to changes in ocean mass and heat content.\nMap the general circulation variability of the ocean, including the ocean mesoscale, over decades and in near real-time using multi-satellite altimetric sampling.\n\n\n\nAbout Sentinel-6 MF\nhttps://search.earthdata.nasa.gov/search?fpj=Sentinel-6\n\nhttps://podaac.jpl.nasa.gov/Sentinel-6\n\nMission Characteristics\nSemi-major axis: 7,714.43 km\nEccentricity: 0.000095\nInclination: 66.04°\nArgument of periapsis: 90.0°\nMean anomaly: 253.13°\nReference altitude: 1,336 km\nNodal period: 6,745.72 sec\nRepeat period: 9.9156 days\nNumber of revolutions within a cycle: 127\nNumber of passes within a cycle: 254\nEquatorial cross track separation: 315 km\nGround track control band: +1 km\nAcute angle at Equator crossings: 39.5°\nGround track speed: 5.8 km/s\n\n\n\nRequirements\nThis workflow was developed using Python 3.9 (and tested against versions 3.7, 3.8).\n\nimport os\nimport tqdm\nimport numpy as np\nimport xarray as xr\nimport matplotlib.pyplot as plt\nfrom concurrent.futures import ThreadPoolExecutor\nfrom pyresample.kd_tree import resample_gauss\nimport pyresample as pr\n\n\n\nDataset\nhttps://podaac.jpl.nasa.gov/dataset/JASON_CS_S6A_L2_ALT_LR_RED_OST_NRT_F\nThis example operates on Level 2 Low Resolution Altimetry from Sentinel-6 Michael Freilich (the Near Real Time Reduced distribution). It is most easily identified by its collection ShortName, given below with the more cryptic concept-id, it’s unique identifier in the CMR.\n\nShortName = 'JASON_CS_S6A_L2_ALT_LR_RED_OST_NRT_F'\nconcept_id = 'C1968980576-POCLOUD'\n\n\n\nOPeNDAP\nhttps://opendap.github.io/documentation/UserGuideComprehensive.html#Constraint_Expressions (Hyrax/OPeNDAP docs)\nHow to get the list of opendap endpoints hard-coded in the following cell:\ncycle = 25\n\nurl = f\"https://cmr.earthdata.nasa.gov/search/granules.csv?ShortName={ShortName}&cycle={cycle}&page_size=200\"\n\n!curl --silent --output \"results.csv\" \"$url\"\n\nfiles = !cat results.csv | tail --lines=+2 | cut --delimiter=',' --field=5 | cut --delimiter='/' --field=6\n\nvariables = ['data_01_time',\n             'data_01_longitude',\n             'data_01_latitude',\n             'data_01_ku_ssha']\n\nv = \",\".join(variables)\n\nfor f in files:\n    print(f\"https://opendap.earthdata.nasa.gov/collections/{concept_id}/granules/{f}4?{v}\")\n\n%%file subsets.txt\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_001_20210713T162644_20210713T182234_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_003_20210713T182234_20210713T201839_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_006_20210713T201839_20210713T215450_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_007_20210713T215450_20210713T234732_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_009_20210713T234732_20210714T014224_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_011_20210714T014224_20210714T033812_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_013_20210714T033812_20210714T053356_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_015_20210714T053357_20210714T072934_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_017_20210714T072934_20210714T090919_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_019_20210714T090919_20210714T110146_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_021_20210714T110146_20210714T125702_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_023_20210714T125702_20210714T145316_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_025_20210714T145317_20210714T164922_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_027_20210714T164922_20210714T184510_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_029_20210714T184510_20210714T204143_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_032_20210714T204143_20210714T221611_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_033_20210714T221611_20210715T000941_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_035_20210715T000941_20210715T020456_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_037_20210715T020456_20210715T040047_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_039_20210715T040047_20210715T055630_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_041_20210715T055630_20210715T075208_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_043_20210715T075208_20210715T093037_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_045_20210715T093037_20210715T112356_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_047_20210715T112356_20210715T131944_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_049_20210715T131944_20210715T151600_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_051_20210715T151602_20210715T165851_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_053_20210715T171228_20210715T190748_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_056_20210715T190748_20210715T204627_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_057_20210715T204627_20210715T223758_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_059_20210715T223758_20210716T003159_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_061_20210716T003159_20210716T022732_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_063_20210716T022732_20210716T042333_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_065_20210716T042333_20210716T061901_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_067_20210716T061901_20210716T081446_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_070_20210716T081446_20210716T095203_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_071_20210716T095203_20210716T114624_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_073_20210716T114624_20210716T134228_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_075_20210716T134228_20210716T153841_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_077_20210716T153841_20210716T173433_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_079_20210716T173433_20210716T193033_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_082_20210716T193033_20210716T210718_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_083_20210716T210718_20210716T225942_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_085_20210716T225942_20210717T005425_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_087_20210717T005425_20210717T025012_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_089_20210717T025012_20210717T044557_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_091_20210717T044557_20210717T064133_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_093_20210717T064133_20210717T082134_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_095_20210717T082134_20210717T101352_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_097_20210717T101352_20210717T120859_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_099_20210717T120859_20210717T140513_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_101_20210717T140513_20210717T160120_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_103_20210717T160120_20210717T175708_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_105_20210717T175708_20210717T195329_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_108_20210717T195329_20210717T212832_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_109_20210717T212832_20210717T232147_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_111_20210717T232147_20210718T011655_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_113_20210718T011655_20210718T031245_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_115_20210718T031245_20210718T050829_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_117_20210718T050829_20210718T070406_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_119_20210718T070406_20210718T084306_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_121_20210718T084306_20210718T103559_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_123_20210718T103559_20210718T123140_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_125_20210718T123140_20210718T142756_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_127_20210718T142756_20210718T162356_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_129_20210718T162356_20210718T181945_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_132_20210718T181945_20210718T195907_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_133_20210718T195907_20210718T215014_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_135_20210718T215014_20210718T234402_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_137_20210718T234402_20210719T013937_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_139_20210719T013937_20210719T033531_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_141_20210719T033531_20210719T053101_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_143_20210719T053101_20210719T072643_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_146_20210719T072643_20210719T090425_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_147_20210719T090425_20210719T105824_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_149_20210719T105824_20210719T125424_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_151_20210719T125424_20210719T145038_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_153_20210719T145541_20210719T164632_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_155_20210719T164632_20210719T184227_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_158_20210719T184227_20210719T201949_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_159_20210719T201949_20210719T221154_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_161_20210719T221154_20210720T000626_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_163_20210720T000626_20210720T020212_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_165_20210720T020212_20210720T035756_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_167_20210720T035756_20210720T055333_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_169_20210720T055333_20210720T073350_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_171_20210720T073350_20210720T092602_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_173_20210720T092602_20210720T112057_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_175_20210720T112057_20210720T131708_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_177_20210720T131708_20210720T151317_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_184_20210720T190549_20210720T204056_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_185_20210720T204056_20210720T223355_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_187_20210720T223355_20210721T002855_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_189_20210721T002855_20210721T022443_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_191_20210721T022443_20210721T042029_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_193_20210721T042029_20210721T061605_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_195_20210721T061605_20210721T075531_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_197_20210721T075531_20210721T094805_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_199_20210721T094805_20210721T114336_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_201_20210721T114336_20210721T133952_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_203_20210721T133952_20210721T153555_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_205_20210721T153555_20210721T173143_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_207_20210721T173143_20210721T191151_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_209_20210721T191151_20210721T210223_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_211_20210721T210223_20210721T225607_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_213_20210721T225607_20210722T005131_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_215_20210722T005131_20210722T024724_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_217_20210722T024724_20210722T044301_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_219_20210722T044301_20210722T063841_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_221_20210722T063841_20210722T081646_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_223_20210722T081646_20210722T101025_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_225_20210722T101025_20210722T120619_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_227_20210722T120619_20210722T140235_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_229_20210722T140235_20210722T155831_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_231_20210722T155831_20210722T175423_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_234_20210722T175423_20210722T193222_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_235_20210722T193222_20210722T212406_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_237_20210722T212406_20210722T231828_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_239_20210722T231828_20210723T011405_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_241_20210723T011405_20210723T030955_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_243_20210723T030955_20210723T050533_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_245_20210723T050533_20210723T064603_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_247_20210723T064603_20210723T083817_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_249_20210723T083817_20210723T103256_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_251_20210723T103256_20210723T122904_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_253_20210723T122904_20210723T142514_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\n\nWriting subsets.txt\n\n\n\n\n\nDownload Subsets\nThese functions download one granule from the remote source to a local target, and will reliably manage simultaneous streaming downloads divided between multiple threads.\n\nwith python3:\nimport requests\n\ndef download(source: str, target: str):\n    with requests.get(source, stream=True) as remote, open(target, 'wb') as local:\n        if remote.status_code // 100 == 2: \n            for chunk in remote.iter_content(chunk_size=1024):\n                if chunk:\n                    local.write(chunk)\n\n\nwith wget:\nhttps://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor\nThe source files range from 2.5MB to 3.0MB. These OPeNDAP subsets are ~100KB apiece. (anecdote: it took less than 10 minutes to download subsets for &gt;1700 granules/files when I ran this routine for all cycles going back to 2021-06-22.)\nhttps://www.gnu.org/software/coreutils/manual/html_node/du-invocation.html\n\n\nAggregate cycle\nSort the list of local subsets to ensure they concatenate in proper order. Call open_mfdataset on the list to open all the subsets in memory as one dataset in xarray.\nhttps://xarray.pydata.org/en/stable/generated/xarray.open_mfdataset.html\nMake a dictionary to rename variables so that the data_01_ prefix is removed from each one.\nhttps://docs.python.org/3/library/functions.html#map\nhttps://docs.python.org/3/library/functions.html#zip\n\n\nPlot ssha variable\nhttps://xarray.pydata.org/en/stable/generated/xarray.Dataset.rename.html\n\n\n\nBorrow 0.5-Degree Grid and Mask from ECCO V4r4\n\nAcknowledgement: This approach using pyresample was shared to me by Ian Fenty, ECCO Lead.\n\nhttps://search.earthdata.nasa.gov/search/granules?p=C2013583732-POCLOUD\nECCO V4r4 products are distributed in two spatial formats. One set of collections provides the ocean state estimates on the native model grid (LLC0090) and the other provides them after interpolating to a regular grid defined in geographic coordinates with horizontal cell size of 0.5-degrees.\nIt’s distributed as its own dataset/collection containing just one file. We can simply download it from the HTTPS download endpoint – the file size is inconsequential. The next cell downloads the file into the data folder from the granule’s https endpoint.\nhttps://xarray.pydata.org/en/stable/generated/xarray.open_dataset.html\nSelect index 0 on the Z axis/dimension to get the depth layer at ocean surface.\nhttps://xarray.pydata.org/en/stable/generated/xarray.DataArray.isel.html\nThe maskC variable contains a boolean mask representing the wet/dry state of the area contained in each cell of the 3d grid defined by Z and latitude and longitude. Here are the variable’s attributes:\nPlot the land/water mask maskC:\nhttps://xarray.pydata.org/en/stable/generated/xarray.DataArray.plot.html\nDefine target grid based on the longitudes and latitudes from the ECCO grid geometry dataset. This time define the grid using two 2-dimensional arrays that give positions of all SSHA values in geographic/longitude-latitude coordinates.\nCreate the target swath definition from the 2d arrays of lons and lats from ECCO V4r4 0.5-degree grid.\npyresample.geometry.SwathDefinition\n\n\nGrid ssha or other variable\nGet one timestamp to represent the midpoint of the 10-day cycle.\nAccess the target variable, ssha in this case. Make a nan mask from the ssha variable.\nCreate the source swath definition from the 1d arrays of lons and lats from the S6 level-2 along-track altimetry time series.\npyresample.geometry.SwathDefinition\nResample ssha data using kd-tree gaussian weighting neighbour approach.\npyresample.kd_tree.resample_gauss\nApply the land/water mask in the numpy array created from the ECCO layer in the steps above. Then, convert the masked numpy array to an xarray data array object named gridded. Print its header.\n\n\nExercise\nCalculate area-weighted mean sea level.\n\n\nReferences\nnumpy (https://numpy.org/doc/stable/reference)\n\nnumpy.ndarray.data\n\nnumpy.where\n\nnumpy.isnan\n\ndatetimes\n\nnumpy.sum\n\nnumpy.nansum\n\nxarray (https://xarray.pydata.org/en/stable)\n\nxarray.DataArray\n\nxarray.DataArray.values\n\nxarray.DataArray.mean"
  },
  {
    "objectID": "tutorials-templates/01_Data_Discovery_CMR.html",
    "href": "tutorials-templates/01_Data_Discovery_CMR.html",
    "title": "01. Data discovery with CMR",
    "section": "",
    "text": "In this tutorial you will learn: - what CMR is;\n- how to use the requests package to search data collections and granules;\n- how to parse the results of these searches.\nWe will focus on datasets in the cloud. Currently, DAACs with data in the cloud are ‘ASF’, ‘GES_DISC’, ‘GHRC_DAAC’, ‘LPCLOUD’, ‘ORNL_CLOUD’, ‘POCLOUD’"
  },
  {
    "objectID": "tutorials-templates/01_Data_Discovery_CMR.html#what-is-cmr",
    "href": "tutorials-templates/01_Data_Discovery_CMR.html#what-is-cmr",
    "title": "01. Data discovery with CMR",
    "section": "What is CMR",
    "text": "What is CMR\nCMR is the Common Metadata Repository. It catalogs all data for NASA’s Earth Observing System Data and Information System (EOSDIS). It is the backend of Earthdata Search, the GUI search interface you are probably familiar with. More information about CMR can be found here.\nUnfortunately, the GUI for Earthdata Search is not accessible from a cloud instance - at least not without some work. Earthdata Search is also not immediately reproducible. What I mean by that is if you create a search using the GUI you would have to note the search criteria (date range, search area, collection name, etc), take a screenshot, copy the search url, or save the list of data granules returned by the search, in order to recreate the search. This information would have to be re-entered each time you or someone else wanted to do the search. You could make typos or other mistakes. A cleaner, reproducible solution is to search CMR programmatically using the CMR API."
  },
  {
    "objectID": "tutorials-templates/01_Data_Discovery_CMR.html#what-is-the-cmr-api",
    "href": "tutorials-templates/01_Data_Discovery_CMR.html#what-is-the-cmr-api",
    "title": "01. Data discovery with CMR",
    "section": "What is the CMR API",
    "text": "What is the CMR API\nAPI stands for Application Programming Interface. It allows applications (software, services, etc) to send information to each other. A helpful analogy is a waiter in a restaurant. The waiter takes your drink or food order that you select from the menu, often translated into short-hand, to the bar or kitchen, and then returns (hopefully) with what you ordered when it is ready.\nThe CMR API accepts search terms such as collection name, keywords, datetime range, and location, queries the CMR database and returns the results."
  },
  {
    "objectID": "tutorials-templates/01_Data_Discovery_CMR.html#how-to-search-cmr-from-python",
    "href": "tutorials-templates/01_Data_Discovery_CMR.html#how-to-search-cmr-from-python",
    "title": "01. Data discovery with CMR",
    "section": "How to search CMR from Python",
    "text": "How to search CMR from Python\nThe first step is to import python packages. We will use:\n- requests This package does most of the work for us accessing the CMR API using HTTP methods. - pprint to pretty print the results of the search.\nA more in depth tutorial on requests is here\n\n#\n\nThen we need to authenticate with EarthData Login. Since we’ve already set this up in the previous lesson, here you need to enter your username before executing the cell.\nTo conduct a search using the CMR API, requests needs the url for the root CMR search endpoint. We’ll build this url as a python variable.\n\n#\n\nCMR allows search by collections, which are datasets, and granules, which are files that contain data. Many of the same search parameters can be used for colections and granules but the type of results returned differ. Search parameters can be found in the API Documentation.\nWhether we search collections or granules is distinguished by adding \"collections\" or \"granules\" to the url for the root CMR endpoint.\nWe are going to search collections first, so we add collections to the url. I’m using a python format string here.\n\n#\n\nIn this first example, I want to retrieve a list of collections that are hosted in the cloud. Each collection has a cloud_hosted parameter that is either True if that collection is in the cloud and False if it is not. The migration of NASA data to the cloud is a work in progress. Not all collections tagged as cloud_hosted have granules. To search for only cloud_hosted datasets with granules, I also set has_granules to True.\nI also want to get the content in json (pronounced “jason”) format, so I pass a dictionary to the header keyword argument to say that I want results returned as json.\nThe .get() method is used to send this information to the CMR API. get() calls the HTTP method GET.\n\n#\n\nrequests returns a Response object.\nOften, we want to check that our request was successful. In a notebook or someother interactive environment, we can just type the name of the variable we have saved our requests Response to, in this case the response variable.\n\n#\n\nA cleaner and more understandable method is to check the status_code attribute. Both methods return a HTTP status code. You’ve probably seen a 404 error when you have tried to access a website that doesn’t exist.\n\n#\n\nTry changing CMR_OPS to https://cmr.earthdata.nasa.gov/searches and run requests.get again. Don’t forget to rerun the cell that assigns the url variable\nThe response from requests.get returns the results of the search and metadata about those results in the headers.\nMore information about the response object can be found by typing help(response).\nheaders contains useful information in a case-insensitive dictionary. This information is printed below. TODO: maybe some context for where the 2 elements k, v, come from?\n\n#\n\nWe can see that the content returned is in json format in the UTF-8 character set. We can also see from CMR-Hits that 919 collections were found.\nEach item in the dictionary can be accessed in the normal way you access a python dictionary but because it is case-insensitive, both\n\n#\n\nand\n\n#\n\nwork.\nThis is a large number of data sets. I’m going to restrict the search to cloud-hosted datasets from ASF (Alaska SAR Facility) because I’m interested in SAR images of sea ice. To do this, I set the provider parameter to ASF.\nYou can modify the code below to explore all of the cloud-hosted datasets or cloud-hosted datasets from other providers. A partial list of providers is given below.\n\n\n\n\n\n\n\n\n\nDAAC\nShort Name\nCloud Provider\nOn-Premises Provider\n\n\n\n\nNSIDC\nNational Snow and Ice Data Center\nNSIDC_CPRD\nNSIDC_ECS\n\n\nGHRC DAAC\nGlobal Hydrometeorology Resource Center\nGHRC_DAAC\nGHRC_DAAC\n\n\nPO DAAC\nPhysical Oceanography Distributed Active Archive Center\nPOCLOUD\nPODAAC\n\n\nASF\nAlaska Satellite Facility\nASF\nASF\n\n\nORNL DAAC\nOak Ridge National Laboratory\nORNL_CLOUD\nORNL_DAAC\n\n\nLP DAAC\nLand Processes Distributed Active Archive Center\nLPCLOUD\nLPDAAC_ECS\n\n\nGES DISC\nNASA Goddard Earth Sciences (GES) Data and Information Services Center (DISC)\nGES_DISC\nGES_DISC\n\n\nOB DAAC\nNASA’s Ocean Biology Distributed Active Archive Center\n\nOB_DAAC\n\n\nSEDAC\nNASA’s Socioeconomic Data and Applications Center\n\nSEDAC\n\n\n\nWhen search by provider, use Cloud Provider to search for cloud-hosted datasets and On-Premises Provider to search for datasets archived at the DAACs.\n\n#\n\n\n#\n\nSearch results are contained in the content part of the Response object. However, response.content returns information in bytes.\n\n#\n\nIt is more convenient to work with json formatted data. I’m using pretty print pprint to print the data in an easy to read way.\nStep through response.json(), then to response.json()['feed']['entry'][0]. A reminder that python starts indexing at 0, not 1!\n\n#\n\nThe first response is not the result I am looking for TODO: because xyz…but it does show a few variables that we can use to further refine the search. So I want to print the name of the dataset (dataset_id) and the concept id (id). We can build this variable and print statement like we did above with the url variable. TODO: is it worth saying something about what “feed” and “entry” are?\n\n#\n\n\n#\n\nBut there is a problem. We know from CMR-Hits that there are 49 datasets but only 10 are printed. This is because CMR restricts the number of results returned by a query. The default is 10 but it can be set to a maximum of 2000. Knowing that there were 49 ‘hits’, I’ll set page_size to 49. Then, we can re-run our for loop for the collections.\n\n#\n\n\n#"
  },
  {
    "objectID": "tutorials-templates/01_Data_Discovery_CMR.html#granule-search",
    "href": "tutorials-templates/01_Data_Discovery_CMR.html#granule-search",
    "title": "01. Data discovery with CMR",
    "section": "Granule Search",
    "text": "Granule Search\nIn NASA speak, Granules are files. In this example, we will search for recent Sentinel-1 Ground Range Detected (GRD) Medium Resolution Synthetic Aperture Radar images over the east coast of Greenland. The data in these files are most useful for sea ice mapping.\nI’ll use the data range 2021-10-17 00:00 to 2021-10-18 23:59:59.\nI’ll use a simple bounding box to search. - SW: 76.08166,-67.1746 - NW: 88.19689,21.04862\nFrom the collections search, I know the concept ids for Sentinel-1A and Sentinel-1B GRD medium resolution are - C1214472336-ASF - C1327985578-ASF\nWe need to change the resource url to look for granules instead of collections\n\n#\n\nWe will search by concept_id, temporal, and bounding_box. Details about these search parameters can be found in the CMR API Documentation.\nThe formatting of the values for each parameter is quite specific.\nTemporal parameters are in ISO 8061 format yyyy-MM-ddTHH:mm:ssZ.\nBounding box coordinates are lower left longitude, lower left latitude, upper right longitude, upper right latitude.\n\n#\n\n\n#\n\n\n#\n\n\n#"
  },
  {
    "objectID": "tutorials-templates/09_Zarr_Access.html#timing",
    "href": "tutorials-templates/09_Zarr_Access.html#timing",
    "title": "09. Zarr Access for NetCDF4 files",
    "section": "Timing:",
    "text": "Timing:\n\nExercise: 45 minutes"
  },
  {
    "objectID": "tutorials-templates/09_Zarr_Access.html#summary",
    "href": "tutorials-templates/09_Zarr_Access.html#summary",
    "title": "09. Zarr Access for NetCDF4 files",
    "section": "Summary",
    "text": "Summary\nZarr is an open source library for storing N-dimensional array data. It supports multidimensional arrays with attributes and dimensions similar to NetCDF4, and it can be read by XArray. Zarr is often used for data held in cloud object storage (like Amazon S3), because it is better optimized for these situations than NetCDF4.\nThe zarr-eosdis-store library allows NASA EOSDIS NetCDF4 files to be read more efficiently by transferring only file metadata and data needed for computation in a small number of requests, rather than moving the whole file or making many small requests. It works by making the files directly readable by the Zarr Python library and XArray across a network. To use it, files must have a corresponding metadata file ending in .dmrpp, which increasingly true for cloud-accessible EOSDIS data. https://github.com/nasa/zarr-eosdis-store\nThe zarr-eosdis-store library provides several benefits over downloading EOSDIS data files and accessing them using XArray, NetCDF4, or HDF5 Python libraries:\n\nIt only downloads the chunks of data you actually read, so if you don’t read all variables or the full spatiotemporal extent of a file, you usually won’t spend time downloading those portions of the file\nIt parallelizes and optimizes downloads for the portions of files you do read, so download speeds can be faster in general\nIt automatically interoperates with Earthdata Login if you have a .netrc file set up\nIt is aware of some EOSDIS cloud implementation quirks and provides caching that can save time for repeated requests to individual files\n\nIt can also be faster than using XArray pointing NetCDF4 files with s3:// URLs, depending on the file’s internal structure, and is often more convenient.\nConsider using this library when: 1. The portion of the data file you need to use is much smaller than the full file, e.g. in cases of spatial subsets or reading a single variable from a file containing several 1. s3:// URLs are not readily available 1. Code need to run outside of the AWS cloud or us-west-2 region or in a hybrid cloud / non-cloud manner 1. s3:// access using XArray seems slower than you would expect (possibly due to unoptimized internal file structure) 1. No readily-available, public, cloud-optimized version of the data exists already. The example we show is also available as an AWS Public Dataset: https://registry.opendata.aws/mur/ 1. Adding “.dmrpp” to the end of a data URL returns a file\n\nObjectives\n\nBuild on prior knowledge from CMR and Earthdata Login tutorials\nWork through an example of using the EOSDIS Zarr Store to access data using XArray\nLearn about the Zarr format and library for accessing data in the cloud ___"
  },
  {
    "objectID": "tutorials-templates/09_Zarr_Access.html#exercise",
    "href": "tutorials-templates/09_Zarr_Access.html#exercise",
    "title": "09. Zarr Access for NetCDF4 files",
    "section": "Exercise",
    "text": "Exercise\nIn this exercise, we will be using the eosdis-zarr-store library to aggregate and analyze a month of sea surface temperature for the Great Lakes region\n\nSet up\n\nImport Required Packages\n\n#\n\nAlso set the width / height for plots we show\n\n#\n\n\n\nSet Dataset, Time, and Region of Interest\nLook in PO.DAAC’s cloud archive for Group for High Resolution Sea Surface Temperature (GHRSST) Level 4 Multiscale Ultrahigh Resolution (MUR) data\n\n#\n\nLooking for data from the month of September over the Great Lakes\n\n#\n\n\n\n\nFind URLs for the dataset and AOI\nSet up a CMR granules search for our area of interest, as we saw in prior tutorials\n\n#\n\nSearch for granules in our area of interest, expecting one granule per day of September\n\n#\n\n\n#\n\n\n#\n\n\n#\n\n\n\nOpen and view our AOI without downloading a whole file\n\nCheck to see if we can use an efficient partial-access technique\n\n#\n\nOpen our first URL using the Zarr library\n\n#\n\nThat’s it! No downloads, temporary credentials, or S3 filesystems. Hereafter, we interact with the ds variable as with any XArray dataset. We need not worry about the EosdisStore anymore.\nView the file’s variable structure\n\n#\n\n\n#\n\n\n#\n\n\n#\n\n\n\n\nAggregate and analyze 30 files\nSet up a function to open all of our URLs as XArrays in parallel\n\n#\n\nCombine the individual file-based datasets into a single xarray dataset with a time axis\n\n#\n\nLook at the Analysed SST variable metadata\n\n#\n\nCreate a dataset / variable that is only our area of interest and view its metadata\n\n#\n\nXArray reads data lazily, i.e. only when our code actually needs it. Up to this point, we haven’t read any data values, only metadata. The next line will force XArray to read the portions of the source files containing our area of interest. Behind the scenes, the eosdis-zarr-store library is ensuring data is fetched as efficiently as possible.\nNote: This line isn’t strictly necessary, since XArray will automatically read the data we need the first time our code tries to use it, but calling this will make sure that we can read the data multiple times later on without re-fetching anything from the source files.\nThis line will take several seconds to complete, but since it is retrieving only about 50 MB of data from 22 GB of source files, several seconds constitutes a significant time, bandwidth, and disk space savings.\n\n#\n\nNow we can start looking at aggregations across the time dimension. In this case, plot the standard deviation of the temperature at each point to get a visual sense of how much temperatures fluctuate over the course of the month.\n\n#\n\n\nInteractive animation of a month of data\nThis section isn’t as important to fully understand. It shows us a way to get an interactive animation to see what we have retrieved so far\nDefine an animation function to plot the ith time step. We need to make sure each plot is using the same color scale, set by vmin and vmax so the animation is consistent\n\n#\n\nRender each time slice once and show it as an HTML animation with interactive controls\n\n#\n\n\n\n\nSupplemental: What’s happening here?\nFor EOSDIS data in the cloud, we have begun producing a metadata sidecar file in a format called DMR++ that extracts all of the information about arrays, variables, and dimensions from data files, as well as the byte offsets in the NetCDF4 file where data can be found. This information is sufficient to let the Zarr library read data from our NetCDF4 files, but it’s in the wrong format. zarr-eosdis-store knows how to fetch the sidecar file and transform it into something the Zarr library understands. Passing it when reading Zarr using XArray or the Zarr library lets these libraries interact with EOSDIS data exactly as if they were Zarr stores in a way that’s more optimal for reading data in the cloud. Beyond this, the zarr-eosdis-store library makes some optimizations in the way it reads data to help make up for situations where the NetCDF4 file is not internally arranged well for cloud-based access patterns."
  },
  {
    "objectID": "tutorials-templates/07_Harmony_Subsetting.html",
    "href": "tutorials-templates/07_Harmony_Subsetting.html",
    "title": "07. Data Subsetting and Transformation Services in the Cloud",
    "section": "",
    "text": "Exercise: 40 minutes"
  },
  {
    "objectID": "tutorials-templates/07_Harmony_Subsetting.html#using-the-harmony-py-library-to-access-customized-data-from-nasa-earthdata",
    "href": "tutorials-templates/07_Harmony_Subsetting.html#using-the-harmony-py-library-to-access-customized-data-from-nasa-earthdata",
    "title": "07. Data Subsetting and Transformation Services in the Cloud",
    "section": "",
    "text": "Exercise: 40 minutes"
  },
  {
    "objectID": "tutorials-templates/07_Harmony_Subsetting.html#summary",
    "href": "tutorials-templates/07_Harmony_Subsetting.html#summary",
    "title": "07. Data Subsetting and Transformation Services in the Cloud",
    "section": "Summary",
    "text": "Summary\nWe have already explored direct access to the NASA EOSDIS archive in the cloud via the Amazon Simple Storage Service (S3) by using the Common Metadata Repository (CMR) to search for granule locations. In addition to directly accessing the files archived and distributed by each of the NASA DAACs, many datasets also support services that allow us to customize the data via subsetting, reformatting, reprojection, and other transformations.\nThis tutorial demonstrates how to find, request, and use customized data from a new ecosystem of services operating within the NASA Earthdata Cloud: NASA Harmony.\n\nBenefits\nBut first, why use this option when we’ve already learned how to access data directly from the NASA Earthdata Cloud?\n\nConsistent access patterns to EOSDIS holdings make cross-data center data access easier\nData reduction services allow us to request only the data we want, in the format and projection we want\nAnalysis Ready Data and cloud access will help reduce time-to-science\nCommunity Development helps reduce the barriers for re-use of code and sharing of domain knowledge\n\n\n\n\nData file filtering and subsetting\n\n\nSee more on the Earthdata Harmony landing page, including documentation on the Harmony API itself.\n\n\nObjectives\n\nConceptualize the data transformation service types and offerings provided by NASA Earthdata, including Harmony.\nPractice skills learned from the introductory CMR tutorial to discover what access and service options exist for a given data set, as well as variable metadata.\nUtilize the Harmony-py library to request subsetted MODIS L2 Sea Surface Temperature data over the Gulf of Mexico.\nRead Harmony subsetted outputs directly into xarray. ___"
  },
  {
    "objectID": "tutorials-templates/07_Harmony_Subsetting.html#import-packages",
    "href": "tutorials-templates/07_Harmony_Subsetting.html#import-packages",
    "title": "07. Data Subsetting and Transformation Services in the Cloud",
    "section": "Import Packages",
    "text": "Import Packages\n\n#"
  },
  {
    "objectID": "tutorials-templates/07_Harmony_Subsetting.html#discover-service-options-for-a-given-data-set",
    "href": "tutorials-templates/07_Harmony_Subsetting.html#discover-service-options-for-a-given-data-set",
    "title": "07. Data Subsetting and Transformation Services in the Cloud",
    "section": "Discover service options for a given data set",
    "text": "Discover service options for a given data set\n\nFirst, what do we mean by a “service”?\nIn the context of NASA Earthdata, we are usually referring to a service as any data transformation or customization process that packages or delivers data in a way that makes it easier to work with compared to how the data are natively archived at NASA EOSDIS. Basic customization options may include: * Subsetting (cropping) the data by: * Variable * Spatial boundary, * Temporal range * Reformatting * For example: From NetCDF-4 to Cloud Optimized GeoTIFF * Reprojection and/or Resampling * For example: From Sinusoidal to Polar Stereographic * Mosaicking * Aggregating\nA few main types or pathways for services that are commonly supported across the NASA DAACs include: * NASA Global Imagery Browse Service * Web services providing imagery, much of which is updated daily, to broaden accessibility of NASA EOSDIS data to the media and public. * Web Map Tile Service (WMTS) * Tiled Web Map Service (TWMS) * Web Map Service (WMS) * Keyhole Markup Language (KML) * Geospatial Data Abstraction Library (GDAL) * OPeNDAP * The Open-source Project for a Network Data Access Protocol is a NASA community standard DAP that provides a simple way to access and work with data over the internet. OPeNDAP’s client/server software allows us to subset and reformat data using an internet browser, command line interface, and other applications. * Harmony * In the most basic sense, Harmony is an Application Programming Interface, or API, allowing us to request customization options described above, which are then processed and returned as file outputs. Harmony helps to reduce pre-processing steps so we can spend less time preparing the data, and more time doing science.\nNote: These service offerings are unique to each NASA EOSDIS dataset.\nWhy is this?\nDue to varying levels of service, cloud migration status, and unique characteristics of the datasets themselves, not all service options are provided for all datasets. Therefore it is important to first explore a given dataset’s metadata to discover what service options are provided.\nLet’s utilize the CMR API skills we learned on Day 1 to inspect service metadata:\n\n#\n\nWe want to search by collection to inspect the access and service options that exist:\n\n#\n\nIn the CMR introduction tutorial, we explored cloud-hosted collections from different DAAC providers, and identified the CMR concept-id for a given data set id (also referred to as a short_name).\nHere we are jumping ahead and already know the concept_id we are interested in, by browsing cloud-hosted datasets from PO.DAAC in Earthdata Search: https://search.earthdata.nasa.gov/portal/podaac-cloud/search.\nWe are going to focus on MODIS_A-JPL-L2P-v2019.0: GHRSST Level 2P Global Sea Surface Skin Temperature from the Moderate Resolution Imaging Spectroradiometer (MODIS) on the NASA Aqua satellite (GDS2). Let’s first save this as a variable that we can use later on once we request data from Harmony.\n\n#\n\nWe will view the top-level metadata for this collection to see what additional service and variable metadata exist.\n\n#\n\nPrint the response:\n\n#\n\n\n\nWhat do each of these service values mean?\n\nAssociations\n\nCMR is a large web of interconnected metadata “schemas”, including Collections, Granules, Services, Tools, and Variables. In this case, this collection is associated with two unique services, two tools, and several unique variables.\n\nTags\n\nThere are also tags that describe what service options exist at a high-level. In this case, we see that this dataset supports the ability to reformat, subset by space and time, as well as by variable. This is used in web applications like Earthdata Search to surface those customization options more readily.\n\nService Features\n\nIn this case, we see three separate “features” listed here: esi, Harmony, and OPeNDAP.\n\n\nWe will dig into more details on what Harmony offers for this dataset.\nFirst, we need to isolate the services returned for this dataset:\n\n#\n\n\n#\n\nInspect the first service returned. Now we’re going to search the services endpoint to view that individual service’s metadata, like we did with our dataset above. This time, we’re explicitly setting the format of the response to umm-json in the Accept Header in order to see detailed metadata about the service.\n\n#\n\nDetails about the service metadata record include the service options provided by the “backend” processor connected to Harmony, in this case the PODAAC Level 2 Cloud Subsetter:\n\n#\n\n\n\nDiscover all datasets that support Harmony services\nInstead of searching for services on a known dataset of interest, we may want to discovery all available datasets that are supported for a given service. We can utilize GraphQL, which is a way for us to efficiently gain information across service and collection metadata so that we can print out all supported Harmony datasets. First, we need to specify a query string. Here we are asking to query all collections with service type “Harmony”, and to provide details on the service options attached to those services:\n\n#\n\nThis utilizes a different API endpoint to query CMR metdata using GraphQL. Here we set up another request, passing our query string above:\n\n#\n\nA json response is returned that provides all collections with Harmony-supported services. We can then extract just the collectionshortName, conceptID, and the service names supported for each collection:\n\n#"
  },
  {
    "objectID": "tutorials-templates/07_Harmony_Subsetting.html#discover-variable-names",
    "href": "tutorials-templates/07_Harmony_Subsetting.html#discover-variable-names",
    "title": "07. Data Subsetting and Transformation Services in the Cloud",
    "section": "Discover variable names",
    "text": "Discover variable names\nJust like services, a dataset may also be associated to metadata on their individual data variables, which can be used as input if we wish to subset by one or more variables of interest.\n\n#\n\nSeveral variable records are returned. Again, like we did for services, we’ll search the variables endpoint to view an individual variable’s metadata, and we’ll print out the list of variables for our dataset.\n\n#\n\n\n#\n\n\n#\n\nNext, print out a simple list of all associated variable names by looping the same variable response we submitted above, this time for each variable:\n\n#\n\n\n#"
  },
  {
    "objectID": "tutorials-templates/07_Harmony_Subsetting.html#using-harmony-py-to-subset-data",
    "href": "tutorials-templates/07_Harmony_Subsetting.html#using-harmony-py-to-subset-data",
    "title": "07. Data Subsetting and Transformation Services in the Cloud",
    "section": "Using Harmony-Py to subset data",
    "text": "Using Harmony-Py to subset data\nHarmony-Py provides a pip installable Python alternative to directly using Harmony’s RESTful API to make it easier to request data and service options, especially when interacting within a Python Jupyter Notebook environment.\nThe next steps are adopted from the introduction tutorial notebook provided in the Harmony-Py library:\n\nCreate Harmony Client object\nFirst, we need to create a Harmony Client, which is what we will interact with to submit and inspect a data request to Harmony, as well as to retrieve results.\nWhen creating the Client, we need to provide Earthdata Login credentials, which are required to access data from NASA EOSDIS. This basic line below assumes that we have a .netrc available.\n\n#\n\n\n\nHurricane Ida snapshot\nUsing NASA Worldview, we can first explore SST during a tropical storm event; in this case, we can overlay L2 and L4 SST variables against true color imagery to observe Hurricane Ida in August 2021. Although this is a small sample set, this use case could be expanded to explore how SST responds during the Atlantic hurricane over the next several months. The same data that we are requesting below using Harmony-py can also be requested using NASA Earthdata Search\n\n\n\nHurrican Ida snapshot - Worldview\n\n\n\n\nCreate Harmony Request\nThe following are common request parameters:\n\ncollection: Required parameter. This is the NASA EOSDIS collection, or data product. There are two options for inputting a collection of interest:\n\nProvide a concept ID (e.g. C1940473819-POCLOUD)\nData product short name (e.g. MODIS_A-JPL-L2P-v2019.0).\n\nspatial: Bounding box spatial constraints on the data. The Harmony Bbox class accepts spatial coordinates as decimal degrees in w, s, e, n order, where longitude = -180, 180 and latitude = -90, 90.\ntemporal: Date/time constraints on the data. The example below demonstrates temporal start and end ranges using the python datetime library.\n\nAs we identified above, only subsetting options are available for this dataset. If other service options such as reformatting are available for a given dataset, these can also be specified using Harmony-py: See the documentation for details on how to construct these parameters.\n\n#\n\n\n\nCheck Request validity\nBefore submitting a Harmony Request, we can test your request to see if it’s valid and how to fix it if not. In particular, request.is_valid will check to ensure that the spatial BBox bounds and temporal ranges are entered correctly.\n\n#\n\n\n\nSubmit request\nNow that the request is created, we can now submit it to Harmony using the Harmony Client object. A job id is returned, which is a unique identifier that represents the submitted request.\n\n#\n\n\n\nCheck request status\nWe can check on the progress of a processing job with status(). This method blocks while communicating with the server but returns quickly.\n\n#\n\nDepending on the size of the request, it may be helpful to wait until the request has completed processing before the remainder of the code is executed. The wait_for_processing() method will block subsequent lines of code while optionally showing a progress bar.\n\n#\n\n\n\nView Harmony job response and output URLs\nOnce the data request has finished processing, we can view details on the job that was submitted to Harmony, including the API call to Harmony, and informational messages on the request if available.\nresult_json() calls wait_for_processing() and returns the complete job in JSON format once processing is complete.\n\n#\n\n\n\nDirect cloud access\nNote that the remainder of this tutorial will only succeed when running this notebook within the AWS us-west-2 region.\nHarmony data outputs can be accessed within the cloud using the s3 URLs and AWS credentials provided in the Harmony job response.\n\nRetrieve list of output URLs.\nThe result_urls() method calls wait_for_processing() and returns a list of the processed data URLs once processing is complete. You may optionally show the progress bar as shown below.\n\n#\n\nWe can see that the first file returned does not include the _subsetted suffix, which indicates that a blank file was returned, as no data values were located within our subsetted region. We’ll select the second URL in the list to bring into xarray below.\n\n#\n\n\n\nAWS credential retrieval\nUsing aws_credentials you can retrieve the credentials needed to access the Harmony s3 staging bucket and its contents.\n\n#\n\n\n\n\nOpen staged files with s3fs and xarray\nWe use the AWS s3fs package to create a file system that can then be read by xarray:\n\n#\n\nNow that we have our s3 file system set, including our declared credentials, we’ll use that to open the url, and read in the file through xarray. This extra step is needed because xarray cannot open the S3 location directly. Instead, the S3 file object is passed to xarray, in order to then open the dataset.\n\n#\n\n\n\nPlot the data\nUse the xarray built in plotting function to create a simple plot along the x and y dimensions of the dataset:\n\n#"
  },
  {
    "objectID": "tutorials-templates/07_Harmony_Subsetting.html#resources",
    "href": "tutorials-templates/07_Harmony_Subsetting.html#resources",
    "title": "07. Data Subsetting and Transformation Services in the Cloud",
    "section": "Resources",
    "text": "Resources\n\nIn-depth exploration of the MODIS_A-JPL-L2P-v2019.0 data set, co-locating in-situ and remote sensing data: https://github.com/podaac/tutorials/blob/master/notebooks/SWOT-EA-2021/Colocate_satellite_insitu_ocean.ipynb\nHarmony-Py library introduction tutorial: https://github.com/nasa/harmony-py/blob/main/examples/intro_tutorial.ipynb"
  },
  {
    "objectID": "tutorials-templates/05_Data_Access_Direct_S3.html",
    "href": "tutorials-templates/05_Data_Access_Direct_S3.html",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "",
    "text": "Exercise: 20 minutes"
  },
  {
    "objectID": "tutorials-templates/05_Data_Access_Direct_S3.html#timing",
    "href": "tutorials-templates/05_Data_Access_Direct_S3.html#timing",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "",
    "text": "Exercise: 20 minutes"
  },
  {
    "objectID": "tutorials-templates/05_Data_Access_Direct_S3.html#summary",
    "href": "tutorials-templates/05_Data_Access_Direct_S3.html#summary",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "Summary",
    "text": "Summary\nIn the previous exercises we searched for and discovered cloud data assets that met certain search criteria (i.e., intersects with our region of interest and for a specified date range). The end goal was to find and save web links to the data assets we want to use in our workflow. The links we found allow us to download data via HTTPS (Hypertext Transfer Protocol Secure). However, NASA allows for direct in-region S3 bucket access for the same assets. In addition to saving the HTTPS links, we also created and saved the S3 links for those same cloud assets and we will use them here. In this exercise we will demonstrate how to perform direct in-region S3 bucket access for Harmonized Landsat Sentinel-2 (HLS) cloud data assets.\n\nDirect S3 Access\nNASA Earthdata Cloud provides two pathways for accessing data from the cloud. The first is via HTTPS. The other is through direct S3 bucket access. Below are some benefits and considerations when choosing to use direct S3 bucket access for NASA cloud assets.\n\nBenefits\n\nRetrieving data can be much quicker\nNo need to download data! Work with data in a more efficient manner, “next to it, in the cloud”\n\nIncreased capacity to do parallel processing, due to working in the cloud\n\nYou are working completely within the AWS cloud ecosystem and thus have access to the might of all AWS offerings (e.g., infrastructure, S3 API, services, etc.)\n\n\n\nConsiderations\n\nIf your workflow is in the cloud, choose S3 over HTTPS\n\nAccess only works within AWS us-west-2 region\n\nNeed an AWS S3 “token” to access S3 Bucket\n\nToken expires after 1 hour (currently)\n\nToken only works at the DAAC that generates it, e.g.,\n\nPO.DAAC token generator: https://archive.podaac.earthdata.nasa.gov/s3credentials\n\nLP DAAC token generator: https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials\n\n\nDirect S3 access on its own does not solve ‘cloud’ problems, but it is one key technology in solving big data problems\n\nStill have to load things in to memory, parallelize the computation, if working with really large data volumes. There are a lot of tools that allow you to do that, but are not discussed in this tutorial"
  },
  {
    "objectID": "tutorials-templates/05_Data_Access_Direct_S3.html#what-you-will-learn-from-this-tutorial",
    "href": "tutorials-templates/05_Data_Access_Direct_S3.html#what-you-will-learn-from-this-tutorial",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "What you will learn from this tutorial",
    "text": "What you will learn from this tutorial\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\n\nhow to configure our notebook environment for in-region direct S3 bucket access\n\nhow to access a single HLS file via in-region direct S3 bucket access\n\nhow to create an HLS time series data array from cloud assets via in-region direct S3 bucket access\n\nhow to plot results\n\nThis exercise can be found in the 2021 Cloud Hackathon Book"
  },
  {
    "objectID": "tutorials-templates/05_Data_Access_Direct_S3.html#import-required-packages",
    "href": "tutorials-templates/05_Data_Access_Direct_S3.html#import-required-packages",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "Import Required Packages",
    "text": "Import Required Packages\n\n#"
  },
  {
    "objectID": "tutorials-templates/05_Data_Access_Direct_S3.html#configure-local-environment-and-get-temporary-credentials",
    "href": "tutorials-templates/05_Data_Access_Direct_S3.html#configure-local-environment-and-get-temporary-credentials",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "Configure Local Environment and Get Temporary Credentials",
    "text": "Configure Local Environment and Get Temporary Credentials\nTo perform direct S3 data access one needs to acquire temporary S3 credentials. The credentials give users direct access to S3 buckets in NASA Earthdata Cloud. AWS credentials should not be shared, so take precautions when using them in notebooks our scripts. Note, these temporary credentials are valid for only 1 hour. For more information regarding the temporary credentials visit https://data.lpdaac.earthdatacloud.nasa.gov/s3credentialsREADME. A netrc file is required to aquire these credentials. Use the NASA Earthdata Authentication to create a netrc file in your home directory.\n\n#\n\n\n#\n\n\n#\n\n\nInsert the credentials into our boto3 session and configure our rasterio environment for data access\nCreate a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n\n#\n\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nGDAL environment variables must be configured to access Earthdata Cloud data assets. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\n#"
  },
  {
    "objectID": "tutorials-templates/05_Data_Access_Direct_S3.html#read-in-s3-links",
    "href": "tutorials-templates/05_Data_Access_Direct_S3.html#read-in-s3-links",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "Read in S3 Links",
    "text": "Read in S3 Links\nIn the CMR-STAC API tutorial we saved off multiple text file containing links, both HTTPS and S3 links, to Harmonized Landsat Sentinel-2 (HLS) cloud data assets. We will now read in one of those file and show how to access those data assets.\n\nList the available files in the data directory\nNOTE: If you are running the notebook from the tutorials-templates directory, please use the following path to connect to the data directoy: “../tutorials/data”\n\n#\n\nWe will save our list of links and a single link as Python objects for use later.\nNOTE: If you are running the notebook from the tutorials-templates directory, please use the following path to connect to the data directoy: “../tutorials/data”\n\n#\n\n\n#"
  },
  {
    "objectID": "tutorials-templates/05_Data_Access_Direct_S3.html#read-in-a-single-hls-file",
    "href": "tutorials-templates/05_Data_Access_Direct_S3.html#read-in-a-single-hls-file",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "Read in a single HLS file",
    "text": "Read in a single HLS file\nWe’ll access the HLS S3 object using the rioxarray Python package. The package is an extension of xarray and rasterio, allowing users to read in and interact with geospatial data using xarray data structures. We will also be leveraging the tight integration between xarray and dask to lazily read in data via the chunks parameter. This allows us to connect to the HLS S3 object, reading only metadata, an not load the data into memory until we request it via the loads() function.\n\n#\n\nWhen GeoTIFFS/Cloud Optimized GeoTIFFS are read in, a band coordinate variable is automatically created (see the print out above). In this exercise we will not use that coordinate variable, so we will remove it using the squeeze() function to avoid confusion.\n\n#\n\n\nPlot the HLS S3 object\n\n#\n\nWe can print out the data value as a numpy array by typing .values\n\n#\n\nUp to this point, we have not saved anything but metadata into memory. To save or load the data into memory we can call the .load() function.\n\n#\n\n\n#"
  },
  {
    "objectID": "tutorials-templates/05_Data_Access_Direct_S3.html#read-in-hls-as-a-time-series",
    "href": "tutorials-templates/05_Data_Access_Direct_S3.html#read-in-hls-as-a-time-series",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "Read in HLS as a time series",
    "text": "Read in HLS as a time series\nNow we’ll read in multiple HLS S3 objects as a time series xarray. Let’s print the links list again to see what we’re working with.\n\n#\n\nCurrently, the utilities and packages used in Python to read in GeoTIFF/COG file do not recognize associated dates stored in the internal metadata. To account for the dates for each file we must create a time variable and add it as a dimension in our final time series xarray. We’ll create a function that extracts the date from the file link and create an xarray variable with a time array of datetime objects.\n\n#\n\n\n#\n\nWe’ll now specify a chunk size to use that matches the internal tiling of HLS files. This will help improve performance.\n\n#\n\nNow, we will create our time series.\n\n#\n\nSince we used the chunks parameter while reading the data, the hls_ts_da object is not read into memory yet. To do that we’ll use the load() function.\nNow, we’ll see what we have. Use hvplot to plot our time series\n\n#\n\n\n#\n\n\n#"
  },
  {
    "objectID": "tutorials-templates/05_Data_Access_Direct_S3.html#concluding-remarks",
    "href": "tutorials-templates/05_Data_Access_Direct_S3.html#concluding-remarks",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nThe above exercise demonstrated how to perform in-region direct S3 bucket access for HLS cloud data assets. HLS cloud data assets are stored as Cloud Optimized GeoTIFFs, a format that has been the benifactor of data discovery and access advancements within the Python ecosystem. Knowing what the data storage format is (e.g., COG, netcdf4, or zarr store) and/or what data access protocol you’re using is critical in determining what Python data access method you will use. For COG data, rioxarray package is often prefered due to is ability to bring the geospatial data format into an xarray object. For netcdf4 files, the standard xarray package incombination with s3fs allow users to perform in-region direct access reads into an xarray object. Finally, if you are using OPeNDAP to connect to data, specialized packages like pydap have been integrated into xarray for streamline access directly to an xarray object."
  },
  {
    "objectID": "tutorials-templates/05_Data_Access_Direct_S3.html#resources",
    "href": "tutorials-templates/05_Data_Access_Direct_S3.html#resources",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "Resources",
    "text": "Resources\n\nBuild time series from multiple GeoTIFF files\nHvplot/Holoview Colormap\nhttps://git.earthdata.nasa.gov/projects/LPDUR/repos/lpdaac_cloud_data_access/browse\nhttps://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-tutorial/browse\nDirect S3 Data Access - Rough PODAAC ECCO SSH Example\nDirect access to ECCO data in S3 (from us-west-2)\nDirect S3 Data Access with GDAL Virtual Raster Format (VRT)\nDirect S3 Data Access with rioxarray - Clipping Example"
  },
  {
    "objectID": "team.html",
    "href": "team.html",
    "title": "Our Team",
    "section": "",
    "text": "This effort was supported by the whole NASA Openscapes Mentors cohort, with the following main folks listed alphabetically.\n\n\n\nAffiliations: NSIDC DAAC, University of Colorado \n\n\n\n\nAffiliations: USGS, LP DAAC\n\n\n\n\nAffiliations: NSIDC\n\n\n\n\nAffiliations: Openscapes, NCEAS, UC Santa Barbara\n\n\n\n\nAffiliations: PO.DAAC\n\n\n\n\nAffiliations: PO.DAAC, NASA Jet Propulsion Laboratory/California Institute of Technology\n\n\n\n\nAffiliations: Openscapes, Metadata Game Changers, University of Colorado\n\n\n\n\nAffiliations: NSIDC DAAC, CIRES, University of Colorado\n\n\n\n\nAffiliations: Atmospheric Science Data Center (ASDC), NASA Langley Research Center (LaRC)",
    "crumbs": [
      "Welcome",
      "Our Team"
    ]
  },
  {
    "objectID": "team.html#main-instructors-and-support",
    "href": "team.html#main-instructors-and-support",
    "title": "Our Team",
    "section": "",
    "text": "This effort was supported by the whole NASA Openscapes Mentors cohort, with the following main folks listed alphabetically.\n\n\n\nAffiliations: NSIDC DAAC, University of Colorado \n\n\n\n\nAffiliations: USGS, LP DAAC\n\n\n\n\nAffiliations: NSIDC\n\n\n\n\nAffiliations: Openscapes, NCEAS, UC Santa Barbara\n\n\n\n\nAffiliations: PO.DAAC\n\n\n\n\nAffiliations: PO.DAAC, NASA Jet Propulsion Laboratory/California Institute of Technology\n\n\n\n\nAffiliations: Openscapes, Metadata Game Changers, University of Colorado\n\n\n\n\nAffiliations: NSIDC DAAC, CIRES, University of Colorado\n\n\n\n\nAffiliations: Atmospheric Science Data Center (ASDC), NASA Langley Research Center (LaRC)",
    "crumbs": [
      "Welcome",
      "Our Team"
    ]
  },
  {
    "objectID": "team.html#hackathon-helpers",
    "href": "team.html#hackathon-helpers",
    "title": "Our Team",
    "section": "Hackathon Helpers",
    "text": "Hackathon Helpers\n\nMike Gangl, PO.DAAC\nJinbo Wang, JPL\nCelia Ou, PO.DAAC\nEd, Armstrong, JPL",
    "crumbs": [
      "Welcome",
      "Our Team"
    ]
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects Overview",
    "section": "",
    "text": "The following was borrowed and adapted from the excellent SnowEx Hackathon 2021\nThis section contains everything you need to know about cloud hackathon projects.",
    "crumbs": [
      "Projects",
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/index.html#purpose-of-the-projects",
    "href": "projects/index.html#purpose-of-the-projects",
    "title": "Projects Overview",
    "section": "Purpose of the projects:",
    "text": "Purpose of the projects:\nDuring the Cloud Hackathon we will be facilitating team hacking sessions in the second half of each day. The purpose of these sessions is for you to gain hands-on experience in working together on a well-defined problem, in a collaborative space where you can talk things through and get help.",
    "crumbs": [
      "Projects",
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/index.html#what-is-hacking",
    "href": "projects/index.html#what-is-hacking",
    "title": "Projects Overview",
    "section": "What is hacking?",
    "text": "What is hacking?\nHacking is a session of focused, highly collaborative work time – often involving coding – in which the group creates conditions for rapid absorption of new ideas and methods. The word “hack” or “hackathon” has many different interpretations, both positive and negative. Here our intention is to foster the idea of hacking as a fun, interactive and welcoming environment to explore and experiment with computer code.",
    "crumbs": [
      "Projects",
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/index.html#how-will-the-projects-be-conducted",
    "href": "projects/index.html#how-will-the-projects-be-conducted",
    "title": "Projects Overview",
    "section": "How will the projects be conducted?",
    "text": "How will the projects be conducted?\nParticipants are invited to start conversations about projects in the Slack channel 2021-nasacloudhack-projects one week before the Cloud Hackathon begins.\n\nIf you have a project idea brewing, please pitch it in this channel (even if you have signed up for the cloud hackathon as a team; tag your proposed teammates if you already have that worked out).\nStart a thread with “Project idea:” and then provide a few sentences. Include whether you are looking for teammates to join this project. Others who are interested can respond in a thread.\nWe welcome a broad range of project topics. People often use project time to dig deeper into concepts introduced in tutorials, to explore problems within their own research, or to advance community data sharing and software building efforts.\nThe Cloud Hackathon team is here to help you get clear on project ideas and decide on what is possible within 5 days. Feel free to reach out to any of us between now and the Cloud Hackathon in the 2021-nasacloudhack-help channel, or @ us – we all have “helper” appended to the front of our names. !\n\nAt the end of Day 1 of the Cloud Hackathon we will have a Pitchfest where proposer(s) can pitch their idea, and mention whether they are still looking for teammates or if they have already formed a team using the Slack _#2021-nasacloudhack-projects _channel. At this time we will finalize the project teams for the week.\nTeam hacktime will begin on Day 2.\nEach team is encouraged to identify a project lead, likely the person who pitched the idea, who has knowledge of the datasets and the specific problem to be explored. But roles can be assigned as the group decides to best fit skills and needs.\nThroughout the hackathon we will have optional morning office hours 8-9am PT for additional support or team check-in time.\nOn the final day of the Cloud Hackathon, each team will present their work in a series of lightning talks.",
    "crumbs": [
      "Projects",
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/index.html#what-can-i-do-to-prepare-in-advance",
    "href": "projects/index.html#what-can-i-do-to-prepare-in-advance",
    "title": "Projects Overview",
    "section": "What can I do to prepare in advance?",
    "text": "What can I do to prepare in advance?\n\nIf you have a project idea already brewing, we encourage you to share that with participants on our Slack channel 2021-nasacloudhack-projects.\nFeel free to explore various projects and initiate conversations. The goal is to gather as much information as you can to inform your decision about which team to join during the Cloud Hackathon",
    "crumbs": [
      "Projects",
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/index.html#how-to-start-hacking",
    "href": "projects/index.html#how-to-start-hacking",
    "title": "Projects Overview",
    "section": "How to start hacking",
    "text": "How to start hacking\nSuggestions for how to set up your GitHub workflow is in the GitHub Workflows section.\nThis is a great slide deck for collaborating on project teamwork collaborating from SnowEx Hackweek.",
    "crumbs": [
      "Projects",
      "Projects Overview"
    ]
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#timing",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#timing",
    "title": "Direct S3 Data Access",
    "section": "Timing:",
    "text": "Timing:\n\nExercise: 20 minutes"
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#summary",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#summary",
    "title": "Direct S3 Data Access",
    "section": "Summary",
    "text": "Summary\nIn the previous exercises we searched for and discovered cloud data assets that met certain search criteria (i.e., intersects with our region of interest and for a specified date range). The end goal was to find and save web links to the data assets we want to use in our workflow. The links we found allow us to download data via HTTPS (Hypertext Transfer Protocol Secure). However, NASA allows for direct in-region S3 bucket access for the same assets. In addition to saving the HTTPS links, we also created and saved the S3 links for those same cloud assets and we will use them here. In this exercise we will demonstrate how to perform direction in-region S3 bucket access for Harmonized Landsat Sentinel-2 (HLS) cloud data assets.\n\nDirect S3 Access\nNASA Eartdata Cloud provides two pathways for accessing data from the cloud. The first is via HTTPS. The other is through direct S3 bucket access. Below are some benefits and considerations when choosing to use direct S3 bucket access for NASA cloud assets.\n\nBenefits\n\nRetrieve data is very quickly\nNo need to download data! Work with data in a more efficient manner\nIncreased capacity to do parallel processing\nYou are working completely with the AWS cloud ecosystem and thus have access to the might of all AWS offerings (e.g., infrastructure, S3 API, services, etc.)\n\n\n\nConsiderations\n\nAccess only works within AWS us-west-2 region\nNeed an AWS S3 “token” to access S3 Bucket\nToken expires after 1 hour\nToken only works at the DAAC that generates it, e.g.,\n\nPO.DAAC token generator: https://archive.podaac.earthdata.nasa.gov/s3credentials\nLP DAAC token generator: https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials\n\nS3 on its own does not solve ‘cloud’ problems, but it is one key technology in solving big data problems\nStill have to load things in to memory, parallelize the computation, if working with really large data volumes. There are a lot of tool that allow you to do that, not discussed in this tutorial\n\n\n\n\nObjective\n\nConfigure our notebook environment and retrieve temporary S3 credentials for in-region direct S3 bucket access\nAccess a single HLS file\nAccess and clip an HLS file to a region of interest\nCreate an HLS time series data array\n\nLet’s get started!\n\n\n\nImport Required Packages\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport os\nimport subprocess\nimport requests\nimport boto3\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nfrom rasterio.plot import show\nimport rioxarray\nimport geopandas\nimport pyproj\nfrom pyproj import Proj\nfrom shapely.ops import transform\nimport geoviews as gv\nfrom cartopy import crs\nimport hvplot.xarray\nimport holoviews as hv\ngv.extension('bokeh', 'matplotlib')"
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#configure-local-environment-and-get-temporary-credentials",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#configure-local-environment-and-get-temporary-credentials",
    "title": "Direct S3 Data Access",
    "section": "Configure Local Environment and Get Temporary Credentials",
    "text": "Configure Local Environment and Get Temporary Credentials\nTo perform direct S3 data access one needs to acquire temporary S3 credentials. The credentials give users direct access to S3 buckets in NASA Earthdata Cloud. AWS credentials should not be shared, so take precautions when using them in notebooks our scripts. Note, these temporary credentials are valid for only 1 hour. For more information regarding the temporary credentials visit https://data.lpdaac.earthdatacloud.nasa.gov/s3credentialsREADME. A netrc file is required to aquire these credentials. Use the NASA Earthdata Authentication to create a netrc file in your home directory.\n\ns3_cred_endpoint = 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\n\n\ndef get_temp_creds():\n    temp_creds_url = s3_cred_endpoint\n    return requests.get(temp_creds_url).json()\n\n\ntemp_creds_req = get_temp_creds()\n#temp_creds_req                      # !!! BEWARE, removing the # on this line will print your temporary S3 credentials.\n\n\nInsert the credentials into our boto3 session and configure our rasterio environment for data access\nCreate a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nGDAL environment variables must be configured to access Earthdata Cloud data assets. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\n&lt;rasterio.env.Env at 0x7fe992047490&gt;"
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-s3-links",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-s3-links",
    "title": "Direct S3 Data Access",
    "section": "Read in S3 Links",
    "text": "Read in S3 Links\nIn the CMR-STAC API tutorial we saved off multiple text file containing links, both HTTPS and S3 links, to Harmonized Landsat Sentinel-2 (HLS) cloud data assets. We will now read in one of those file and show how to access those data assets.\n\nList the available files in the data directory\n\nfor f in os.listdir('./data'):\n    print(f)\n\nHTTPS_T13TGF_B02_Links.txt\nS3_T13TGF_B05_Links.txt\nne_w_agfields.geojson\nHTTPS_T13TGF_Fmask_Links.txt\n.ipynb_checkpoints\nS3_T13TGF_B8A_Links.txt\nHTTPS_T13TGF_B04_Links.txt\nS3_T13TGF_B04_Links.txt\nS3_T13TGF_Fmask_Links.txt\nHTTPS_T13TGF_B8A_Links.txt\ndataset-diagram.png\nHTTPS_T13TGF_B05_Links.txt\nS3_T13TGF_B02_Links.txt\n\n\nWe will safe our list of links and a single link as Python objects for use later.\n\ns3_links = open('./data/S3_T13TGF_B04_Links.txt').read().splitlines()\ns3_links\n\n['s3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021133T173859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021140T173021.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021140T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021145T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021156T173029.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021163T173909.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021165T172422.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021165T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021185T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021188T173037.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021190T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021198T173911.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021200T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021203T173909.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021204T173042.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021215T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021220T173049.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021229T172441.v1.5.B04.tif']\n\n\n\ns3_link = s3_links[0]\ns3_link\n\n's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B04.tif'"
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-a-single-hls-file",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-a-single-hls-file",
    "title": "Direct S3 Data Access",
    "section": "Read in a single HLS file",
    "text": "Read in a single HLS file\nWe’ll access the HLS S3 object using the rioxarray Python package. The package is an extension of xarray and rasterio, allowing users to read in and interact with geospatial data using xarray data structures. We will also be leveraging the tight integration between xarray and dask to lazily read in data via the chunks parameter. This allows us to connect to the HLS S3 object, reading only metadata, an not load the data into memory until we request it via the loads() function.\n\nhls_da = rioxarray.open_rasterio(s3_link, chuncks=True)\nhls_da\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 3660, x: 3660)&gt;\n[13395600 values with dtype=int16]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayband: 1y: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (4)band(band)int641array([1])x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\nWhen GeoTIFFS/Cloud Optimized GeoTIFFS are read in, a band coordinate variable is automatically created (see the print out above). In this exercise we will not use that coordinate variable, so we will remove it using the squeeze() function to avoid confusion.\n\nhls_da = hls_da.squeeze('band', drop=True)\nhls_da\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (y: 3660, x: 3660)&gt;\n[13395600 values with dtype=int16]\nCoordinates:\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayy: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (3)x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\n\nPlot the HLS S3 object\n\nhls_da.hvplot.image(x='x', y='y', cmap='fire', rasterize=True, width=800, height=600, colorbar=True)    # colormaps -&gt; https://holoviews.org/user_guide/Colormaps.html\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe can print out the data value as a numpy array by typing .values\n\nhls_da.values\n\narray([[-9999, -9999, -9999, ...,  1527,  1440,  1412],\n       [-9999, -9999, -9999, ...,  1493,  1476,  1407],\n       [-9999, -9999, -9999, ...,  1466,  1438,  1359],\n       ...,\n       [-9999, -9999, -9999, ...,  1213,  1295,  1159],\n       [-9999, -9999, -9999, ...,  1042,  1232,  1185],\n       [-9999, -9999, -9999, ...,   954,  1127,  1133]], dtype=int16)\n\n\nUp to this point, we have not saved anything but metadata into memory. To save or load the data into memory we can call the .load() function.\n\nhls_da_data = hls_da.load()\nhls_da_data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (y: 3660, x: 3660)&gt;\narray([[-9999, -9999, -9999, ...,  1527,  1440,  1412],\n       [-9999, -9999, -9999, ...,  1493,  1476,  1407],\n       [-9999, -9999, -9999, ...,  1466,  1438,  1359],\n       ...,\n       [-9999, -9999, -9999, ...,  1213,  1295,  1159],\n       [-9999, -9999, -9999, ...,  1042,  1232,  1185],\n       [-9999, -9999, -9999, ...,   954,  1127,  1133]], dtype=int16)\nCoordinates:\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayy: 3660x: 3660-9999 -9999 -9999 -9999 -9999 -9999 ... 1676 1486 1112 954 1127 1133array([[-9999, -9999, -9999, ...,  1527,  1440,  1412],\n       [-9999, -9999, -9999, ...,  1493,  1476,  1407],\n       [-9999, -9999, -9999, ...,  1466,  1438,  1359],\n       ...,\n       [-9999, -9999, -9999, ...,  1213,  1295,  1159],\n       [-9999, -9999, -9999, ...,  1042,  1232,  1185],\n       [-9999, -9999, -9999, ...,   954,  1127,  1133]], dtype=int16)Coordinates: (3)x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\n\ndel(hls_da_data)"
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-and-clip-a-single-hls-file",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-and-clip-a-single-hls-file",
    "title": "Direct S3 Data Access",
    "section": "Read in and clip a single HLS file",
    "text": "Read in and clip a single HLS file\nTo clip the HLS file, our feature representing our region of interest must be in the same coordinate reference system (CRS) or projection coordinate system as the HLS file. The map projection for our HLS file is Universal Transverse Mercator (UTM) zone 13N. Our feature is mapped to WGS84 geographic coordinate system grid space. We need to transform the geographic coordinate reference system (CRS) of our feature to the UTM projected coordinate system (i.e., UTM Zone 13N)\n\nRead in our geojson file and transform its CRS\n\nfield = geopandas.read_file('./data/ne_w_agfields.geojson')\n\nLet’s take a look at the bounding coordinate values.\n\nfield_shape = field.geometry[0]\nfield_shape.bounds\n\n(-101.67271614074707,\n 41.04754380304359,\n -101.65344715118408,\n 41.06213891056728)\n\n\nNote, the values above are in decimal degrees and represent the longitude and latitude for the lower left corner (-101.67271614074707, 41.04754380304359) and upper right corner (-101.65344715118408, 41.06213891056728) respectively.\n\n\nGet the projection information from the HLS file\n\nhls_proj = hls_da.rio.crs\nhls_proj\n\nCRS.from_epsg(32613)\n\n\n\n\nTransform coordinates from lat lon (units = dd) to UTM (units = m)\n\ngeo_CRS = Proj('+proj=longlat +datum=WGS84 +no_defs', preserve_units=True)   # Source coordinate system of the ROI\n\n\nproject = pyproj.Transformer.from_proj(geo_CRS, hls_proj)                    # Set up the transformation\n\n\nfsUTM = transform(project.transform, field_shape)\nfsUTM.bounds\n\n(779588.4994601272, 4549370.366049466, 781270.1479326887, 4551052.979639321)\n\n\nThe coordinates for our feature have now been converted to UTM Zone 13N whether meters is the designated unit. Note the difference in the values between field_shape.bounds (in geographic) and fsUTM.bounds (in UTM projection).\nNow we can clip our HLS file to our region of insterest!\n\n\nAccess and clip the HLS file\nWe can now use our transformed ROI bounding box to clip the HLS S3 object we accessed before. We’ll use the `rio.clip\n\nhls_da_clip = rioxarray.open_rasterio(s3_link, chunks=True).squeeze('band', drop=True).rio.clip([fsUTM])\n\n\nhls_da_clip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (y: 56, x: 56)&gt;\ndask.array&lt;astype, shape=(56, 56), dtype=int16, chunksize=(56, 56), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * y            (y) float64 4.551e+06 4.551e+06 ... 4.549e+06 4.549e+06\n  * x            (x) float64 7.796e+05 7.796e+05 ... 7.812e+05 7.812e+05\n    spatial_ref  int64 0\nAttributes:\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Red\n    _FillValue:    -9999xarray.DataArrayy: 56x: 56dask.array&lt;chunksize=(56, 56), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n6.12 kiB\n6.12 kiB\n\n\nShape\n(56, 56)\n(56, 56)\n\n\nCount\n12 Tasks\n1 Chunks\n\n\nType\nint16\nnumpy.ndarray\n\n\n\n\n         56 56\n\n\n\n\nCoordinates: (3)y(y)float644.551e+06 4.551e+06 ... 4.549e+06axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metrearray([4551045., 4551015., 4550985., 4550955., 4550925., 4550895., 4550865.,\n       4550835., 4550805., 4550775., 4550745., 4550715., 4550685., 4550655.,\n       4550625., 4550595., 4550565., 4550535., 4550505., 4550475., 4550445.,\n       4550415., 4550385., 4550355., 4550325., 4550295., 4550265., 4550235.,\n       4550205., 4550175., 4550145., 4550115., 4550085., 4550055., 4550025.,\n       4549995., 4549965., 4549935., 4549905., 4549875., 4549845., 4549815.,\n       4549785., 4549755., 4549725., 4549695., 4549665., 4549635., 4549605.,\n       4549575., 4549545., 4549515., 4549485., 4549455., 4549425., 4549395.])x(x)float647.796e+05 7.796e+05 ... 7.812e+05axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metrearray([779595., 779625., 779655., 779685., 779715., 779745., 779775., 779805.,\n       779835., 779865., 779895., 779925., 779955., 779985., 780015., 780045.,\n       780075., 780105., 780135., 780165., 780195., 780225., 780255., 780285.,\n       780315., 780345., 780375., 780405., 780435., 780465., 780495., 780525.,\n       780555., 780585., 780615., 780645., 780675., 780705., 780735., 780765.,\n       780795., 780825., 780855., 780885., 780915., 780945., 780975., 781005.,\n       781035., 781065., 781095., 781125., 781155., 781185., 781215., 781245.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :779580.0 30.0 0.0 4551060.0 0.0 -30.0array(0)Attributes: (4)scale_factor :0.0001add_offset :0.0long_name :Red_FillValue :-9999\n\n\n\nhls_da_clip.hvplot.image(x = 'x', y = 'y', crs = 'EPSG:32613', cmap='fire', rasterize=True, width=800, height=600, colorbar=True)"
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-and-clip-an-hls-time-series",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#read-in-and-clip-an-hls-time-series",
    "title": "Direct S3 Data Access",
    "section": "Read in and clip an HLS time series",
    "text": "Read in and clip an HLS time series\nNow we’ll read in multiple HLS S3 objects as a time series xarray. Let’s print the links list again to see what we’re working with.\n\ns3_links\n\n['s3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021133T173859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021140T173021.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021140T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021145T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021156T173029.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021163T173909.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021165T172422.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021165T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021185T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021188T173037.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021190T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021198T173911.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021200T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021203T173909.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021204T173042.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021215T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021220T173049.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021229T172441.v1.5.B04.tif']\n\n\nCurrently, the utilities and packages used in Python to read in GeoTIFF/COG file do not recognize associated dates stored in the internal metadata. To account for the dates for each file we must create a time variable and add it as a dimension in our final time series xarray. We’ll create a function that extracts the date from the file link and create an xarray variable with a time array of datetime objects.\n\ndef time_index_from_filenames(file_links):\n    '''\n    Helper function to create a pandas DatetimeIndex\n    '''\n    return [datetime.strptime(f.split('.')[-5], '%Y%jT%H%M%S') for f in file_links]\n\n\ntime = xr.Variable('time', time_index_from_filenames(s3_links))\n\nWe’ll now specify a chunk size to use that matches the internal tiling of HLS files. This will help improve performance.\n\nchunks=dict(band=1, x=1024, y=1024)\n\nNow, we will create our time series.\n\nhls_ts_da = xr.concat([rioxarray.open_rasterio(f, chunks=chunks).squeeze('band', drop=True) for f in s3_links], dim=time)\nhls_ts_da\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (time: 19, y: 3660, x: 3660)&gt;\ndask.array&lt;concatenate, shape=(19, 3660, 3660), dtype=int16, chunksize=(1, 1024, 1024), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArraytime: 19y: 3660x: 3660dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n485.45 MiB\n2.00 MiB\n\n\nShape\n(19, 3660, 3660)\n(1, 1024, 1024)\n\n\nCount\n1235 Tasks\n304 Chunks\n\n\nType\nint16\nnumpy.ndarray\n\n\n\n\n                                                                         3660 3660 19\n\n\n\n\nCoordinates: (4)x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:30:21.000000000', '2021-05-20T17:28:59.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\nSince we used the chunks parameter while reading the data, the hls_ts_da object is read into memory. To do that we’ll use the load() function. But, before that, we’ll clip the hls_ts_da object to our roi using our transformed roi coordinates.\n\nhls_ts_da_clip = hls_ts_da.rio.clip([fsUTM]).load()\nhls_ts_da_clip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (time: 19, y: 56, x: 56)&gt;\narray([[[-9999, -9999, -9999, ...,   980, -9999, -9999],\n        [-9999, -9999, -9999, ...,   287, -9999, -9999],\n        [ 1573,  1692,  1708, ...,   410, -9999, -9999],\n        ...,\n        [-9999, -9999,  1165, ...,  1808,  1869,  1906],\n        [-9999, -9999,   989, ..., -9999, -9999, -9999],\n        [-9999, -9999,  1085, ..., -9999, -9999, -9999]],\n\n       [[-9999, -9999, -9999, ...,   860, -9999, -9999],\n        [-9999, -9999, -9999, ...,   496, -9999, -9999],\n        [ 2681,  2773,  2496, ...,   550, -9999, -9999],\n        ...,\n        [-9999, -9999,  3847, ...,  1997,  1914,  1831],\n        [-9999, -9999,  4062, ..., -9999, -9999, -9999],\n        [-9999, -9999,  4313, ..., -9999, -9999, -9999]],\n\n       [[-9999, -9999, -9999, ...,   808, -9999, -9999],\n        [-9999, -9999, -9999, ...,   230, -9999, -9999],\n        [ 1802,  1828,  1863, ...,   306, -9999, -9999],\n        ...,\n...\n        ...,\n        [-9999, -9999,  1124, ...,   804,   934,  1008],\n        [-9999, -9999,  1003, ..., -9999, -9999, -9999],\n        [-9999, -9999,   904, ..., -9999, -9999, -9999]],\n\n       [[-9999, -9999, -9999, ...,  1313, -9999, -9999],\n        [-9999, -9999, -9999, ...,  1327, -9999, -9999],\n        [ 1091,  1094,  1179, ...,  1223, -9999, -9999],\n        ...,\n        [-9999, -9999,  1145, ...,  1005,  1097,  1197],\n        [-9999, -9999,  1037, ..., -9999, -9999, -9999],\n        [-9999, -9999,  1114, ..., -9999, -9999, -9999]],\n\n       [[-9999, -9999, -9999, ...,  1272, -9999, -9999],\n        [-9999, -9999, -9999, ...,  1231, -9999, -9999],\n        [ 1086,  1105,  1193, ...,  1205, -9999, -9999],\n        ...,\n        [-9999, -9999,  1045, ...,  1049,  1142,  1219],\n        [-9999, -9999,   926, ..., -9999, -9999, -9999],\n        [-9999, -9999,  1076, ..., -9999, -9999, -9999]]], dtype=int16)\nCoordinates:\n  * y            (y) float64 4.551e+06 4.551e+06 ... 4.549e+06 4.549e+06\n  * x            (x) float64 7.796e+05 7.796e+05 ... 7.812e+05 7.812e+05\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n    spatial_ref  int64 0\nAttributes:\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Red\n    _FillValue:    -9999xarray.DataArraytime: 19y: 56x: 56-9999 -9999 -9999 -9999 -9999 -9999 ... -9999 -9999 -9999 -9999 -9999array([[[-9999, -9999, -9999, ...,   980, -9999, -9999],\n        [-9999, -9999, -9999, ...,   287, -9999, -9999],\n        [ 1573,  1692,  1708, ...,   410, -9999, -9999],\n        ...,\n        [-9999, -9999,  1165, ...,  1808,  1869,  1906],\n        [-9999, -9999,   989, ..., -9999, -9999, -9999],\n        [-9999, -9999,  1085, ..., -9999, -9999, -9999]],\n\n       [[-9999, -9999, -9999, ...,   860, -9999, -9999],\n        [-9999, -9999, -9999, ...,   496, -9999, -9999],\n        [ 2681,  2773,  2496, ...,   550, -9999, -9999],\n        ...,\n        [-9999, -9999,  3847, ...,  1997,  1914,  1831],\n        [-9999, -9999,  4062, ..., -9999, -9999, -9999],\n        [-9999, -9999,  4313, ..., -9999, -9999, -9999]],\n\n       [[-9999, -9999, -9999, ...,   808, -9999, -9999],\n        [-9999, -9999, -9999, ...,   230, -9999, -9999],\n        [ 1802,  1828,  1863, ...,   306, -9999, -9999],\n        ...,\n...\n        ...,\n        [-9999, -9999,  1124, ...,   804,   934,  1008],\n        [-9999, -9999,  1003, ..., -9999, -9999, -9999],\n        [-9999, -9999,   904, ..., -9999, -9999, -9999]],\n\n       [[-9999, -9999, -9999, ...,  1313, -9999, -9999],\n        [-9999, -9999, -9999, ...,  1327, -9999, -9999],\n        [ 1091,  1094,  1179, ...,  1223, -9999, -9999],\n        ...,\n        [-9999, -9999,  1145, ...,  1005,  1097,  1197],\n        [-9999, -9999,  1037, ..., -9999, -9999, -9999],\n        [-9999, -9999,  1114, ..., -9999, -9999, -9999]],\n\n       [[-9999, -9999, -9999, ...,  1272, -9999, -9999],\n        [-9999, -9999, -9999, ...,  1231, -9999, -9999],\n        [ 1086,  1105,  1193, ...,  1205, -9999, -9999],\n        ...,\n        [-9999, -9999,  1045, ...,  1049,  1142,  1219],\n        [-9999, -9999,   926, ..., -9999, -9999, -9999],\n        [-9999, -9999,  1076, ..., -9999, -9999, -9999]]], dtype=int16)Coordinates: (4)y(y)float644.551e+06 4.551e+06 ... 4.549e+06axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metrearray([4551045., 4551015., 4550985., 4550955., 4550925., 4550895., 4550865.,\n       4550835., 4550805., 4550775., 4550745., 4550715., 4550685., 4550655.,\n       4550625., 4550595., 4550565., 4550535., 4550505., 4550475., 4550445.,\n       4550415., 4550385., 4550355., 4550325., 4550295., 4550265., 4550235.,\n       4550205., 4550175., 4550145., 4550115., 4550085., 4550055., 4550025.,\n       4549995., 4549965., 4549935., 4549905., 4549875., 4549845., 4549815.,\n       4549785., 4549755., 4549725., 4549695., 4549665., 4549635., 4549605.,\n       4549575., 4549545., 4549515., 4549485., 4549455., 4549425., 4549395.])x(x)float647.796e+05 7.796e+05 ... 7.812e+05axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metrearray([779595., 779625., 779655., 779685., 779715., 779745., 779775., 779805.,\n       779835., 779865., 779895., 779925., 779955., 779985., 780015., 780045.,\n       780075., 780105., 780135., 780165., 780195., 780225., 780255., 780285.,\n       780315., 780345., 780375., 780405., 780435., 780465., 780495., 780525.,\n       780555., 780585., 780615., 780645., 780675., 780705., 780735., 780765.,\n       780795., 780825., 780855., 780885., 780915., 780945., 780975., 781005.,\n       781035., 781065., 781095., 781125., 781155., 781185., 781215., 781245.])time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:30:21.000000000', '2021-05-20T17:28:59.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :779580.0 30.0 0.0 4551060.0 0.0 -30.0array(0)Attributes: (4)scale_factor :0.0001add_offset :0.0long_name :Red_FillValue :-9999\n\n\nNow, we’ll see what we have. Use hvplot to plot the clipped time series\n\nhls_ts_da_clip.hvplot.image(x='x', y='y', width=800, height=600, colorbar=True, cmap='fire').opts(clim=(hls_ts_da_clip.values.min(), hls_ts_da_clip.values.max()))\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n# Exit our context\nrio_env.__exit__()"
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#resourses",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__rioxarray_clipping.html#resourses",
    "title": "Direct S3 Data Access",
    "section": "Resourses",
    "text": "Resourses\n\nBuild time series from multiple GeoTIFF files\nHvplot/Holoview Colormap\nhttps://git.earthdata.nasa.gov/projects/LPDUR/repos/lpdaac_cloud_data_access/browse\nhttps://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-tutorial/browse"
  },
  {
    "objectID": "tutorials/01_Data_Discovery_CMR.html",
    "href": "tutorials/01_Data_Discovery_CMR.html",
    "title": "01. Data discovery with CMR",
    "section": "",
    "text": "In this tutorial you will learn: - what CMR is;\n- how to use the requests package to search data collections and granules;\n- how to parse the results of these searches.\nWe will focus on datasets in the cloud. Currently, DAACs with data in the cloud are ‘ASF’, ‘GES_DISC’, ‘GHRC_DAAC’, ‘LPCLOUD’, ‘ORNL_CLOUD’, ‘POCLOUD’",
    "crumbs": [
      "Tutorials",
      "01. Data discovery with CMR"
    ]
  },
  {
    "objectID": "tutorials/01_Data_Discovery_CMR.html#what-is-cmr",
    "href": "tutorials/01_Data_Discovery_CMR.html#what-is-cmr",
    "title": "01. Data discovery with CMR",
    "section": "What is CMR",
    "text": "What is CMR\nCMR is the Common Metadata Repository. It catalogs all data for NASA’s Earth Observing System Data and Information System (EOSDIS). It is the backend of Earthdata Search, the GUI search interface you are probably familiar with. More information about CMR can be found here.\nUnfortunately, the GUI for Earthdata Search is not accessible from a cloud instance - at least not without some work. Earthdata Search is also not immediately reproducible. What I mean by that is if you create a search using the GUI you would have to note the search criteria (date range, search area, collection name, etc), take a screenshot, copy the search url, or save the list of data granules returned by the search, in order to recreate the search. This information would have to be re-entered each time you or someone else wanted to do the search. You could make typos or other mistakes. A cleaner, reproducible solution is to search CMR programmatically using the CMR API.",
    "crumbs": [
      "Tutorials",
      "01. Data discovery with CMR"
    ]
  },
  {
    "objectID": "tutorials/01_Data_Discovery_CMR.html#what-is-the-cmr-api",
    "href": "tutorials/01_Data_Discovery_CMR.html#what-is-the-cmr-api",
    "title": "01. Data discovery with CMR",
    "section": "What is the CMR API",
    "text": "What is the CMR API\nAPI stands for Application Programming Interface. It allows applications (software, services, etc) to send information to each other. A helpful analogy is a waiter in a restaurant. The waiter takes your drink or food order that you select from the menu, often translated into short-hand, to the bar or kitchen, and then returns (hopefully) with what you ordered when it is ready.\nThe CMR API accepts search terms such as collection name, keywords, datetime range, and location, queries the CMR database and returns the results.",
    "crumbs": [
      "Tutorials",
      "01. Data discovery with CMR"
    ]
  },
  {
    "objectID": "tutorials/01_Data_Discovery_CMR.html#how-to-search-cmr-from-python",
    "href": "tutorials/01_Data_Discovery_CMR.html#how-to-search-cmr-from-python",
    "title": "01. Data discovery with CMR",
    "section": "How to search CMR from Python",
    "text": "How to search CMR from Python\nThe first step is to import python packages. We will use:\n- requests This package does most of the work for us accessing the CMR API using HTTP methods. - pprint to pretty print the results of the search.\nA more in depth tutorial on requests is here\n\nimport requests\nfrom pprint import pprint\n\nThen we need to authenticate with EarthData Login. Since we’ve already set this up in the previous lesson, here you need to enter your username before executing the cell.\nTo conduct a search using the CMR API, requests needs the url for the root CMR search endpoint. We’ll build this url as a python variable.\n\nCMR_OPS = 'https://cmr.earthdata.nasa.gov/search'\n\nCMR allows search by collections, which are datasets, and granules, which are files that contain data. Many of the same search parameters can be used for colections and granules but the type of results returned differ. Search parameters can be found in the API Documentation.\nWhether we search collections or granules is distinguished by adding \"collections\" or \"granules\" to the url for the root CMR endpoint.\nWe are going to search collections first, so we add collections to the url. I’m using a python format string here.\n\nurl = f'{CMR_OPS}/{\"collections\"}'\n\nIn this first example, I want to retrieve a list of collections that are hosted in the cloud. Each collection has a cloud_hosted parameter that is either True if that collection is in the cloud and False if it is not. The migration of NASA data to the cloud is a work in progress. Not all collections tagged as cloud_hosted have granules. To search for only cloud_hosted datasets with granules, I also set has_granules to True.\nI also want to get the content in json (pronounced “jason”) format, so I pass a dictionary to the header keyword argument to say that I want results returned as json.\nThe .get() method is used to send this information to the CMR API. get() calls the HTTP method GET.\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                        },\n                        headers={\n                            'Accept': 'application/json',\n                        }\n                       )\n\nrequests returns a Response object.\nOften, we want to check that our request was successful. In a notebook or someother interactive environment, we can just type the name of the variable we have saved our requests Response to, in this case the response variable.\n\nresponse\n\n&lt;Response [200]&gt;\n\n\nA cleaner and more understandable method is to check the status_code attribute. Both methods return a HTTP status code. You’ve probably seen a 404 error when you have tried to access a website that doesn’t exist.\n\nresponse.status_code\n\n200\n\n\nTry changing CMR_OPS to https://cmr.earthdata.nasa.gov/searches and run requests.get again. Don’t forget to rerun the cell that assigns the url variable\nThe response from requests.get returns the results of the search and metadata about those results in the headers.\nMore information about the response object can be found by typing help(response).\nheaders contains useful information in a case-insensitive dictionary. This information is printed below. TODO: maybe some context for where the 2 elements k, v, come from?\n\nfor k, v in response.headers.items():\n    print(f'{k}: {v}')\n\nContent-Type: application/json;charset=utf-8\nContent-Length: 3820\nConnection: keep-alive\nDate: Tue, 02 Nov 2021 22:07:37 GMT\nX-Frame-Options: SAMEORIGIN\nAccess-Control-Allow-Origin: *\nX-XSS-Protection: 1; mode=block\nCMR-Request-Id: 510cf611-d65b-4b46-981b-f4719405676a\nStrict-Transport-Security: max-age=31536000\nCMR-Search-After: [0.0,14000.0,\"SENTINEL-1A_RAW\",\"1\",1214470561,1320]\nCMR-Hits: 917\nAccess-Control-Expose-Headers: CMR-Hits, CMR-Request-Id, X-Request-Id, CMR-Scroll-Id, CMR-Search-After, CMR-Timed-Out, CMR-Shapefile-Original-Point-Count, CMR-Shapefile-Simplified-Point-Count\nX-Content-Type-Options: nosniff\nCMR-Took: 435\nX-Request-Id: 510cf611-d65b-4b46-981b-f4719405676a\nVary: Accept-Encoding, User-Agent\nContent-Encoding: gzip\nServer: ServerTokens ProductOnly\nX-Cache: Miss from cloudfront\nVia: 1.1 3f7e5e686bf8f19b9c786efbe99c7589.cloudfront.net (CloudFront)\nX-Amz-Cf-Pop: DEN52-C1\nX-Amz-Cf-Id: BXEUlb5ygB9nj9ygUbSZvIc3BE4k0d1Mn1qDd66IkkuX1rC_Z-mN6Q==\n\n\nWe can see that the content returned is in json format in the UTF-8 character set. We can also see from CMR-Hits that 919 collections were found.\nEach item in the dictionary can be accessed in the normal way you access a python dictionary but because it is case-insensitive, both\n\nresponse.headers['CMR-Hits']\n\n'917'\n\n\nand\n\nresponse.headers['cmr-hits']\n\n'917'\n\n\nwork.\nThis is a large number of data sets. I’m going to restrict the search to cloud-hosted datasets from ASF (Alaska SAR Facility) because I’m interested in SAR images of sea ice. To do this, I set the provider parameter to ASF.\nYou can modify the code below to explore all of the cloud-hosted datasets or cloud-hosted datasets from other providers. A partial list of providers is given below.\n\n\n\n\n\n\n\n\n\nDAAC\nShort Name\nCloud Provider\nOn-Premises Provider\n\n\n\n\nNSIDC\nNational Snow and Ice Data Center\nNSIDC_CPRD\nNSIDC_ECS\n\n\nGHRC DAAC\nGlobal Hydrometeorology Resource Center\nGHRC_DAAC\nGHRC_DAAC\n\n\nPO DAAC\nPhysical Oceanography Distributed Active Archive Center\nPOCLOUD\nPODAAC\n\n\nASF\nAlaska Satellite Facility\nASF\nASF\n\n\nORNL DAAC\nOak Ridge National Laboratory\nORNL_CLOUD\nORNL_DAAC\n\n\nLP DAAC\nLand Processes Distributed Active Archive Center\nLPCLOUD\nLPDAAC_ECS\n\n\nGES DISC\nNASA Goddard Earth Sciences (GES) Data and Information Services Center (DISC)\nGES_DISC\nGES_DISC\n\n\nOB DAAC\nNASA’s Ocean Biology Distributed Active Archive Center\n\nOB_DAAC\n\n\nSEDAC\nNASA’s Socioeconomic Data and Applications Center\n\nSEDAC\n\n\n\nWhen search by provider, use Cloud Provider to search for cloud-hosted datasets and On-Premises Provider to search for datasets archived at the DAACs.\n\nprovider = 'ASF'\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                            'provider': provider,\n                        },\n                        headers={\n                            'Accept': 'application/json'\n                        }\n                       )\n\n\nresponse.headers['cmr-hits']\n\n'45'\n\n\nSearch results are contained in the content part of the Response object. However, response.content returns information in bytes.\n\nresponse.content\n\nb'{\"feed\":{\"updated\":\"2021-11-02T22:41:34.322Z\",\"id\":\"https://cmr.earthdata.nasa.gov:443/search/collections.json?cloud_hosted=True&has_granules=True&provider=ASF\",\"title\":\"ECHO dataset metadata\",\"entry\":[{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:39.000Z\",\"dataset_id\":\"SENTINEL-1A_SLC\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_SLC\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1A_SLC\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1A slant-range product\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214470488-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2016-04-25T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:17:22.000Z\",\"dataset_id\":\"SENTINEL-1B_SLC\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1B_SLC\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1B_SLC\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1B slant-range product\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1327985661-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1B\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:15:56.000Z\",\"dataset_id\":\"SENTINEL-1A_DUAL_POL_GRD_HIGH_RES\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_DP_GRD_HIGH\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1A_DUAL_POL_GRD_HIGH_RES\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1A Dual-pol ground projected high and full resolution images\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214470533-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2016-04-25T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:47.000Z\",\"dataset_id\":\"SENTINEL-1B_DUAL_POL_GRD_HIGH_RES\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1B_DP_GRD_HIGH\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1B_DUAL_POL_GRD_HIGH_RES\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1B Dual-pol ground projected high and full resolution images\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1327985645-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1B\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2016-04-25T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:49.000Z\",\"dataset_id\":\"SENTINEL-1B_DUAL_POL_GRD_MEDIUM_RES\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1B_DP_GRD_MEDIUM\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1B_DUAL_POL_GRD_MEDIUM_RES\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1B Dual-pol ground projected medium resolution images\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1327985660-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1B\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:15:58.000Z\",\"dataset_id\":\"SENTINEL-1A_DUAL_POL_GRD_MEDIUM_RES\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_DP_GRD_MEDIUM\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1A_DUAL_POL_GRD_MEDIUM_RES\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1A Dual-pol ground projected medium resolution images\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214471521-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:20.000Z\",\"dataset_id\":\"SENTINEL-1A_RAW\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_RAW\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1A_RAW\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1A level zero product\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214470561-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:15.000Z\",\"dataset_id\":\"SENTINEL-1A_METADATA_SLC\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_META_SLC\",\"organizations\":[\"ASF\"],\"title\":\"SENTINEL-1A_METADATA_SLC\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Metadata for Sentinel-1A slant-range product\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214470496-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2016-04-25T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:17:02.000Z\",\"dataset_id\":\"SENTINEL-1B_METADATA_SLC\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1B_META_SLC\",\"organizations\":[\"ASF\"],\"title\":\"SENTINEL-1B_METADATA_SLC\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Metadata for Sentinel-1B slant-range product\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1327985617-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1B\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]},{\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2014-04-03T00:00:00.000Z\",\"version_id\":\"1\",\"updated\":\"2021-07-15T19:16:26.000Z\",\"dataset_id\":\"SENTINEL-1A_SINGLE_POL_GRD_HIGH_RES\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"ASF\",\"short_name\":\"SENTINEL-1A_SP_GRD_HIGH\",\"organizations\":[\"ASF\",\"ESA/CS1CGS\"],\"title\":\"SENTINEL-1A_SINGLE_POL_GRD_HIGH_RES\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"Sentinel-1A Single-pol ground projected high and full resolution images\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1214470682-ASF\",\"has_formats\":false,\"original_format\":\"ECHO10\",\"archive_center\":\"ASF\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"Sentinel-1A\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://vertex.daac.asf.alaska.edu/\"}]}]}}'\n\n\nIt is more convenient to work with json formatted data. I’m using pretty print pprint to print the data in an easy to read way.\nStep through response.json(), then to response.json()['feed']['entry'][0]. A reminder that python starts indexing at 0, not 1!\n\npprint(response.json()['feed']['entry'][0])\n\n{'archive_center': 'ASF',\n 'boxes': ['-90 -180 90 180'],\n 'browse_flag': False,\n 'coordinate_system': 'CARTESIAN',\n 'data_center': 'ASF',\n 'dataset_id': 'SENTINEL-1A_SLC',\n 'has_formats': False,\n 'has_spatial_subsetting': False,\n 'has_temporal_subsetting': False,\n 'has_transforms': False,\n 'has_variables': False,\n 'id': 'C1214470488-ASF',\n 'links': [{'href': 'https://vertex.daac.asf.alaska.edu/',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n 'online_access_flag': True,\n 'orbit_parameters': {},\n 'organizations': ['ASF', 'ESA/CS1CGS'],\n 'original_format': 'ECHO10',\n 'platforms': ['Sentinel-1A'],\n 'service_features': {'esi': {'has_formats': False,\n                              'has_spatial_subsetting': False,\n                              'has_temporal_subsetting': False,\n                              'has_transforms': False,\n                              'has_variables': False},\n                      'harmony': {'has_formats': False,\n                                  'has_spatial_subsetting': False,\n                                  'has_temporal_subsetting': False,\n                                  'has_transforms': False,\n                                  'has_variables': False},\n                      'opendap': {'has_formats': False,\n                                  'has_spatial_subsetting': False,\n                                  'has_temporal_subsetting': False,\n                                  'has_transforms': False,\n                                  'has_variables': False}},\n 'short_name': 'SENTINEL-1A_SLC',\n 'summary': 'Sentinel-1A slant-range product',\n 'time_start': '2014-04-03T00:00:00.000Z',\n 'title': 'SENTINEL-1A_SLC',\n 'updated': '2021-07-15T19:16:39.000Z',\n 'version_id': '1'}\n\n\nThe first response is not the result I am looking for TODO: because xyz…but it does show a few variables that we can use to further refine the search. So I want to print the name of the dataset (dataset_id) and the concept id (id). We can build this variable and print statement like we did above with the url variable. TODO: is it worth saying something about what “feed” and “entry” are?\n\ncollections = response.json()['feed']['entry']\n\n\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} {collection[\"dataset_id\"]} {collection[\"id\"]}')\n\nASF SENTINEL-1A_SLC C1214470488-ASF\nASF SENTINEL-1B_SLC C1327985661-ASF\nASF SENTINEL-1A_DUAL_POL_GRD_HIGH_RES C1214470533-ASF\nASF SENTINEL-1B_DUAL_POL_GRD_HIGH_RES C1327985645-ASF\nASF SENTINEL-1B_DUAL_POL_GRD_MEDIUM_RES C1327985660-ASF\nASF SENTINEL-1A_DUAL_POL_GRD_MEDIUM_RES C1214471521-ASF\nASF SENTINEL-1A_RAW C1214470561-ASF\nASF SENTINEL-1A_METADATA_SLC C1214470496-ASF\nASF SENTINEL-1B_METADATA_SLC C1327985617-ASF\nASF SENTINEL-1A_SINGLE_POL_GRD_HIGH_RES C1214470682-ASF\n\n\nBut there is a problem. We know from CMR-Hits that there are 49 datasets but only 10 are printed. This is because CMR restricts the number of results returned by a query. The default is 10 but it can be set to a maximum of 2000. Knowing that there were 49 ‘hits’, I’ll set page_size to 49. Then, we can re-run our for loop for the collections.\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'provider': provider,\n                            'page_size': 49,\n                        },\n                        headers={\n                            'Accept': 'application/json'\n                        }\n                       )\n\n\ncollections = response.json()['feed']['entry']\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} {collection[\"dataset_id\"]} {collection[\"id\"]}')\n\nASF SENTINEL-1A_SLC C1214470488-ASF\nASF SENTINEL-1B_SLC C1327985661-ASF\nASF SENTINEL-1A_DUAL_POL_GRD_HIGH_RES C1214470533-ASF\nASF SENTINEL-1B_DUAL_POL_GRD_HIGH_RES C1327985645-ASF\nASF SENTINEL-1B_DUAL_POL_GRD_MEDIUM_RES C1327985660-ASF\nASF SENTINEL-1A_DUAL_POL_GRD_MEDIUM_RES C1214471521-ASF\nASF SENTINEL-1A_RAW C1214470561-ASF\nASF SENTINEL-1A_METADATA_SLC C1214470496-ASF\nASF SENTINEL-1B_METADATA_SLC C1327985617-ASF\nASF SENTINEL-1A_SINGLE_POL_GRD_HIGH_RES C1214470682-ASF\nASF SENTINEL-1A_OCN C1214472977-ASF\nASF SENTINEL-1B_RAW C1327985647-ASF\nASF SENTINEL-1A_DUAL_POL_METADATA_GRD_HIGH_RES C1214470576-ASF\nASF SENTINEL-1B_OCN C1327985579-ASF\nASF SENTINEL-1A_METADATA_RAW C1214470532-ASF\nASF SENTINEL-1B_DUAL_POL_METADATA_GRD_HIGH_RES C1327985741-ASF\nAlaska Satellite Facility Sentinel-1 Interferograms (BETA) C1595422627-ASF\nASF ALOS_AVNIR_OBS_ORI C1808440897-ASF\nASF SENTINEL-1A_SINGLE_POL_GRD_MEDIUM_RES C1214472994-ASF\nASF SENTINEL-1B_METADATA_RAW C1327985650-ASF\nASF SENTINEL-1B_SINGLE_POL_GRD_HIGH_RES C1327985571-ASF\nASF SENTINEL-1B_SINGLE_POL_GRD_MEDIUM_RES C1327985740-ASF\nASF SENTINEL-1A_METADATA_OCN C1266376001-ASF\nASF SENTINEL-1B_METADATA_OCN C1327985646-ASF\nASF SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES C1214472336-ASF\nASF SENTINEL-1A_SINGLE_POL_METADATA_GRD_HIGH_RES C1214470732-ASF\nASF SENTINEL-1A_SINGLE_POL_METADATA_GRD_MEDIUM_RES C1214473170-ASF\nASF SENTINEL-1B_DUAL_POL_METADATA_GRD_MEDIUM_RES C1327985578-ASF\nASF SENTINEL-1B_SINGLE_POL_METADATA_GRD_HIGH_RES C1327985619-ASF\nASF SENTINEL-1B_SINGLE_POL_METADATA_GRD_MEDIUM_RES C1327985739-ASF\nASF STS-68_BROWSE_GRD C1661710593-ASF\nASF STS-68_BROWSE_SLC C1661710596-ASF\nASF STS-59_BROWSE_GRD C1661710578-ASF\nASF STS-59_BROWSE_SLC C1661710581-ASF\nASF SENTINEL-1A_DUAL_POL_GRD_FULL_RES C1214471197-ASF\nASF SENTINEL-1A_DUAL_POL_METADATA_GRD_FULL_RES C1214471960-ASF\nASF SENTINEL-1A_SINGLE_POL_GRD_FULL_RES C1214472978-ASF\nASF SENTINEL-1A_SINGLE_POL_METADATA_GRD_FULL_RES C1214473165-ASF\nASF SENTINEL-1B_DUAL_POL_GRD_FULL_RES C1327985697-ASF\nASF SENTINEL-1B_DUAL_POL_METADATA_GRD_FULL_RES C1327985651-ASF\nASF SENTINEL-1B_SINGLE_POL_GRD_FULL_RES C1327985644-ASF\nASF SENTINEL-1B_SINGLE_POL_METADATA_GRD_FULL_RES C1327985674-ASF\nAlaska Satellite Facility Sentinel-1 Unwrapped Interferogram and Coherence Map (BETA) C1379535600-ASF\nAlaska Satellite Facility Sentinel-1 Interferograms - Amplitude (BETA) C1596065640-ASF\nAlaska Satellite Facility Sentinel-1 Interferograms - Coherence (BETA) C1596065639-ASF\nAlaska Satellite Facility Sentinel-1 Interferograms - Connected Components (BETA) C1596065641-ASF\nAlaska Satellite Facility Sentinel-1 Interferograms - Unwrapped Phase (BETA) C1595765183-ASF\nASF STS-59_GRD C1661710583-ASF\nASF STS-59_METADATA_GRD C1661710586-ASF",
    "crumbs": [
      "Tutorials",
      "01. Data discovery with CMR"
    ]
  },
  {
    "objectID": "tutorials/01_Data_Discovery_CMR.html#granule-search",
    "href": "tutorials/01_Data_Discovery_CMR.html#granule-search",
    "title": "01. Data discovery with CMR",
    "section": "Granule Search",
    "text": "Granule Search\nIn NASA speak, Granules are files. In this example, we will search for recent Sentinel-1 Ground Range Detected (GRD) Medium Resolution Synthetic Aperture Radar images over the east coast of Greenland. The data in these files are most useful for sea ice mapping.\nI’ll use the data range 2021-10-17 00:00 to 2021-10-18 23:59:59.\nI’ll use a simple bounding box to search. - SW: 76.08166,-67.1746 - NW: 88.19689,21.04862\nFrom the collections search, I know the concept ids for Sentinel-1A and Sentinel-1B GRD medium resolution are - C1214472336-ASF - C1327985578-ASF\nWe need to change the resource url to look for granules instead of collections\n\nurl = f'{CMR_OPS}/{\"granules\"}'\n\nWe will search by concept_id, temporal, and bounding_box. Details about these search parameters can be found in the CMR API Documentation.\nThe formatting of the values for each parameter is quite specific.\nTemporal parameters are in ISO 8061 format yyyy-MM-ddTHH:mm:ssZ.\nBounding box coordinates are lower left longitude, lower left latitude, upper right longitude, upper right latitude.\n\nresponse = requests.get(url, \n                        params={\n                            'concept_id': 'C1214472336-ASF',\n                            'temporal': '2020-10-17T00:00:00Z,2020-10-18T23:59:59Z',\n                            'bounding_box': '76.08166,-67.1746,88.19689,21.04862',\n                            'page_size': 200,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nprint(response.status_code)\n\n200\n\n\n\nprint(response.headers['CMR-Hits'])\n\n6\n\n\n\ngranules = response.json()['feed']['entry']\n#for granule in granules:\n#    print(f'{granule[\"archive_center\"]} {granule[\"dataset_id\"]} {granule[\"id\"]}')\n\n\npprint(granules)\n\n[{'browse_flag': True,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633258819580078',\n  'id': 'G1954601581-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201017T132009_20201017T132039_034836_040F98_404E.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://datapool.asf.alaska.edu/BROWSE/SA/S1A_EW_GRDM_1SDH_20201017T132009_20201017T132039_034836_040F98_404E.jpg',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34836'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-59.163563 87.942726 -60.893669 89.293564 -59.279579 '\n                '96.119583 -57.619923 94.49958 -59.163563 87.942726']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201017T132009_20201017T132039_034836_040F98_404E',\n  'time_end': '2020-10-17T13:20:39.000Z',\n  'time_start': '2020-10-17T13:20:09.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201017T132009_20201017T132039_034836_040F98_404E-METADATA_GRD_MD',\n  'updated': '2020-10-19T17:13:39.000Z'},\n {'browse_flag': False,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633354187011719',\n  'id': 'G1954616816-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201017T145616_20201017T145720_034837_040FA0_0B4B.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34837'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-66.104271 69.819366 -69.571243 74.741966 -67.42112 83.209152 '\n                '-64.210938 77.59269 -66.104271 69.819366']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201017T145616_20201017T145720_034837_040FA0_0B4B',\n  'time_end': '2020-10-17T14:57:20.000Z',\n  'time_start': '2020-10-17T14:56:16.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201017T145616_20201017T145720_034837_040FA0_0B4B-METADATA_GRD_MD',\n  'updated': '2020-10-19T18:35:34.000Z'},\n {'browse_flag': True,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633354187011719',\n  'id': 'G1954616638-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201017T145720_20201017T145820_034837_040FA0_72CE.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://datapool.asf.alaska.edu/BROWSE/SA/S1A_EW_GRDM_1SDH_20201017T145720_20201017T145820_034837_040FA0_72CE.jpg',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34837'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-62.765087 66.277176 -66.103951 69.81897 -64.211227 77.590164 '\n                '-61.060097 73.427185 -62.765087 66.277176']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201017T145720_20201017T145820_034837_040FA0_72CE',\n  'time_end': '2020-10-17T14:58:20.000Z',\n  'time_start': '2020-10-17T14:57:20.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201017T145720_20201017T145820_034837_040FA0_72CE-METADATA_GRD_MD',\n  'updated': '2020-10-19T18:33:31.000Z'},\n {'browse_flag': True,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633354187011719',\n  'id': 'G1954805829-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201018T135856_20201018T140000_034851_041027_2659.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://datapool.asf.alaska.edu/BROWSE/SA/S1A_EW_GRDM_1SDH_20201018T135856_20201018T140000_034851_041027_2659.jpg',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34851'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-65.236397 83.18071 -68.712318 87.887711 -66.630493 96.161102 '\n                '-63.400326 90.779785 -65.236397 83.18071']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201018T135856_20201018T140000_034851_041027_2659',\n  'time_end': '2020-10-18T14:00:00.000Z',\n  'time_start': '2020-10-18T13:58:56.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201018T135856_20201018T140000_034851_041027_2659-METADATA_GRD_MD',\n  'updated': '2020-10-20T07:16:54.000Z'},\n {'browse_flag': True,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633354187011719',\n  'id': 'G1954799806-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201018T140000_20201018T140100_034851_041027_2777.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://datapool.asf.alaska.edu/BROWSE/SA/S1A_EW_GRDM_1SDH_20201018T140000_20201018T140100_034851_041027_2777.jpg',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34851'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-61.876564 79.848251 -65.236069 83.180344 -63.400402 90.778 '\n                '-60.217258 86.840034 -61.876564 79.848251']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201018T140000_20201018T140100_034851_041027_2777',\n  'time_end': '2020-10-18T14:01:00.000Z',\n  'time_start': '2020-10-18T14:00:00.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201018T140000_20201018T140100_034851_041027_2777-METADATA_GRD_MD',\n  'updated': '2020-10-20T06:51:16.000Z'},\n {'browse_flag': False,\n  'collection_concept_id': 'C1214472336-ASF',\n  'coordinate_system': 'GEODETIC',\n  'data_center': 'ASF',\n  'dataset_id': 'SENTINEL-1A_DUAL_POL_METADATA_GRD_MEDIUM_RES',\n  'day_night_flag': 'UNSPECIFIED',\n  'granule_size': '0.05633258819580078',\n  'id': 'G1954798927-ASF',\n  'links': [{'href': 'https://datapool.asf.alaska.edu/METADATA_GRD_MD/SA/S1A_EW_GRDM_1SDH_20201018T140100_20201018T140148_034851_041027_C95D.iso.xml',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n             'title': 'This link provides direct download access to the '\n                      'granule.'},\n            {'href': 'www.asf.alaska.edu/sar-data-sets/sentinel-1',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 data set landing page (VIEW RELATED '\n                      'INFORMATION)'},\n            {'href': 'www.asf.alaska.edu/sar-information/sentinel-1-documents-tools',\n             'hreflang': 'en-US',\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n             'title': 'ASF DAAC Sentinel-1 User Guide and Technical '\n                      'Documentation (VIEW RELATED INFORMATION)'},\n            {'href': 'https://vertex.daac.asf.alaska.edu/',\n             'hreflang': 'en-US',\n             'inherited': True,\n             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n  'online_access_flag': True,\n  'orbit_calculated_spatial_domains': [{'orbit_number': '34851'}],\n  'original_format': 'ECHO10',\n  'polygons': [['-59.113174 77.623962 -61.876228 79.847961 -60.217464 '\n                '86.837784 -57.570774 84.174744 -59.113174 77.623962']],\n  'producer_granule_id': 'S1A_EW_GRDM_1SDH_20201018T140100_20201018T140148_034851_041027_C95D',\n  'time_end': '2020-10-18T14:01:48.000Z',\n  'time_start': '2020-10-18T14:01:00.000Z',\n  'title': 'S1A_EW_GRDM_1SDH_20201018T140100_20201018T140148_034851_041027_C95D-METADATA_GRD_MD',\n  'updated': '2020-10-20T06:46:01.000Z'}]",
    "crumbs": [
      "Tutorials",
      "01. Data discovery with CMR"
    ]
  },
  {
    "objectID": "tutorials/04_NASA_Earthdata_Authentication.html",
    "href": "tutorials/04_NASA_Earthdata_Authentication.html",
    "title": "04. Authentication for NASA Earthdata",
    "section": "",
    "text": "This notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin &lt;USERNAME&gt;\npassword &lt;PASSWORD&gt;\n&lt;USERNAME&gt; and &lt;PASSWORD&gt; would be replaced by your actual Earthdata Login username and password respectively.",
    "crumbs": [
      "Tutorials",
      "04. Authentication for NASA Earthdata"
    ]
  },
  {
    "objectID": "tutorials/04_NASA_Earthdata_Authentication.html#summary",
    "href": "tutorials/04_NASA_Earthdata_Authentication.html#summary",
    "title": "04. Authentication for NASA Earthdata",
    "section": "",
    "text": "This notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin &lt;USERNAME&gt;\npassword &lt;PASSWORD&gt;\n&lt;USERNAME&gt; and &lt;PASSWORD&gt; would be replaced by your actual Earthdata Login username and password respectively.",
    "crumbs": [
      "Tutorials",
      "04. Authentication for NASA Earthdata"
    ]
  },
  {
    "objectID": "tutorials/04_NASA_Earthdata_Authentication.html#import-required-packages",
    "href": "tutorials/04_NASA_Earthdata_Authentication.html#import-required-packages",
    "title": "04. Authentication for NASA Earthdata",
    "section": "Import Required Packages",
    "text": "Import Required Packages\n\nfrom netrc import netrc\nfrom subprocess import Popen\nfrom platform import system\nfrom getpass import getpass\nimport os\n\nThe code below will:\n\ncheck what operating system (OS) is being used to determine which netrc file to check for/create (.netrc or _netrc)\ncheck if you have an netrc file, and if so, varify if those credentials are for the Earthdata endpoint\ncreate a netrc file if a netrc file is not present.\n\n\nurs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\nprompts = ['Enter NASA Earthdata Login Username: ',\n           'Enter NASA Earthdata Login Password: ']\n\n# Determine the OS (Windows machines usually use an '_netrc' file)\nnetrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n\n# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\ntry:\n    netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n    netrc(netrcDir).authenticators(urs)[0]\n\n# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\nexcept FileNotFoundError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('touch {0}{2} | echo machine {1} &gt;&gt; {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} &gt;&gt; {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'&gt;&gt; {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n    # Set restrictive permissions\n    Popen('chmod 0600 {0}{1}'.format(homeDir + os.sep, netrc_name), shell=True)\n\n    # Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\nexcept TypeError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('echo machine {1} &gt;&gt; {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} &gt;&gt; {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'&gt;&gt; {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n\n\nSee if the file was created\nIf the file was created, we’ll see a .netrc file (_netrc for Window OS) in the list printed below. To view the contents from a Jupyter environment, click File on the top toolbar, select Open from Path…, type .netrc, and click Open. The .netrc file will open within the text editor.\n\n!!! Beware, your password will be visible if the .netrc file is opened in the text editor.\n\n\n!ls -al ~/",
    "crumbs": [
      "Tutorials",
      "04. Authentication for NASA Earthdata"
    ]
  },
  {
    "objectID": "tutorials/02_Data_Discovery_CMR-STAC_API.html",
    "href": "tutorials/02_Data_Discovery_CMR-STAC_API.html",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "",
    "text": "Exercise: 30 min",
    "crumbs": [
      "Tutorials",
      "02. Data Discovery with CMR-STAC API"
    ]
  },
  {
    "objectID": "tutorials/02_Data_Discovery_CMR-STAC_API.html#timing",
    "href": "tutorials/02_Data_Discovery_CMR-STAC_API.html#timing",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "",
    "text": "Exercise: 30 min",
    "crumbs": [
      "Tutorials",
      "02. Data Discovery with CMR-STAC API"
    ]
  },
  {
    "objectID": "tutorials/02_Data_Discovery_CMR-STAC_API.html#summary",
    "href": "tutorials/02_Data_Discovery_CMR-STAC_API.html#summary",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "Summary",
    "text": "Summary\nIn this example we will access the NASA’s Harmonized Landsat Sentinel-2 (HLS) version 2 assets, which are archived in cloud optimized geoTIFF (COG) format in the LP DAAC Cumulus cloud space. The COGs can be used like any other geoTIFF file, but have some added features that make them more efficient within the cloud data access paradigm. These features include: overviews and internal tiling. Below we will demonstrate how to leverage these features.\n\nBut first, what is STAC?\nSpatioTemporal Asset Catalog (STAC) is a specification that provides a common language for interpreting geospatial information in order to standardize indexing and discovering data.\nThe STAC specification is made up of a collection of related, yet independent specifications that when used together provide search and discovery capabilities for remote assets.\n\nFour STAC Specifications\nSTAC Catalog (aka DAAC Archive)\nSTAC Collection (aka Data Product)\nSTAC Item (aka Granule)\nSTAC API\nIn the following sections, we will explore each of STAC element using NASA’s Common Metadata Repository (CMR) STAC application programming interface (API), or CMR-STAC API for short.\n\n\n\nCMR-STAC API\nThe CMR-STAC API is NASA’s implementation of the STAC API specification for all NASA data holdings within EOSDIS. The current implementation does not allow for querries accross the entire NASA catalog. Users must execute searches within provider catalogs (e.g., LPCLOUD) to find the STAC Items they are searching for. All the providers can be found at the CMR-STAC endpoint here: https://cmr.earthdata.nasa.gov/stac/.\nIn this exercise, we will query the LPCLOUD provider to identify STAC Items from the Harmonized Landsat Sentinel-2 (HLS) collection that fall within our region of interest (ROI) and within our specified time range.",
    "crumbs": [
      "Tutorials",
      "02. Data Discovery with CMR-STAC API"
    ]
  },
  {
    "objectID": "tutorials/02_Data_Discovery_CMR-STAC_API.html#what-you-will-learn-from-this-tutorial",
    "href": "tutorials/02_Data_Discovery_CMR-STAC_API.html#what-you-will-learn-from-this-tutorial",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "What you will learn from this tutorial",
    "text": "What you will learn from this tutorial\n\nhow to connect to NASA CMR-STAC API using Python’s pystac-client\n\nhow to navigate CMR-STAC records\n\nhow to read in a geojson file using geopandas to specify your region of interest\nhow to use the CMR-STAC API to search for data\nhow to perform post-search filtering of CMR-STAC API search result in Python\n\nhow to extract and save data access URLs for geospatial assets\n\nThis exercise can be found in the 2021 Cloud Hackathon Book",
    "crumbs": [
      "Tutorials",
      "02. Data Discovery with CMR-STAC API"
    ]
  },
  {
    "objectID": "tutorials/02_Data_Discovery_CMR-STAC_API.html#import-required-packages",
    "href": "tutorials/02_Data_Discovery_CMR-STAC_API.html#import-required-packages",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "Import Required Packages",
    "text": "Import Required Packages\n\nfrom pystac_client import Client  \nfrom collections import defaultdict    \nimport json\nimport geopandas\nimport geoviews as gv\nfrom cartopy import crs\ngv.extension('bokeh', 'matplotlib')",
    "crumbs": [
      "Tutorials",
      "02. Data Discovery with CMR-STAC API"
    ]
  },
  {
    "objectID": "tutorials/02_Data_Discovery_CMR-STAC_API.html#explored-available-nasa-providers",
    "href": "tutorials/02_Data_Discovery_CMR-STAC_API.html#explored-available-nasa-providers",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "Explored available NASA Providers",
    "text": "Explored available NASA Providers\n\nSTAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n\n\nConnect to the CMR-STAC API\n\nprovider_cat = Client.open(STAC_URL)\n\nWe’ll create a providers variable so we can take a deeper look into available data providers - subcategories are referred to as “children”. We can then print them as a for loop.\n\nproviders = [p for p in provider_cat.get_children()]\n\nfor count, provider in enumerate(providers):\n    print(f'{count} - {provider.title}')\n\n0 - LARC_ASDC\n1 - USGS_EROS\n2 - ESA\n3 - GHRC\n4 - LAADS\n5 - OBPG\n6 - OB_DAAC\n7 - ECHO\n8 - ISRO\n9 - LPCUMULUS\n10 - EDF_DEV04\n11 - GES_DISC\n12 - ASF\n13 - OMINRT\n14 - EUMETSAT\n15 - NCCS\n16 - NSIDCV0\n17 - PODAAC\n18 - LARC\n19 - USGS\n20 - SCIOPS\n21 - LANCEMODIS\n22 - CDDIS\n23 - JAXA\n24 - AU_AADC\n25 - ECHO10_OPS\n26 - LPDAAC_ECS\n27 - NSIDC_ECS\n28 - ORNL_DAAC\n29 - LM_FIRMS\n30 - SEDAC\n31 - LANCEAMSR2\n32 - NOAA_NCEI\n33 - USGS_LTA\n34 - GESDISCCLD\n35 - GHRSSTCWIC\n36 - ASIPS\n37 - ESDIS\n38 - POCLOUD\n39 - NSIDC_CPRD\n40 - ORNL_CLOUD\n41 - FEDEO\n42 - XYZ_PROV\n43 - GHRC_DAAC\n44 - CSDA\n45 - NRSCC\n46 - CEOS_EXTRA\n47 - MOPITT\n48 - GHRC_CLOUD\n49 - LPCLOUD\n50 - CCMEO",
    "crumbs": [
      "Tutorials",
      "02. Data Discovery with CMR-STAC API"
    ]
  },
  {
    "objectID": "tutorials/02_Data_Discovery_CMR-STAC_API.html#connect-to-the-lpcloud-providerstac-catalog",
    "href": "tutorials/02_Data_Discovery_CMR-STAC_API.html#connect-to-the-lpcloud-providerstac-catalog",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "Connect to the LPCLOUD Provider/STAC Catalog",
    "text": "Connect to the LPCLOUD Provider/STAC Catalog\nFor this next step we need the provider title (e.g., LPCLOUD) from above. We will add the provider to the end of the CMR-STAC API URL (i.e., https://cmr.earthdata.nasa.gov/stac/) to connect to the LPCLOUD STAC Catalog.\n\ncatalog = Client.open(f'{STAC_URL}/LPCLOUD/')\n\nSince we are using a dedicated client (i.e., pystac-client.Client) to connect to our STAC Provider Catalog, we will have access to some useful internal methods and functions (e.g., get_children() or get_all_items()) we can use to get information from these objects.",
    "crumbs": [
      "Tutorials",
      "02. Data Discovery with CMR-STAC API"
    ]
  },
  {
    "objectID": "tutorials/02_Data_Discovery_CMR-STAC_API.html#list-stac-collections",
    "href": "tutorials/02_Data_Discovery_CMR-STAC_API.html#list-stac-collections",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "List STAC Collections",
    "text": "List STAC Collections\nWe’ll create a products variable to view deeper in the STAC Catalog.\n\nproducts = [c for c in catalog.get_children()]\n\n\nPrint one of the STAC Collection records\nTo view the products variable we just created, let’s look at one entry as a dictionary.\n\nproducts[1].to_dict()\n\n{'type': 'Collection',\n 'id': 'HLSL30.v2.0',\n 'stac_version': '1.0.0',\n 'description': 'The Harmonized Landsat and Sentinel-2 (HLS) project provides consistent surface reflectance (SR) and top of atmosphere (TOA) brightness data from the Operational Land Imager (OLI) aboard the joint NASA/USGS Landsat 8 satellite and the Multi-Spectral Instrument (MSI) aboard Europe’s Copernicus Sentinel-2A and Sentinel-2B satellites. The combined measurement enables global observations of the land every 2–3 days at 30-meter (m) spatial resolution. The HLS project uses a set of algorithms to obtain seamless products from OLI and MSI that include atmospheric correction, cloud and cloud-shadow masking, spatial co-registration and common gridding, illumination and view angle normalization, and spectral bandpass adjustment.\\r\\n\\r\\nThe HLSL30 product provides 30-m Nadir Bidirectional Reflectance Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is derived from Landsat 8 OLI data products. The HLSS30 and HLSL30 products are gridded to the same resolution and Military Grid Reference System ([MGRS](https://hls.gsfc.nasa.gov/products-description/tiling-system/)) tiling system, and thus are “stackable” for time series analysis.\\r\\n\\r\\nThe HLSL30 product is provided in Cloud Optimized GeoTIFF (COG) format, and each band is distributed as a separate file. There are 11 bands included in the HLSL30 product along with one quality assessment (QA) band and four angle bands. See the User Guide for a more detailed description of the individual bands provided in the HLSL30 product.',\n 'links': [{'rel': &lt;RelType.ROOT: 'root'&gt;,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/',\n   'type': &lt;MediaType.JSON: 'application/json'&gt;},\n  {'rel': 'items',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/items',\n   'type': 'application/json',\n   'title': 'Granules in this collection'},\n  {'rel': 'about',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2021957657-LPCLOUD.html',\n   'type': 'text/html',\n   'title': 'HTML metadata for collection'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2021957657-LPCLOUD.json',\n   'type': 'application/json',\n   'title': 'CMR JSON metadata for collection'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2020',\n   'type': 'application/json',\n   'title': '2020 catalog'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2021',\n   'type': 'application/json',\n   'title': '2021 catalog'},\n  {'rel': &lt;RelType.SELF: 'self'&gt;,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0',\n   'type': &lt;MediaType.JSON: 'application/json'&gt;},\n  {'rel': &lt;RelType.PARENT: 'parent'&gt;,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/',\n   'type': &lt;MediaType.JSON: 'application/json'&gt;}],\n 'stac_extensions': [],\n 'title': 'HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0',\n 'extent': {'spatial': {'bbox': [[-180, -90, 180, 90]]},\n  'temporal': {'interval': [['2013-05-01T00:00:00Z', None]]}},\n 'license': 'not-provided'}\n\n\n\n\nPrint the STAC Collection ids with their title\nIn the above output, id and title are two elements of interest that we can print for all products using a for loop.\n\nfor p in products: \n    print(f\"{p.id}: {p.title}\")\n\nASTGTM.v003: ASTER Global Digital Elevation Model V003\nHLSL30.v2.0: HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0\nHLSL30.v1.5: HLS Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30 m V1.5\nHLSS30.v1.5: HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30 m V1.5\nHLSS30.v2.0: HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0",
    "crumbs": [
      "Tutorials",
      "02. Data Discovery with CMR-STAC API"
    ]
  },
  {
    "objectID": "tutorials/02_Data_Discovery_CMR-STAC_API.html#search-for-granulesstac-items---set-up-query-parameters-to-submit-to-the-cmr-stac-api",
    "href": "tutorials/02_Data_Discovery_CMR-STAC_API.html#search-for-granulesstac-items---set-up-query-parameters-to-submit-to-the-cmr-stac-api",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "Search for Granules/STAC Items - Set up query parameters to submit to the CMR-STAC API",
    "text": "Search for Granules/STAC Items - Set up query parameters to submit to the CMR-STAC API\nWe will define our ROI using a geojson file containing a small polygon feature in western Nebraska, USA. The geojson file is found in the ~/data directory. We’ll also specify the data collections and a time range for our example.\n\nRead in a geojson file\nReading in a geojson file with geopandas will return the geometry of our polygon (our ROI).\nNOTE: If you are running the notebook from the tutorials-templates directory, please use the following path to connect to the geojson file: “../tutorials/data/ne_w_agfields.geojson”\n\nfield = geopandas.read_file('./data/ne_w_agfields.geojson')\nfield\n\n\n\n\n\n\n\n\ngeometry\n\n\n\n\n0\nPOLYGON ((-101.67272 41.04754, -101.65345 41.0...\n\n\n\n\n\n\n\n\n\nVisualize contents of geojson file\nWe can use that geometry to visualize the polygon: here, a square. But wait for it –\n\nfieldShape = field['geometry'][0]\nfieldShape\n\n\n\n\n\n\n\n\nWe can plot the polygon using the geoviews package that we imported as gv with ‘bokeh’ and ‘matplotlib’ extensions. The following has reasonable width, height, color, and line widths to view our polygon when it is overlayed on a base tile map.\n\nbase = gv.tile_sources.EsriImagery.opts(width=650, height=500)\nfarmField = gv.Polygons(fieldShape).opts(line_color='yellow', line_width=10, color=None)\nbase * farmField\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe will now start to specify the search criteria we are interested in, i.e, the date range, the ROI, and the data collections, that we will pass to the STAC API.\n\n\nExtract the coordinates for the ROI\n\nroi = json.loads(field.to_json())['features'][0]['geometry']\nroi\n\n{'type': 'Polygon',\n 'coordinates': [[[-101.67271614074707, 41.04754380304359],\n   [-101.65344715118408, 41.04754380304359],\n   [-101.65344715118408, 41.06213891056728],\n   [-101.67271614074707, 41.06213891056728],\n   [-101.67271614074707, 41.04754380304359]]]}\n\n\nSo, what just happen there? Let’s take a quick detour to break it down.\n\n\n\nSpecify date range\nNext up is to specify our date range using ISO_8601 date formatting.\n\n#date_range = \"2021-05-01T00:00:00Z/2021-08-30T23:59:59Z\"    # closed interval\n#date_range = \"2021-05-01T00:00:00Z/..\"                      # open interval - does not currently work with the CMR-STAC API\ndate_range = \"2021-05/2021-08\"\n\n\n\nSpecify the STAC Collections\nSTAC Collection is synonomous with what we usually consider a NASA data product. Desired STAC Collections are submitted to the search API as a list containing the collection id. We can use the ids that we printed from our products for loop above. Let’s focus on S30 and L30 collections.\n\ncollections = ['HLSL30.v2.0', 'HLSS30.v2.0']\ncollections\n\n['HLSL30.v2.0', 'HLSS30.v2.0']",
    "crumbs": [
      "Tutorials",
      "02. Data Discovery with CMR-STAC API"
    ]
  },
  {
    "objectID": "tutorials/02_Data_Discovery_CMR-STAC_API.html#search-the-cmr-stac-api-with-our-search-criteria",
    "href": "tutorials/02_Data_Discovery_CMR-STAC_API.html#search-the-cmr-stac-api-with-our-search-criteria",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "Search the CMR-STAC API with our search criteria",
    "text": "Search the CMR-STAC API with our search criteria\nNow we can put all our search criteria together using catalog.search from the pystac_client package.\n\nsearch = catalog.search(\n    collections=collections,\n    intersects=roi,\n    datetime=date_range,\n    limit=100\n)\n\n\nPrint out how many STAC Items match our search query\n\nsearch.matched()\n\n113\n\n\nWe now have a search object containing the STAC Items that matched our query. Now, let’s pull out all of the STAC Items (as a PySTAC ItemCollection object) and explore the contents (i.e., the STAC Items)\n\nitem_collection = search.get_all_items()\n\nLet’s list some of the Items from our pystac item_collection:\n\nlist(item_collection)[0:5]\n\n[&lt;Item id=HLS.L30.T13TGF.2021124T173013.v2.0&gt;,\n &lt;Item id=HLS.L30.T14TKL.2021124T173013.v2.0&gt;,\n &lt;Item id=HLS.S30.T14TKL.2021125T172901.v2.0&gt;,\n &lt;Item id=HLS.S30.T13TGF.2021125T172901.v2.0&gt;,\n &lt;Item id=HLS.S30.T14TKL.2021128T173901.v2.0&gt;]\n\n\nWe can view a single Item as a dictionary, as we did above with STAC Collections/products.\n\nitem_collection[0].to_dict()\n\n{'type': 'Feature',\n 'stac_version': '1.0.0',\n 'id': 'HLS.L30.T13TGF.2021124T173013.v2.0',\n 'properties': {'datetime': '2021-05-04T17:30:13.428000Z',\n  'start_datetime': '2021-05-04T17:30:13.428Z',\n  'end_datetime': '2021-05-04T17:30:37.319Z',\n  'eo:cloud_cover': 36},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-101.5423534, 40.5109845],\n    [-101.3056118, 41.2066375],\n    [-101.2894253, 41.4919436],\n    [-102.6032964, 41.5268623],\n    [-102.638891, 40.5386175],\n    [-101.5423534, 40.5109845]]]},\n 'links': [{'rel': 'self',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/items/HLS.L30.T13TGF.2021124T173013.v2.0'},\n  {'rel': 'parent',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0'},\n  {'rel': 'collection',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0'},\n  {'rel': &lt;RelType.ROOT: 'root'&gt;,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/',\n   'type': &lt;MediaType.JSON: 'application/json'&gt;},\n  {'rel': 'provider', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2144020713-LPCLOUD.json'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2144020713-LPCLOUD.umm_json'}],\n 'assets': {'B11': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B11.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B11.tif'},\n  'B07': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B07.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B07.tif'},\n  'SAA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.SAA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.SAA.tif'},\n  'B06': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B06.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B06.tif'},\n  'B09': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B09.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B09.tif'},\n  'B10': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B10.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B10.tif'},\n  'VZA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.VZA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.VZA.tif'},\n  'SZA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.SZA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.SZA.tif'},\n  'B01': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B01.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B01.tif'},\n  'VAA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.VAA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.VAA.tif'},\n  'B05': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B05.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B05.tif'},\n  'B02': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B02.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B02.tif'},\n  'Fmask': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.Fmask.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.Fmask.tif'},\n  'B03': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B03.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B03.tif'},\n  'B04': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B04.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B04.tif'},\n  'browse': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.jpg',\n   'type': 'image/jpeg',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.jpg'},\n  'metadata': {'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2144020713-LPCLOUD.xml',\n   'type': 'application/xml'}},\n 'bbox': [-102.638891, 40.510984, -101.289425, 41.526862],\n 'stac_extensions': ['https://stac-extensions.github.io/eo/v1.0.0/schema.json'],\n 'collection': 'HLSL30.v2.0'}",
    "crumbs": [
      "Tutorials",
      "02. Data Discovery with CMR-STAC API"
    ]
  },
  {
    "objectID": "tutorials/02_Data_Discovery_CMR-STAC_API.html#filtering-stac-items",
    "href": "tutorials/02_Data_Discovery_CMR-STAC_API.html#filtering-stac-items",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "Filtering STAC Items",
    "text": "Filtering STAC Items\nWhile the CMR-STAC API is a powerful search and discovery utility, it is still maturing and currently does not have the full gamut of filtering capabilities that the STAC API specification allows for. Hence, additional filtering is required if we want to filter by a property, for example cloud cover. Below we will loop through and filter the item_collection by a specified cloud cover as well as extract the band we’d need to do an Enhanced Vegetation Index (EVI) calculation for a future analysis.\nWe’ll make a cloudcover variable where we will set the maximum allowable cloud cover and extract the band links for those Items that match or are less than the max cloud cover.\n\ncloudcover = 25\n\nWe will also specify the STAC Assets (i.e., bands/layers) of interest for both the S30 and L30 collections (also in our collections variable above).\nIn this hypothetical workflow, we’ll extract the bands needed to calculate an enhanced vegetation index (EVI). Thus, the band needed include red, near infrared (NIR), and blue. We’ll also extract a quality band (i.e., Fmask) that we’d eventually use to perform per-pixel quality filtering.\nNotice that the band ids are in some case not one-to-one between the S30 and the L30 product. This is evident in the NIR band for each product where S30’s NIR band id is B8A and L30’s is B05. Note, the S30 product has an additional NIR band with a band id of B08, but the spectral ranges between B8A and B05 are more closely aligned. Visit the HLS Overview page to learn more about HLS spectral bands.\n\ns30_bands = ['B8A', 'B04', 'B02', 'Fmask']    # S30 bands for EVI calculation and quality filtering -&gt; NIR, RED, BLUE, Quality \nl30_bands = ['B05', 'B04', 'B02', 'Fmask']    # L30 bands for EVI calculation and quality filtering -&gt; NIR, RED, BLUE, Quality \n\nAnd now to loop through and filter the item_collection by cloud cover and bands:\n\nevi_band_links = []\n\nfor i in item_collection:\n    if i.properties['eo:cloud_cover'] &lt;= cloudcover:\n        if i.collection_id == 'HLSS30.v2.0':\n            #print(i.properties['eo:cloud_cover'])\n            evi_bands = s30_bands\n        elif i.collection_id == 'HLSL30.v2.0':\n            #print(i.properties['eo:cloud_cover'])\n            evi_bands = l30_bands\n\n        for a in i.assets:\n            if any(b==a for b in evi_bands):\n                evi_band_links.append(i.assets[a].href)\n\nThe filtering done in the previous steps produces a list of links to STAC Assets. Let’s print out the first ten links.\n\nevi_band_links[:10]\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B05.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.Fmask.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B02.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.B02.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.B05.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.Fmask.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T14TKL.2021133T173859.v2.0/HLS.S30.T14TKL.2021133T173859.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T14TKL.2021133T173859.v2.0/HLS.S30.T14TKL.2021133T173859.v2.0.B8A.tif']\n\n\nNOTE that HLS data is mapped to the Universal Transverse Mercator (UTM) projection and is tiled using the Sentinel-2 Military Grid Reference System (MGRS) UTM grid. Notice that in the list of links we have multiple tiles, i.e. T14TKL & T13TGF, that intersect with our region of interest. In this case, these two tiles represent neighboring UTM zones. The tiles can be discern from the file name, which is the last element in a link (far right) following the last forward slash (/) - e.g., HLS.L30.T14TKL.2021133T172406.v1.5.B04.tif. The figure below explains where to find the tile/UTM zone from the file name.\n\nWe will now split the list of links into separate logical sub-lists.",
    "crumbs": [
      "Tutorials",
      "02. Data Discovery with CMR-STAC API"
    ]
  },
  {
    "objectID": "tutorials/02_Data_Discovery_CMR-STAC_API.html#split-data-links-list-into-logical-groupings",
    "href": "tutorials/02_Data_Discovery_CMR-STAC_API.html#split-data-links-list-into-logical-groupings",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "Split Data Links List into Logical Groupings",
    "text": "Split Data Links List into Logical Groupings\nWe have a list of links to data assets that meet our search and filtering criteria. Below we’ll split our list from above into lists first by tile/UTM zone and then further by individual bands bands. The commands that follow will do the splitting with python routines.\n\nSplit by UTM tile specified in the file name (e.g., T14TKL & T13TGF)\n\ntile_dicts = defaultdict(list)    # https://stackoverflow.com/questions/26367812/appending-to-list-in-python-dictionary\n\n\nfor l in evi_band_links:\n    tile = l.split('.')[-6]\n    tile_dicts[tile].append(l)\n\n\nPrint dictionary keys and values, i.e. the data links\n\ntile_dicts.keys()\n\ndict_keys(['T13TGF', 'T14TKL'])\n\n\n\ntile_dicts['T13TGF'][:5]\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B05.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.Fmask.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B02.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021133T173859.v2.0/HLS.S30.T13TGF.2021133T173859.v2.0.B8A.tif']\n\n\nNow we will create a separate list of data links for each tile\n\ntile_links_T14TKL = tile_dicts['T14TKL']\ntile_links_T13TGF = tile_dicts['T13TGF']\n\n\n\nPrint band/layer links for HLS tile T13TGF\n\n# tile_links_T13TGF[:10]\n\n\n\n\nSplit the links by band\n\nbands_dicts = defaultdict(list)\n\n\nfor b in tile_links_T13TGF:\n    band = b.split('.')[-2]\n    bands_dicts[band].append(b)\n\n\nbands_dicts.keys()\n\ndict_keys(['B04', 'B05', 'Fmask', 'B02', 'B8A'])\n\n\n\nbands_dicts['B04']\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021133T173859.v2.0/HLS.S30.T13TGF.2021133T173859.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021140T173021.v2.0/HLS.L30.T13TGF.2021140T173021.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021140T172859.v2.0/HLS.S30.T13TGF.2021140T172859.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021145T172901.v2.0/HLS.S30.T13TGF.2021145T172901.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021155T172901.v2.0/HLS.S30.T13TGF.2021155T172901.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021156T173029.v2.0/HLS.L30.T13TGF.2021156T173029.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021158T173901.v2.0/HLS.S30.T13TGF.2021158T173901.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021163T173909.v2.0/HLS.S30.T13TGF.2021163T173909.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021165T172422.v2.0/HLS.L30.T13TGF.2021165T172422.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021165T172901.v2.0/HLS.S30.T13TGF.2021165T172901.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021173T173909.v2.0/HLS.S30.T13TGF.2021173T173909.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021185T172901.v2.0/HLS.S30.T13TGF.2021185T172901.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021188T173037.v2.0/HLS.L30.T13TGF.2021188T173037.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021190T172859.v2.0/HLS.S30.T13TGF.2021190T172859.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021193T173909.v2.0/HLS.S30.T13TGF.2021193T173909.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021198T173911.v2.0/HLS.S30.T13TGF.2021198T173911.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021200T172859.v2.0/HLS.S30.T13TGF.2021200T172859.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021203T173909.v2.0/HLS.S30.T13TGF.2021203T173909.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021204T173042.v2.0/HLS.L30.T13TGF.2021204T173042.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021208T173911.v2.0/HLS.S30.T13TGF.2021208T173911.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021210T172859.v2.0/HLS.S30.T13TGF.2021210T172859.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021215T172901.v2.0/HLS.S30.T13TGF.2021215T172901.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021218T173911.v2.0/HLS.S30.T13TGF.2021218T173911.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021220T173049.v2.0/HLS.L30.T13TGF.2021220T173049.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021220T172859.v2.0/HLS.S30.T13TGF.2021220T172859.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021223T173909.v2.0/HLS.S30.T13TGF.2021223T173909.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021228T173911.v2.0/HLS.S30.T13TGF.2021228T173911.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021229T172441.v2.0/HLS.L30.T13TGF.2021229T172441.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021230T172859.v2.0/HLS.S30.T13TGF.2021230T172859.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021235T172901.v2.0/HLS.S30.T13TGF.2021235T172901.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021243T173859.v2.0/HLS.S30.T13TGF.2021243T173859.v2.0.B04.tif']",
    "crumbs": [
      "Tutorials",
      "02. Data Discovery with CMR-STAC API"
    ]
  },
  {
    "objectID": "tutorials/02_Data_Discovery_CMR-STAC_API.html#save-links-to-a-text-file",
    "href": "tutorials/02_Data_Discovery_CMR-STAC_API.html#save-links-to-a-text-file",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "Save links to a text file",
    "text": "Save links to a text file\nTo complete this exercise, we will save the individual link lists as separate text files with descriptive names.\nNOTE: If you are running the notebook from the tutorials-templates directory, please use the following path to write to the data directory: “../tutorials/data/{name}”\n\nWrite links from CMR-STAC API to a file\n\nfor k, v in bands_dicts.items():\n    name = (f'HTTPS_T13TGF_{k}_Links.txt')\n    with open(f'./data/{name}', 'w') as f:    # use ../tutorials/data/{name} as your path if running the notebook from \"tutorials-template\"\n        for l in v:\n            f.write(f\"{l}\" + '\\n')\n\n\n\nWrite links to file for S3 access\n\nfor k, v in bands_dicts.items():\n    name = (f'S3_T13TGF_{k}_Links.txt')\n    with open(f'./data/{name}', 'w') as f:    # use ../tutorials/data/{name} as your path if running the notebook from \"tutorials-template\"\n        for l in v:\n            s3l = l.replace('https://data.lpdaac.earthdatacloud.nasa.gov/', 's3://')\n            f.write(f\"{s3l}\" + '\\n')",
    "crumbs": [
      "Tutorials",
      "02. Data Discovery with CMR-STAC API"
    ]
  },
  {
    "objectID": "tutorials/02_Data_Discovery_CMR-STAC_API.html#resources",
    "href": "tutorials/02_Data_Discovery_CMR-STAC_API.html#resources",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "Resources",
    "text": "Resources\n\nSTAC Specification Webpage\nSTAC API Documentation\nCMR-STAC API Github\nPySTAC Client Documentation\nhttps://stackoverflow.com/questions/26367812/appending-to-list-in-python-dictionary\nGeopandas\nHLS Overview",
    "crumbs": [
      "Tutorials",
      "02. Data Discovery with CMR-STAC API"
    ]
  },
  {
    "objectID": "tutorials/07_Harmony_Subsetting.html",
    "href": "tutorials/07_Harmony_Subsetting.html",
    "title": "07. Data Subsetting and Transformation Services in the Cloud",
    "section": "",
    "text": "Exercise: 40 minutes",
    "crumbs": [
      "Tutorials",
      "07. Data Subsetting and Transformation Services in the Cloud"
    ]
  },
  {
    "objectID": "tutorials/07_Harmony_Subsetting.html#using-the-harmony-py-library-to-access-customized-data-from-nasa-earthdata",
    "href": "tutorials/07_Harmony_Subsetting.html#using-the-harmony-py-library-to-access-customized-data-from-nasa-earthdata",
    "title": "07. Data Subsetting and Transformation Services in the Cloud",
    "section": "",
    "text": "Exercise: 40 minutes",
    "crumbs": [
      "Tutorials",
      "07. Data Subsetting and Transformation Services in the Cloud"
    ]
  },
  {
    "objectID": "tutorials/07_Harmony_Subsetting.html#summary",
    "href": "tutorials/07_Harmony_Subsetting.html#summary",
    "title": "07. Data Subsetting and Transformation Services in the Cloud",
    "section": "Summary",
    "text": "Summary\nWe have already explored direct access to the NASA EOSDIS archive in the cloud via the Amazon Simple Storage Service (S3) by using the Common Metadata Repository (CMR) to search for granule locations. In addition to directly accessing the files archived and distributed by each of the NASA DAACs, many datasets also support services that allow us to customize the data via subsetting, reformatting, reprojection, and other transformations.\nThis tutorial demonstrates how to find, request, and use customized data from a new ecosystem of services operating within the NASA Earthdata Cloud: NASA Harmony.\n\nBenefits\nBut first, why use this option when we’ve already learned how to access data directly from the NASA Earthdata Cloud?\n\nConsistent access patterns to EOSDIS holdings make cross-data center data access easier\nData reduction services allow us to request only the data we want, in the format and projection we want\nAnalysis Ready Data and cloud access will help reduce time-to-science\nCommunity Development helps reduce the barriers for re-use of code and sharing of domain knowledge\n\n\n\n\nData file filtering and subsetting\n\n\nSee more on the Earthdata Harmony landing page, including documentation on the Harmony API itself.\n\n\nObjectives\n\nConceptualize the data transformation service types and offerings provided by NASA Earthdata, including Harmony.\nPractice skills learned from the introductory CMR tutorial to discover what access and service options exist for a given data set, as well as variable metadata.\nUtilize the Harmony-py library to request subsetted MODIS L2 Sea Surface Temperature data over the Gulf of Mexico.\nRead Harmony subsetted outputs directly into xarray. ___",
    "crumbs": [
      "Tutorials",
      "07. Data Subsetting and Transformation Services in the Cloud"
    ]
  },
  {
    "objectID": "tutorials/07_Harmony_Subsetting.html#import-packages",
    "href": "tutorials/07_Harmony_Subsetting.html#import-packages",
    "title": "07. Data Subsetting and Transformation Services in the Cloud",
    "section": "Import Packages",
    "text": "Import Packages\n\nfrom harmony import BBox, Client, Collection, Request, LinkType\nfrom harmony.config import Environment\nimport requests\nfrom pprint import pprint\nimport datetime as dt\nimport s3fs\nimport xarray as xr",
    "crumbs": [
      "Tutorials",
      "07. Data Subsetting and Transformation Services in the Cloud"
    ]
  },
  {
    "objectID": "tutorials/07_Harmony_Subsetting.html#discover-service-options-for-a-given-data-set",
    "href": "tutorials/07_Harmony_Subsetting.html#discover-service-options-for-a-given-data-set",
    "title": "07. Data Subsetting and Transformation Services in the Cloud",
    "section": "Discover service options for a given data set",
    "text": "Discover service options for a given data set\n\nFirst, what do we mean by a “service”?\nIn the context of NASA Earthdata, we are usually referring to a service as any data transformation or customization process that packages or delivers data in a way that makes it easier to work with compared to how the data are natively archived at NASA EOSDIS. Basic customization options may include: * Subsetting (cropping) the data by: * Variable * Spatial boundary, * Temporal range * Reformatting * For example: From NetCDF-4 to Cloud Optimized GeoTIFF * Reprojection and/or Resampling * For example: From Sinusoidal to Polar Stereographic * Mosaicking * Aggregating\nA few main types or pathways for services that are commonly supported across the NASA DAACs include: * NASA Global Imagery Browse Service * Web services providing imagery, much of which is updated daily, to broaden accessibility of NASA EOSDIS data to the media and public. * Web Map Tile Service (WMTS) * Tiled Web Map Service (TWMS) * Web Map Service (WMS) * Keyhole Markup Language (KML) * Geospatial Data Abstraction Library (GDAL) * OPeNDAP * The Open-source Project for a Network Data Access Protocol is a NASA community standard DAP that provides a simple way to access and work with data over the internet. OPeNDAP’s client/server software allows us to subset and reformat data using an internet browser, command line interface, and other applications. * Harmony * In the most basic sense, Harmony is an Application Programming Interface, or API, allowing us to request customization options described above, which are then processed and returned as file outputs. Harmony helps to reduce pre-processing steps so we can spend less time preparing the data, and more time doing science.\nNote: These service offerings are unique to each NASA EOSDIS dataset.\nWhy is this?\nDue to varying levels of service, cloud migration status, and unique characteristics of the datasets themselves, not all service options are provided for all datasets. Therefore it is important to first explore a given dataset’s metadata to discover what service options are provided.\nLet’s utilize the CMR API skills we learned on Day 1 to inspect service metadata:\n\nurl = 'https://cmr.earthdata.nasa.gov/search'\n\nWe want to search by collection to inspect the access and service options that exist:\n\ncollection_url = f'{url}/{\"collections\"}'\n\nIn the CMR introduction tutorial, we explored cloud-hosted collections from different DAAC providers, and identified the CMR concept-id for a given data set id (also referred to as a short_name).\nHere we are jumping ahead and already know the concept_id we are interested in, by browsing cloud-hosted datasets from PO.DAAC in Earthdata Search: https://search.earthdata.nasa.gov/portal/podaac-cloud/search.\nWe are going to focus on MODIS_A-JPL-L2P-v2019.0: GHRSST Level 2P Global Sea Surface Skin Temperature from the Moderate Resolution Imaging Spectroradiometer (MODIS) on the NASA Aqua satellite (GDS2). Let’s first save this as a variable that we can use later on once we request data from Harmony.\n\nshort_name= 'MODIS_A-JPL-L2P-v2019.0'\nconcept_id = 'C1940473819-POCLOUD'\n\nWe will view the top-level metadata for this collection to see what additional service and variable metadata exist.\n\nresponse = requests.get(collection_url, \n                        params={\n                            'concept_id': concept_id,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nresponse = response.json()\n\nPrint the response:\n\npprint(response)\n\n{'feed': {'entry': [{'archive_center': 'NASA/JPL/PODAAC',\n                     'associations': {'services': ['S1962070864-POCLOUD',\n                                                   'S2004184019-POCLOUD'],\n                                      'tools': ['TL2108419875-POCLOUD',\n                                                'TL2092786348-POCLOUD'],\n                                      'variables': ['V1997812737-POCLOUD',\n                                                    'V1997812697-POCLOUD',\n                                                    'V2112014688-POCLOUD',\n                                                    'V1997812756-POCLOUD',\n                                                    'V1997812688-POCLOUD',\n                                                    'V1997812670-POCLOUD',\n                                                    'V1997812724-POCLOUD',\n                                                    'V2112014684-POCLOUD',\n                                                    'V1997812701-POCLOUD',\n                                                    'V1997812681-POCLOUD',\n                                                    'V2112014686-POCLOUD',\n                                                    'V1997812663-POCLOUD',\n                                                    'V1997812676-POCLOUD',\n                                                    'V1997812744-POCLOUD',\n                                                    'V1997812714-POCLOUD']},\n                     'boxes': ['-90 -180 90 180'],\n                     'browse_flag': True,\n                     'collection_data_type': 'SCIENCE_QUALITY',\n                     'coordinate_system': 'CARTESIAN',\n                     'data_center': 'POCLOUD',\n                     'dataset_id': 'GHRSST Level 2P Global Sea Surface Skin '\n                                   'Temperature from the Moderate Resolution '\n                                   'Imaging Spectroradiometer (MODIS) on the '\n                                   'NASA Aqua satellite (GDS2)',\n                     'has_formats': True,\n                     'has_spatial_subsetting': True,\n                     'has_temporal_subsetting': True,\n                     'has_transforms': False,\n                     'has_variables': True,\n                     'id': 'C1940473819-POCLOUD',\n                     'links': [{'href': 'https://podaac.jpl.nasa.gov/Podaac/thumbnails/MODIS_A-JPL-L2P-v2019.0.jpg',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n                               {'href': 'https://github.com/podaac/data-readers',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://podaac-tools.jpl.nasa.gov/drive/files/OceanTemperature/ghrsst/docs/GDS20r5.pdf',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://ghrsst.jpl.nasa.gov',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://oceancolor.gsfc.nasa.gov/atbd/sst/flag/',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://oceancolor.gsfc.nasa.gov/reprocessing/r2019/sst/',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://oceancolor.gsfc.nasa.gov/atbd/sst4/',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://modis.gsfc.nasa.gov/data/atbd/atbd_mod25.pdf',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://oceancolor.gsfc.nasa.gov/atbd/sst/',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'http://www.ghrsst.org',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://podaac.jpl.nasa.gov/forum/viewforum.php?f=18&sid=e2d67e5a01815fc6e39fcd2087ed8bc8',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://podaac.jpl.nasa.gov/CitingPODAAC',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://cmr.earthdata.nasa.gov/virtual-directory/collections/C1940473819-POCLOUD',\n                                'hreflang': 'en-US',\n                                'length': '75.0MB',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n                               {'href': 'https://github.com/podaac/tutorials/blob/master/notebooks/MODIS_L2P_SST_DataCube.ipynb',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n                               {'href': 'https://search.earthdata.nasa.gov/search/granules?p=C1940473819-POCLOUD',\n                                'hreflang': 'en-US',\n                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'}],\n                     'online_access_flag': True,\n                     'orbit_parameters': {'inclination_angle': '98.1',\n                                          'number_of_orbits': '1.0',\n                                          'period': '98.4',\n                                          'swath_width': '2330.0'},\n                     'organizations': ['NASA/JPL/PODAAC'],\n                     'original_format': 'UMM_JSON',\n                     'platforms': ['Aqua'],\n                     'processing_level_id': '2',\n                     'service_features': {'esi': {'has_formats': False,\n                                                  'has_spatial_subsetting': False,\n                                                  'has_temporal_subsetting': False,\n                                                  'has_transforms': False,\n                                                  'has_variables': False},\n                                          'harmony': {'has_formats': True,\n                                                      'has_spatial_subsetting': True,\n                                                      'has_temporal_subsetting': True,\n                                                      'has_transforms': False,\n                                                      'has_variables': True},\n                                          'opendap': {'has_formats': True,\n                                                      'has_spatial_subsetting': True,\n                                                      'has_temporal_subsetting': True,\n                                                      'has_transforms': False,\n                                                      'has_variables': True}},\n                     'short_name': 'MODIS_A-JPL-L2P-v2019.0',\n                     'summary': 'NASA produces skin sea surface temperature '\n                                '(SST) products from the Infrared (IR) '\n                                'channels of the Moderate-resolution Imaging '\n                                'Spectroradiometer (MODIS) onboard the Aqua '\n                                'satellite. Aqua was launched by NASA on May '\n                                '4, 2002, into a sun synchronous, polar orbit '\n                                'with a daylight ascending node at 1:30 pm, '\n                                'formation flying in the A-train with other '\n                                'Earth Observation Satellites (EOS), to study '\n                                'the global dynamics of the Earth atmosphere, '\n                                'land and oceans. MODIS captures data in 36 '\n                                'spectral bands at a variety of spatial '\n                                'resolutions.  Two SST products can be present '\n                                'in these files. The first is a skin SST '\n                                'produced for both day and night (NSST) '\n                                'observations, derived from the long wave IR '\n                                '11 and 12 micron wavelength channels, using a '\n                                'modified nonlinear SST algorithm intended to '\n                                'provide continuity of SST derived from '\n                                'heritage and current NASA sensors. At night, '\n                                'a second SST product is generated using the '\n                                'mid-infrared 3.95 and 4.05 micron  wavelength '\n                                'channels which are unique to MODIS; the SST '\n                                'derived from these measurements is identified '\n                                'as SST4. The SST4 product has lower '\n                                'uncertainty, but due to sun glint can only be '\n                                'used at night. MODIS L2P SST data have a 1 km '\n                                'spatial resolution at nadir and are stored in '\n                                '288 five minute granules per day. Full global '\n                                'coverage is obtained every two days, with '\n                                'coverage poleward of 32.3 degree being '\n                                'complete each day.  The production of MODIS '\n                                'L2P SST files is part of the Group for High '\n                                'Resolution Sea Surface Temperature (GHRSST) '\n                                'project and is a joint collaboration between '\n                                'the NASA Jet Propulsion Laboratory (JPL), the '\n                                'NASA Ocean Biology Processing Group (OBPG), '\n                                'and the Rosenstiel School of Marine and '\n                                'Atmospheric Science (RSMAS). Researchers at '\n                                'RSMAS are responsible for SST algorithm '\n                                'development, error statistics and quality '\n                                'flagging, while the OBPG, as the NASA ground '\n                                'data system, is responsible for the '\n                                'production of daily MODIS ocean products. JPL '\n                                'acquires MODIS ocean granules from the OBPG '\n                                'and reformats them to the GHRSST L2P netCDF '\n                                'specification with complete metadata and '\n                                'ancillary variables, and distributes the data '\n                                'as the official Physical Oceanography Data '\n                                'Archive (PO.DAAC) for SST.  The R2019.0 '\n                                'supersedes the previous R2014.0 datasets '\n                                'which can be found at '\n                                'https://doi.org/10.5067/GHMDA-2PJ02',\n                     'time_start': '2002-07-04T00:00:00.000Z',\n                     'title': 'GHRSST Level 2P Global Sea Surface Skin '\n                              'Temperature from the Moderate Resolution '\n                              'Imaging Spectroradiometer (MODIS) on the NASA '\n                              'Aqua satellite (GDS2)',\n                     'updated': '2019-12-02T22:59:24.849Z',\n                     'version_id': '2019.0'}],\n          'id': 'https://cmr.earthdata.nasa.gov:443/search/collections.json?concept_id=C1940473819-POCLOUD',\n          'title': 'ECHO dataset metadata',\n          'updated': '2021-11-19T17:44:29.026Z'}}\n\n\n\n\nWhat do each of these service values mean?\n\nAssociations\n\nCMR is a large web of interconnected metadata “schemas”, including Collections, Granules, Services, Tools, and Variables. In this case, this collection is associated with two unique services, two tools, and several unique variables.\n\nTags\n\nThere are also tags that describe what service options exist at a high-level. In this case, we see that this dataset supports the ability to reformat, subset by space and time, as well as by variable. This is used in web applications like Earthdata Search to surface those customization options more readily.\n\nService Features\n\nIn this case, we see three separate “features” listed here: esi, Harmony, and OPeNDAP.\n\n\nWe will dig into more details on what Harmony offers for this dataset.\nFirst, we need to isolate the services returned for this dataset:\n\nservices = response['feed']['entry'][0]['associations']['services']\nprint(services)\n\n['S1962070864-POCLOUD', 'S2004184019-POCLOUD']\n\n\n\nservice_url = \"https://cmr.earthdata.nasa.gov/search/services\"\n\nInspect the first service returned. Now we’re going to search the services endpoint to view that individual service’s metadata, like we did with our dataset above. This time, we’re explicitly setting the format of the response to umm-json in the Accept Header in order to see detailed metadata about the service.\n\nservice_response = requests.get(service_url, \n                        params={\n                            'concept_id': services[0],\n                            },\n                        headers={\n                            'Accept': 'application/vnd.nasa.cmr.umm_results+json'\n                            }\n                       )\nservice_response = service_response.json()\n\nDetails about the service metadata record include the service options provided by the “backend” processor connected to Harmony, in this case the PODAAC Level 2 Cloud Subsetter:\n\npprint(service_response)\n\n{'hits': 1,\n 'items': [{'meta': {'concept-id': 'S1962070864-POCLOUD',\n                     'concept-type': 'service',\n                     'deleted': False,\n                     'format': 'application/vnd.nasa.cmr.umm+json',\n                     'native-id': 'POCLOUD_podaac_l2_cloud_subsetter',\n                     'provider-id': 'POCLOUD',\n                     'revision-date': '2021-11-02T22:57:03.597Z',\n                     'revision-id': 19,\n                     'user-id': 'podaaccloud'},\n            'umm': {'AccessConstraints': 'None',\n                    'Description': 'Endpoint for subsetting L2 Subsetter via '\n                                   'Harmony',\n                    'LongName': 'PODAAC Level 2 Cloud Subsetter',\n                    'MetadataSpecification': {'Name': 'UMM-S',\n                                              'URL': 'https://cdn.earthdata.nasa.gov/umm/service/v1.4',\n                                              'Version': '1.4'},\n                    'Name': 'PODAAC L2 Cloud Subsetter',\n                    'OperationMetadata': [{'OperationName': 'SPATIAL_SUBSETTING'},\n                                          {'OperationName': 'VARIABLE_SUBSETTING'},\n                                          {'OperationName': 'TEMPORAL_SUBSETTING'}],\n                    'ServiceKeywords': [{'ServiceCategory': 'EARTH SCIENCE '\n                                                            'SERVICES',\n                                         'ServiceTerm': 'SUBSETTING/SUPERSETTING',\n                                         'ServiceTopic': 'DATA MANAGEMENT/DATA '\n                                                         'HANDLING'}],\n                    'ServiceOptions': {'Subset': {'SpatialSubset': {'BoundingBox': {'AllowMultipleValues': False}},\n                                                  'TemporalSubset': {'AllowMultipleValues': False},\n                                                  'VariableSubset': {'AllowMultipleValues': True}},\n                                       'SupportedReformattings': [{'SupportedInputFormat': 'HDF5',\n                                                                   'SupportedOutputFormats': ['NETCDF-4']},\n                                                                  {'SupportedInputFormat': 'NETCDF-4',\n                                                                   'SupportedOutputFormats': ['NETCDF-4']}]},\n                    'ServiceOrganizations': [{'LongName': 'Physical '\n                                                          'Oceanography '\n                                                          'Distributed Active '\n                                                          'Archive Center, Jet '\n                                                          'Propulsion '\n                                                          'Laboratory, NASA',\n                                              'Roles': ['ORIGINATOR'],\n                                              'ShortName': 'NASA/JPL/PODAAC'}],\n                    'Type': 'Harmony',\n                    'URL': {'Description': 'PROJECT HOME PAGE',\n                            'URLValue': 'https://harmony.earthdata.nasa.gov'},\n                    'Version': '1.1.0'}}],\n 'took': 9}\n\n\n\n\nDiscover all datasets that support Harmony services\nInstead of searching for services on a known dataset of interest, we may want to discovery all available datasets that are supported for a given service. We can utilize GraphQL, which is a way for us to efficiently gain information across service and collection metadata so that we can print out all supported Harmony datasets. First, we need to specify a query string. Here we are asking to query all collections with service type “Harmony”, and to provide details on the service options attached to those services:\n\nquery = \"\"\"query {\n  collections(limit: 2000, serviceType: \"Harmony\") {\n    count\n    items {\n      shortName\n      conceptId\n      services {\n        count\n        items {\n          name\n          supportedReformattings\n          supportedInputProjections\n          supportedOutputProjections\n          serviceOptions\n        }\n      }\n      variables {\n        count\n      }\n    }\n  }\n}\"\"\"\n\nThis utilizes a different API endpoint to query CMR metdata using GraphQL. Here we set up another request, passing our query string above:\n\ngraphql_url = 'https://graphql.earthdata.nasa.gov/api'\n\ngraphql_response = requests.get(graphql_url,\n                        params={\"query\": query},\n                        headers={\n                            'Accept': 'application/json',\n                        }\n                       )\n\nA json response is returned that provides all collections with Harmony-supported services. We can then extract just the collectionshortName, conceptID, and the service names supported for each collection:\n\nservices = graphql_response.json()['data']['collections']['items']\n\nfor service in services:\n    print(service['shortName'], \",\", service['conceptId'])\n    for i in range(len(service['services']['items'])):\n        print(\"Services:\", service['services']['items'][i]['name'])\n\nECCO_L4_ATM_STATE_05DEG_DAILY_V4R4 , C1990404801-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_ATM_STATE_LLC0090GRID_DAILY_V4R4 , C1991543823-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_ATM_STATE_05DEG_MONTHLY_V4R4 , C1990404814-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_ATM_STATE_LLC0090GRID_MONTHLY_V4R4 , C1991543805-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OCEAN_BOLUS_STREAMFUNCTION_LLC0090GRID_DAILY_V4R4 , C1991543818-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OCEAN_BOLUS_STREAMFUNCTION_LLC0090GRID_MONTHLY_V4R4 , C1991543733-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_BOLUS_05DEG_DAILY_V4R4 , C1990404807-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_BOLUS_LLC0090GRID_DAILY_V4R4 , C1991543824-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_BOLUS_05DEG_MONTHLY_V4R4 , C1990404805-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_BOLUS_LLC0090GRID_MONTHLY_V4R4 , C1991543745-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_GEOMETRY_05DEG_V4R4 , C2013583732-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_GEOMETRY_LLC0090GRID_V4R4 , C2013557893-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_GMAP_TIME_SERIES_SNAPSHOT_V4R4 , C1991543729-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_GMAP_TIME_SERIES_SNAPSHOT_V4R4B , C2133160276-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_GMSL_TIME_SERIES_DAILY_V4R4 , C1991543819-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_GMSL_TIME_SERIES_MONTHLY_V4R4 , C1991543742-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OCEAN_3D_MIX_COEFFS_05DEG_V4R4 , C2013584708-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OCEAN_3D_MIX_COEFFS_LLC0090GRID_V4R4 , C2013583906-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_FRESH_FLUX_05DEG_DAILY_V4R4 , C1990404818-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_FRESH_FLUX_LLC0090GRID_DAILY_V4R4 , C1991543820-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_FRESH_FLUX_05DEG_MONTHLY_V4R4 , C1990404792-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_FRESH_FLUX_LLC0090GRID_MONTHLY_V4R4 , C1991543803-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_HEAT_FLUX_05DEG_DAILY_V4R4 , C1990404788-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_HEAT_FLUX_LLC0090GRID_DAILY_V4R4 , C1991543712-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_HEAT_FLUX_05DEG_MONTHLY_V4R4 , C1990404812-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_HEAT_FLUX_LLC0090GRID_MONTHLY_V4R4 , C1991543811-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_STRESS_05DEG_DAILY_V4R4 , C1990404808-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_STRESS_LLC0090GRID_DAILY_V4R4 , C1991543704-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_STRESS_05DEG_MONTHLY_V4R4 , C1990404796-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_STRESS_LLC0090GRID_MONTHLY_V4R4 , C1991543760-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OBP_05DEG_DAILY_V4R4 , C1990404797-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OBP_05DEG_DAILY_V4R4B , C2129192243-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OBP_LLC0090GRID_DAILY_V4R4 , C1991543737-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OBP_LLC0090GRID_DAILY_V4R4B , C2129195053-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OBP_05DEG_MONTHLY_V4R4 , C1990404791-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OBP_05DEG_MONTHLY_V4R4B , C2129193421-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OBP_LLC0090GRID_MONTHLY_V4R4 , C1991543806-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OBP_LLC0090GRID_MONTHLY_V4R4B , C2129197196-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OBP_LLC0090GRID_SNAPSHOT_V4R4 , C1991543804-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_DENS_STRAT_PRESS_05DEG_DAILY_V4R4 , C1990404793-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_DENS_STRAT_PRESS_LLC0090GRID_DAILY_V4R4 , C1991543727-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_DENS_STRAT_PRESS_05DEG_MONTHLY_V4R4 , C1990404798-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_DENS_STRAT_PRESS_LLC0090GRID_MONTHLY_V4R4 , C1991543735-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_MIXED_LAYER_DEPTH_05DEG_DAILY_V4R4 , C1990404810-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_MIXED_LAYER_DEPTH_LLC0090GRID_DAILY_V4R4 , C1991543734-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_MIXED_LAYER_DEPTH_05DEG_MONTHLY_V4R4 , C1990404819-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_MIXED_LAYER_DEPTH_LLC0090GRID_MONTHLY_V4R4 , C1991543741-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_TEMP_SALINITY_05DEG_DAILY_V4R4 , C1990404821-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_TEMP_SALINITY_LLC0090GRID_DAILY_V4R4 , C1991543736-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_TEMP_SALINITY_05DEG_MONTHLY_V4R4 , C1990404795-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_TEMP_SALINITY_LLC0090GRID_MONTHLY_V4R4 , C1991543728-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_TEMP_SALINITY_LLC0090GRID_SNAPSHOT_V4R4 , C1991543757-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OCEAN_3D_MOMENTUM_TEND_LLC0090GRID_DAILY_V4R4 , C1991543726-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OCEAN_3D_MOMENTUM_TEND_LLC0090GRID_MONTHLY_V4R4 , C1991543702-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OCEAN_3D_TEMPERATURE_FLUX_LLC0090GRID_DAILY_V4R4 , C1991543812-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OCEAN_3D_TEMPERATURE_FLUX_LLC0090GRID_MONTHLY_V4R4 , C1991543740-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OCEAN_3D_SALINITY_FLUX_LLC0090GRID_DAILY_V4R4 , C1991543814-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OCEAN_3D_SALINITY_FLUX_LLC0090GRID_MONTHLY_V4R4 , C1991543752-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OCEAN_3D_VOLUME_FLUX_LLC0090GRID_DAILY_V4R4 , C1991543699-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OCEAN_3D_VOLUME_FLUX_LLC0090GRID_MONTHLY_V4R4 , C1991543739-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OCEAN_VEL_05DEG_DAILY_V4R4 , C1990404811-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OCEAN_VEL_LLC0090GRID_DAILY_V4R4 , C1991543808-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OCEAN_VEL_05DEG_MONTHLY_V4R4 , C1990404823-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_OCEAN_VEL_LLC0090GRID_MONTHLY_V4R4 , C1991543732-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SBO_CORE_TIME_SERIES_SNAPSHOT_V4R4 , C1991543766-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SBO_CORE_TIME_SERIES_SNAPSHOT_V4R4B , C2133162585-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SSH_05DEG_DAILY_V4R4 , C1990404813-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SSH_05DEG_DAILY_V4R4B , C2129181904-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SSH_LLC0090GRID_DAILY_V4R4 , C1991543744-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SSH_LLC0090GRID_DAILY_V4R4B , C2129186341-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SSH_05DEG_MONTHLY_V4R4 , C1990404799-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SSH_05DEG_MONTHLY_V4R4B , C2129189405-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SSH_LLC0090GRID_MONTHLY_V4R4 , C1991543813-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SSH_LLC0090GRID_MONTHLY_V4R4B , C2129189870-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SSH_LLC0090GRID_SNAPSHOT_V4R4 , C1991543817-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SEA_ICE_CONC_THICKNESS_05DEG_DAILY_V4R4 , C1990404815-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SEA_ICE_CONC_THICKNESS_LLC0090GRID_DAILY_V4R4 , C1991543763-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SEA_ICE_CONC_THICKNESS_05DEG_MONTHLY_V4R4 , C1990404820-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SEA_ICE_CONC_THICKNESS_LLC0090GRID_MONTHLY_V4R4 , C1991543764-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SEA_ICE_CONC_THICKNESS_LLC0090GRID_SNAPSHOT_V4R4 , C1991543821-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SEA_ICE_HORIZ_VOLUME_FLUX_LLC0090GRID_DAILY_V4R4 , C1991543731-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SEA_ICE_HORIZ_VOLUME_FLUX_LLC0090GRID_MONTHLY_V4R4 , C1991543724-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SEA_ICE_SALT_PLUME_FLUX_LLC0090GRID_DAILY_V4R4 , C1991543807-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SEA_ICE_SALT_PLUME_FLUX_LLC0090GRID_MONTHLY_V4R4 , C1991543730-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SEA_ICE_VELOCITY_05DEG_DAILY_V4R4 , C1990404817-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SEA_ICE_VELOCITY_LLC0090GRID_DAILY_V4R4 , C1991543765-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SEA_ICE_VELOCITY_05DEG_MONTHLY_V4R4 , C1990404790-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SEA_ICE_VELOCITY_LLC0090GRID_MONTHLY_V4R4 , C1991543700-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nECCO_L4_SEA_ICE_VELOCITY_LLC0090GRID_SNAPSHOT_V4R4 , C1991543768-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PO.DAAC harmony-netcdf-to-zarr\nVIIRS_NPP-NAVO-L2P-v1.0 , C1996881807-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PODAAC L2 Cloud Subsetter\nVIIRS_NPP-NAVO-L2P-v3.0 , C1996881636-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PODAAC L2 Cloud Subsetter\nMSG03-OSPO-L2P-v1.0 , C2036878029-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PODAAC L2 Cloud Subsetter\nVIIRS_NPP-JPL-L2P-v2016.2 , C1996881456-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PODAAC L2 Cloud Subsetter\nMODIS_A-JPL-L2P-v2019.0 , C1940473819-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PODAAC L2 Cloud Subsetter\nMODIS_A-JPL-L2P-v2019.0 , C1693233348-PODAAC\nServices: PODAAC L2 Cloud Subsetter\nMODIS_T-JPL-L2P-v2019.0 , C1940475563-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PODAAC L2 Cloud Subsetter\nMODIS_T-JPL-L2P-v2019.0 , C1693233387-PODAAC\nServices: PODAAC L2 Cloud Subsetter\nTMI-REMSS-L2P-v4 , C2036879048-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PODAAC L2 Cloud Subsetter\nVIIRS_NPP-OSPO-L2P-v2.61 , C1996880725-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PODAAC L2 Cloud Subsetter\nVIIRS_N20-OSPO-L2P-v2.61 , C1996880450-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PODAAC L2 Cloud Subsetter\nAVHRR_SST_METOP_B-OSISAF-L2P-v1.0 , C2036880717-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PODAAC L2 Cloud Subsetter\nJASON-1_L2_OST_GPR_E , C1684065156-PODAAC\nServices: PODAAC L2 Cloud Subsetter\nJASON-1_L2_OST_GPN_E , C1684065153-PODAAC\nServices: PODAAC L2 Cloud Subsetter\nJASON-1_L2_OST_GPS_E , C1684065158-PODAAC\nServices: PODAAC L2 Cloud Subsetter\nCYGNSS_NOAA_L2_SWSP_25KM_V1.1 , C2036882072-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PODAAC L2 Cloud Subsetter\nJASON_CS_S6A_L2_AMR_RAD_NRT , C1968979997-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PODAAC L2 Cloud Subsetter\nJASON_CS_S6A_L2_AMR_RAD_STC , C1968979762-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PODAAC L2 Cloud Subsetter\nJASON_CS_S6A_L2_ALT_LR_STD_OST_NRT_F , C1968979597-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PODAAC L2 Cloud Subsetter\nJASON_CS_S6A_L2_ALT_LR_RED_OST_NRT_F , C1968980576-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PODAAC L2 Cloud Subsetter\nJASON_CS_S6A_L2_ALT_LR_STD_OST_STC_F , C1968980609-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PODAAC L2 Cloud Subsetter\nJASON_CS_S6A_L2_ALT_LR_RED_OST_STC_F , C1968979561-POCLOUD\nServices: PO.DAAC Cloud OPeNDAP\nServices: PODAAC L2 Cloud Subsetter",
    "crumbs": [
      "Tutorials",
      "07. Data Subsetting and Transformation Services in the Cloud"
    ]
  },
  {
    "objectID": "tutorials/07_Harmony_Subsetting.html#discover-variable-names",
    "href": "tutorials/07_Harmony_Subsetting.html#discover-variable-names",
    "title": "07. Data Subsetting and Transformation Services in the Cloud",
    "section": "Discover variable names",
    "text": "Discover variable names\nJust like services, a dataset may also be associated to metadata on their individual data variables, which can be used as input if we wish to subset by one or more variables of interest.\n\nvariables = response['feed']['entry'][0]['associations']['variables']\nprint(variables)\n\n['V1997812737-POCLOUD', 'V1997812697-POCLOUD', 'V2112014688-POCLOUD', 'V1997812756-POCLOUD', 'V1997812688-POCLOUD', 'V1997812670-POCLOUD', 'V1997812724-POCLOUD', 'V2112014684-POCLOUD', 'V1997812701-POCLOUD', 'V1997812681-POCLOUD', 'V2112014686-POCLOUD', 'V1997812663-POCLOUD', 'V1997812676-POCLOUD', 'V1997812744-POCLOUD', 'V1997812714-POCLOUD']\n\n\nSeveral variable records are returned. Again, like we did for services, we’ll search the variables endpoint to view an individual variable’s metadata, and we’ll print out the list of variables for our dataset.\n\nvar_url = \"https://cmr.earthdata.nasa.gov/search/variables\"\n\n\nvar_response = requests.get(var_url, \n                        params={\n                            'concept_id': variables[0],\n                            },\n                        headers={\n                            'Accept': 'application/vnd.nasa.cmr.umm_results+json'\n                            }\n                       )\nvar_response = var_response.json()\n\n\npprint(var_response)\n\n{'hits': 1,\n 'items': [{'associations': {'collections': [{'concept-id': 'C1940473819-POCLOUD'}]},\n            'meta': {'concept-id': 'V1997812737-POCLOUD',\n                     'concept-type': 'variable',\n                     'deleted': False,\n                     'format': 'application/vnd.nasa.cmr.umm+json',\n                     'native-id': 'MODIS_A-JPL-L2P-v2019.0-sses_standard_deviation_4um',\n                     'provider-id': 'POCLOUD',\n                     'revision-date': '2021-10-19T02:26:51.560Z',\n                     'revision-id': 6,\n                     'user-id': 'jmcnelis'},\n            'umm': {'DataType': 'byte',\n                    'Definition': 'mid-IR SST standard deviation error; non '\n                                  'L2P core field; signed byte array:  WARNING '\n                                  'Some applications are unable to properly '\n                                  'handle signed byte values. If values are '\n                                  'encountered &gt; 127, please subtract 256 from '\n                                  'this reported value',\n                    'Dimensions': [{'Name': 'time',\n                                    'Size': 1,\n                                    'Type': 'TIME_DIMENSION'},\n                                   {'Name': 'nj',\n                                    'Size': 2030,\n                                    'Type': 'ALONG_TRACK_DIMENSION'},\n                                   {'Name': 'ni',\n                                    'Size': 1354,\n                                    'Type': 'CROSS_TRACK_DIMENSION'}],\n                    'FillValues': [{'Type': 'SCIENCE_FILLVALUE',\n                                    'Value': -128}],\n                    'LongName': 'SSES standard deviation error based on '\n                                'proximity confidence flags',\n                    'Name': 'sses_standard_deviation_4um',\n                    'Offset': 10.0,\n                    'Scale': 0.07874016,\n                    'Sets': [{'Index': 1,\n                              'Name': 'sses_standard_deviation_4um',\n                              'Size': 1,\n                              'Type': 'General'}],\n                    'Units': 'kelvin',\n                    'ValidRanges': [{'Max': 127, 'Min': -127}],\n                    'VariableType': 'ANCILLARY_VARIABLE'}}],\n 'took': 9}\n\n\nNext, print out a simple list of all associated variable names by looping the same variable response we submitted above, this time for each variable:\n\nvar_list = []\nfor i in range(len(variables)):\n    var_response = requests.get(var_url, \n                            params={\n                                'concept_id': variables[i],\n                                },\n                            headers={\n                                'Accept': 'application/vnd.nasa.cmr.umm_results+json'\n                                }\n                           )\n    var_response = var_response.json()\n    var_list.append(var_response['items'][0]['umm']['Name'])\n\n\npprint(var_list)\n\n['sses_standard_deviation_4um',\n 'l2p_flags',\n 'time',\n 'dt_analysis',\n 'sses_standard_deviation',\n 'sst_dtime',\n 'sses_bias_4um',\n 'lat',\n 'sea_surface_temperature_4um',\n 'sses_bias',\n 'lon',\n 'sea_surface_temperature',\n 'quality_level',\n 'wind_speed',\n 'quality_level_4um']",
    "crumbs": [
      "Tutorials",
      "07. Data Subsetting and Transformation Services in the Cloud"
    ]
  },
  {
    "objectID": "tutorials/07_Harmony_Subsetting.html#using-harmony-py-to-subset-data",
    "href": "tutorials/07_Harmony_Subsetting.html#using-harmony-py-to-subset-data",
    "title": "07. Data Subsetting and Transformation Services in the Cloud",
    "section": "Using Harmony-Py to subset data",
    "text": "Using Harmony-Py to subset data\nHarmony-Py provides a pip installable Python alternative to directly using Harmony’s RESTful API to make it easier to request data and service options, especially when interacting within a Python Jupyter Notebook environment.\nThe next steps are adopted from the introduction tutorial notebook provided in the Harmony-Py library:\n\nCreate Harmony Client object\nFirst, we need to create a Harmony Client, which is what we will interact with to submit and inspect a data request to Harmony, as well as to retrieve results.\nWhen creating the Client, we need to provide Earthdata Login credentials, which are required to access data from NASA EOSDIS. This basic line below assumes that we have a .netrc available.\n\nharmony_client = Client()\n\n\n\nHurricane Ida snapshot\nUsing NASA Worldview, we can first explore SST during a tropical storm event; in this case, we can overlay L2 and L4 SST variables against true color imagery to observe Hurricane Ida in August 2021. Although this is a small sample set, this use case could be expanded to explore how SST responds during the Atlantic hurricane over the next several months. The same data that we are requesting below using Harmony-py can also be requested using NASA Earthdata Search\n\n\n\nHurrican Ida snapshot - Worldview\n\n\n\n\nCreate Harmony Request\nThe following are common request parameters:\n\ncollection: Required parameter. This is the NASA EOSDIS collection, or data product. There are two options for inputting a collection of interest:\n\nProvide a concept ID (e.g. C1940473819-POCLOUD)\nData product short name (e.g. MODIS_A-JPL-L2P-v2019.0).\n\nspatial: Bounding box spatial constraints on the data. The Harmony Bbox class accepts spatial coordinates as decimal degrees in w, s, e, n order, where longitude = -180, 180 and latitude = -90, 90.\ntemporal: Date/time constraints on the data. The example below demonstrates temporal start and end ranges using the python datetime library.\n\nAs we identified above, only subsetting options are available for this dataset. If other service options such as reformatting are available for a given dataset, these can also be specified using Harmony-py: See the documentation for details on how to construct these parameters.\n\nrequest = Request(\n    collection=Collection(id=short_name),\n    spatial=BBox(-97.77667,21.20806,-83.05197,30.16605),\n    temporal={\n        'start': dt.datetime(2021, 8, 20),\n        'stop': dt.datetime(2021, 8, 21),\n    },\n)\n\n\n\nCheck Request validity\nBefore submitting a Harmony Request, we can test your request to see if it’s valid and how to fix it if not. In particular, request.is_valid will check to ensure that the spatial BBox bounds and temporal ranges are entered correctly.\n\nrequest.is_valid()\n\nTrue\n\n\n\n\nSubmit request\nNow that the request is created, we can now submit it to Harmony using the Harmony Client object. A job id is returned, which is a unique identifier that represents the submitted request.\n\njob_id = harmony_client.submit(request)\njob_id\n\n'e36f6916-a7d9-4d82-a5fa-924d48d464ef'\n\n\n\n\nCheck request status\nWe can check on the progress of a processing job with status(). This method blocks while communicating with the server but returns quickly.\n\nharmony_client.status(job_id)\n\n{'status': 'running',\n 'message': 'There were 2 collections that matched the provided short name MODIS_A-JPL-L2P-v2019.0. See https://cmr.earthdata.nasa.gov/concepts/C1940473819-POCLOUD for details on the selected collection. The version ID for the selected collection is 2019.0. To use a different collection submit a new request specifying the desired CMR concept ID instead of the collection short name.',\n 'progress': 0,\n 'created_at': datetime.datetime(2021, 11, 19, 17, 44, 40, 768000, tzinfo=tzlocal()),\n 'updated_at': datetime.datetime(2021, 11, 19, 17, 44, 40, 768000, tzinfo=tzlocal()),\n 'request': 'https://harmony.earthdata.nasa.gov/MODIS_A-JPL-L2P-v2019.0/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?forceAsync=true&subset=lat(21.20806%3A30.16605)&subset=lon(-97.77667%3A-83.05197)&subset=time(%222021-08-20T00%3A00%3A00%22%3A%222021-08-21T00%3A00%3A00%22)',\n 'num_input_granules': 6}\n\n\nDepending on the size of the request, it may be helpful to wait until the request has completed processing before the remainder of the code is executed. The wait_for_processing() method will block subsequent lines of code while optionally showing a progress bar.\n\nharmony_client.wait_for_processing(job_id, show_progress=True)\n\n [ Processing: 100% ] |###################################################| [|]\n\n\n\n\nView Harmony job response and output URLs\nOnce the data request has finished processing, we can view details on the job that was submitted to Harmony, including the API call to Harmony, and informational messages on the request if available.\nresult_json() calls wait_for_processing() and returns the complete job in JSON format once processing is complete.\n\ndata = harmony_client.result_json(job_id)\npprint(data)\n\n{'createdAt': '2021-11-19T17:44:40.768Z',\n 'jobID': 'e36f6916-a7d9-4d82-a5fa-924d48d464ef',\n 'links': [{'href': 'https://harmony.earthdata.nasa.gov/stac/e36f6916-a7d9-4d82-a5fa-924d48d464ef/',\n            'rel': 'stac-catalog-json',\n            'title': 'STAC catalog',\n            'type': 'application/json'},\n           {'bbox': [-83.612, 1.103, -58.391, 22.005],\n            'href': 'https://harmony.earthdata.nasa.gov/service-results/harmony-prod-staging/public/podaac/l2-subsetter/cef7a7c7-c01c-4186-ac73-3e03f2940259/20210820062501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0.nc4',\n            'rel': 'data',\n            'temporal': {'end': '2021-08-20T06:29:58.000Z',\n                         'start': '2021-08-20T06:25:01.000Z'},\n            'title': '20210820062501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0.nc4',\n            'type': 'application/x-netcdf4'},\n           {'bbox': [-97.8, 21.2, -83.1, 30.2],\n            'href': 'https://harmony.earthdata.nasa.gov/service-results/harmony-prod-staging/public/podaac/l2-subsetter/cef7a7c7-c01c-4186-ac73-3e03f2940259/20210820080001-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0_subsetted.nc4',\n            'rel': 'data',\n            'temporal': {'end': '2021-08-20T08:04:58.000Z',\n                         'start': '2021-08-20T08:00:01.000Z'},\n            'title': '20210820080001-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0_subsetted.nc4',\n            'type': 'application/x-netcdf4'},\n           {'bbox': [-128.335, 29.006, -95.854, 49.87],\n            'href': 'https://harmony.earthdata.nasa.gov/service-results/harmony-prod-staging/public/podaac/l2-subsetter/cef7a7c7-c01c-4186-ac73-3e03f2940259/20210820093501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0.nc4',\n            'rel': 'data',\n            'temporal': {'end': '2021-08-20T09:39:49.000Z',\n                         'start': '2021-08-20T09:35:01.000Z'},\n            'title': '20210820093501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0.nc4',\n            'type': 'application/x-netcdf4'},\n           {'bbox': [-96.8, 21.2, -83.1, 27.7],\n            'href': 'https://harmony.earthdata.nasa.gov/service-results/harmony-prod-staging/public/podaac/l2-subsetter/cef7a7c7-c01c-4186-ac73-3e03f2940259/20210820185501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0_subsetted.nc4',\n            'rel': 'data',\n            'temporal': {'end': '2021-08-20T18:59:58.000Z',\n                         'start': '2021-08-20T18:55:01.000Z'},\n            'title': '20210820185501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0_subsetted.nc4',\n            'type': 'application/x-netcdf4'},\n           {'bbox': [-97.8, 25.3, -83.1, 30.2],\n            'href': 'https://harmony.earthdata.nasa.gov/service-results/harmony-prod-staging/public/podaac/l2-subsetter/cef7a7c7-c01c-4186-ac73-3e03f2940259/20210820190001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0_subsetted.nc4',\n            'rel': 'data',\n            'temporal': {'end': '2021-08-20T19:04:58.000Z',\n                         'start': '2021-08-20T19:00:01.000Z'},\n            'title': '20210820190001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0_subsetted.nc4',\n            'type': 'application/x-netcdf4'},\n           {'bbox': [-97.8, 21.2, -97.3, 24.6],\n            'href': 'https://harmony.earthdata.nasa.gov/service-results/harmony-prod-staging/public/podaac/l2-subsetter/cef7a7c7-c01c-4186-ac73-3e03f2940259/20210820203501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0_subsetted.nc4',\n            'rel': 'data',\n            'temporal': {'end': '2021-08-20T20:39:58.000Z',\n                         'start': '2021-08-20T20:35:01.000Z'},\n            'title': '20210820203501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0_subsetted.nc4',\n            'type': 'application/x-netcdf4'},\n           {'href': 'https://harmony.earthdata.nasa.gov/jobs/e36f6916-a7d9-4d82-a5fa-924d48d464ef?linktype=https&page=1&limit=2000',\n            'rel': 'self',\n            'title': 'The current page',\n            'type': 'application/json'}],\n 'message': 'There were 2 collections that matched the provided short name '\n            'MODIS_A-JPL-L2P-v2019.0. See '\n            'https://cmr.earthdata.nasa.gov/concepts/C1940473819-POCLOUD for '\n            'details on the selected collection. The version ID for the '\n            'selected collection is 2019.0. To use a different collection '\n            'submit a new request specifying the desired CMR concept ID '\n            'instead of the collection short name.',\n 'numInputGranules': 6,\n 'progress': 100,\n 'request': 'https://harmony.earthdata.nasa.gov/MODIS_A-JPL-L2P-v2019.0/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?forceAsync=true&subset=lat(21.20806%3A30.16605)&subset=lon(-97.77667%3A-83.05197)&subset=time(%222021-08-20T00%3A00%3A00%22%3A%222021-08-21T00%3A00%3A00%22)',\n 'status': 'successful',\n 'updatedAt': '2021-11-19T17:45:17.148Z',\n 'username': 'amy.steiker'}\n\n\n\n\nDirect cloud access\nNote that the remainder of this tutorial will only succeed when running this notebook within the AWS us-west-2 region.\nHarmony data outputs can be accessed within the cloud using the s3 URLs and AWS credentials provided in the Harmony job response.\n\nRetrieve list of output URLs.\nThe result_urls() method calls wait_for_processing() and returns a list of the processed data URLs once processing is complete. You may optionally show the progress bar as shown below.\n\nresults = harmony_client.result_urls(job_id, link_type=LinkType.s3)\nurls = list(results)\npprint(urls)\n\n['s3://harmony-prod-staging/public/podaac/l2-subsetter/cef7a7c7-c01c-4186-ac73-3e03f2940259/20210820062501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0.nc4',\n 's3://harmony-prod-staging/public/podaac/l2-subsetter/cef7a7c7-c01c-4186-ac73-3e03f2940259/20210820080001-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0_subsetted.nc4',\n 's3://harmony-prod-staging/public/podaac/l2-subsetter/cef7a7c7-c01c-4186-ac73-3e03f2940259/20210820093501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0.nc4',\n 's3://harmony-prod-staging/public/podaac/l2-subsetter/cef7a7c7-c01c-4186-ac73-3e03f2940259/20210820185501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0_subsetted.nc4',\n 's3://harmony-prod-staging/public/podaac/l2-subsetter/cef7a7c7-c01c-4186-ac73-3e03f2940259/20210820190001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0_subsetted.nc4',\n 's3://harmony-prod-staging/public/podaac/l2-subsetter/cef7a7c7-c01c-4186-ac73-3e03f2940259/20210820203501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0_subsetted.nc4']\n\n\nWe can see that the first file returned does not include the _subsetted suffix, which indicates that a blank file was returned, as no data values were located within our subsetted region. We’ll select the second URL in the list to bring into xarray below.\n\nurl = urls[1]\nurl\n\n's3://harmony-prod-staging/public/podaac/l2-subsetter/cef7a7c7-c01c-4186-ac73-3e03f2940259/20210820080001-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0_subsetted.nc4'\n\n\n\n\nAWS credential retrieval\nUsing aws_credentials you can retrieve the credentials needed to access the Harmony s3 staging bucket and its contents.\n\ncreds = harmony_client.aws_credentials()\n\n\n\n\nOpen staged files with s3fs and xarray\nWe use the AWS s3fs package to create a file system that can then be read by xarray:\n\ns3_fs = s3fs.S3FileSystem(\n    key=creds['aws_access_key_id'],\n    secret=creds['aws_secret_access_key'],\n    token=creds['aws_session_token'],\n    client_kwargs={'region_name':'us-west-2'},\n)\n\nNow that we have our s3 file system set, including our declared credentials, we’ll use that to open the url, and read in the file through xarray. This extra step is needed because xarray cannot open the S3 location directly. Instead, the S3 file object is passed to xarray, in order to then open the dataset.\n\nf = s3_fs.open(url, mode='rb')\nds = xr.open_dataset(f)\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:                      (nj: 1232, ni: 1132, time: 1)\nCoordinates:\n    lat                          (nj, ni, time) float32 ...\n    lon                          (nj, ni, time) float32 ...\n  * time                         (time) datetime64[ns] 2021-08-20T08:00:01\nDimensions without coordinates: nj, ni\nData variables:\n    sea_surface_temperature      (time, nj, ni) float32 ...\n    sst_dtime                    (time, nj, ni) timedelta64[ns] ...\n    quality_level                (time, nj, ni) float32 ...\n    sses_bias                    (time, nj, ni) float32 ...\n    sses_standard_deviation      (time, nj, ni) float32 ...\n    l2p_flags                    (time, nj, ni) int16 ...\n    sea_surface_temperature_4um  (time, nj, ni) float32 ...\n    quality_level_4um            (time, nj, ni) float32 ...\n    sses_bias_4um                (time, nj, ni) float32 ...\n    sses_standard_deviation_4um  (time, nj, ni) float32 ...\n    wind_speed                   (time, nj, ni) float32 ...\n    dt_analysis                  (time, nj, ni) float32 ...\nAttributes: (12/50)\n    Conventions:                CF-1.7, ACDD-1.3\n    title:                      MODIS Aqua L2P SST\n    summary:                    Sea surface temperature retrievals produced a...\n    references:                 GHRSST Data Processing Specification v2r5\n    institution:                NASA/JPL/OBPG/RSMAS\n    history:                    MODIS L2P created at JPL PO.DAAC\\n2021-11-19 ...\n    ...                         ...\n    processing_level:           L2P\n    cdm_data_type:              swath\n    startDirection:             Descending\n    endDirection:               Descending\n    day_night_flag:             Night\n    history_json:               [{\"date_time\": \"2021-11-19T17:44:56.897126+00...xarray.DatasetDimensions:nj: 1232ni: 1132time: 1Coordinates: (3)lat(nj, ni, time)float32...long_name :latitudestandard_name :latitudeunits :degrees_northvalid_min :[-90.]valid_max :[90.]comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[1394624 values with dtype=float32]lon(nj, ni, time)float32...long_name :longitudestandard_name :longitudeunits :degrees_eastvalid_min :[-180.]valid_max :[180.]comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[1394624 values with dtype=float32]time(time)datetime64[ns]2021-08-20T08:00:01long_name :reference time of sst filestandard_name :timecomment :time of first sensor observationcoverage_content_type :coordinatearray(['2021-08-20T08:00:01.000000000'], dtype='datetime64[ns]')Data variables: (12)sea_surface_temperature(time, nj, ni)float32...long_name :sea surface temperaturestandard_name :sea_surface_skin_temperatureunits :kelvinvalid_min :[-1000]valid_max :[10000]comment :sea surface temperature from thermal IR (11 um) channelssource :NASA and University of Miamicoverage_content_type :physicalMeasurement[1394624 values with dtype=float32]sst_dtime(time, nj, ni)timedelta64[ns]...long_name :time difference from reference timevalid_min :[-32767]valid_max :[32767]comment :time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981coverage_content_type :referenceInformation[1394624 values with dtype=timedelta64[ns]]quality_level(time, nj, ni)float32...long_name :quality level of SST pixelvalid_min :[0]valid_max :[5]comment :thermal IR SST proximity confidence value; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered &gt; 127, please subtract 256 from this reported valueflag_values :[0 1 2 3 4 5]flag_meanings :no_data bad_data worst_quality low_quality acceptable_quality best_qualitycoverage_content_type :qualityInformation[1394624 values with dtype=float32]sses_bias(time, nj, ni)float32...long_name :SSES bias error based on proximity confidence flagsunits :kelvinvalid_min :[-127]valid_max :[127]comment :thermal IR SST bias error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered &gt; 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[1394624 values with dtype=float32]sses_standard_deviation(time, nj, ni)float32...long_name :SSES standard deviation error based on proximity confidence flagsunits :kelvinvalid_min :[-127]valid_max :[127]comment :thermal IR SST standard deviation error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered &gt; 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[1394624 values with dtype=float32]l2p_flags(time, nj, ni)int16...long_name :L2P flagsvalid_min :[0]valid_max :[16]comment :These flags can be used to further filter data variablesflag_meanings :microwave land ice lake riverflag_masks :[ 1  2  4  8 16]coverage_content_type :qualityInformation[1394624 values with dtype=int16]sea_surface_temperature_4um(time, nj, ni)float32...long_name :sea surface temperatureunits :kelvinvalid_min :[-1000]valid_max :[10000]comment :sea surface temperature from mid-IR (4 um) channels; non L2P core fieldcoverage_content_type :physicalMeasurement[1394624 values with dtype=float32]quality_level_4um(time, nj, ni)float32...long_name :quality level of SST pixelvalid_min :[0]valid_max :[5]comment :mid-IR SST proximity confidence value; non L2P core field; signed byte array:  WARNING Some applications are unable to properly handle signed byte values. If values are encountered &gt; 127, please subtract 256 from this reported valueflag_values :[0 1 2 3 4 5]flag_meanings :no_data bad_data worst_quality low_quality acceptable_quality best_qualitycoverage_content_type :qualityInformation[1394624 values with dtype=float32]sses_bias_4um(time, nj, ni)float32...long_name :SSES bias error based on proximity confidence flagsunits :kelvinvalid_min :[-127]valid_max :[127]comment :mid-IR SST bias error; non L2P core field; signed byte array:  WARNING Some applications are unable to properly handle signed byte values. If values are encountered &gt; 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[1394624 values with dtype=float32]sses_standard_deviation_4um(time, nj, ni)float32...long_name :SSES standard deviation error based on proximity confidence flagsunits :kelvinvalid_min :[-127]valid_max :[127]comment :mid-IR SST standard deviation error; non L2P core field; signed byte array:  WARNING Some applications are unable to properly handle signed byte values. If values are encountered &gt; 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[1394624 values with dtype=float32]wind_speed(time, nj, ni)float32...long_name :10m wind speedstandard_name :wind_speedunits :m s-1valid_min :[-127]valid_max :[127]comment :Wind at 10 meters above the sea surfacesource :TBD.  Placeholder.  Currently emptygrid_mapping :TBDtime_offset :[2.]height :10 mcoverage_content_type :auxiliaryInformation[1394624 values with dtype=float32]dt_analysis(time, nj, ni)float32...long_name :deviation from SST reference climatologyunits :kelvinvalid_min :[-127]valid_max :[127]comment :TBDsource :TBD. Placeholder.  Currently emptycoverage_content_type :auxiliaryInformation[1394624 values with dtype=float32]Attributes: (50)Conventions :CF-1.7, ACDD-1.3title :MODIS Aqua L2P SSTsummary :Sea surface temperature retrievals produced at the NASA OBPG for the MODIS Aqua sensor.  These have been reformatted to GHRSST GDS specifications by the JPL PO.DAACreferences :GHRSST Data Processing Specification v2r5institution :NASA/JPL/OBPG/RSMAShistory :MODIS L2P created at JPL PO.DAAC\n2021-11-19 17:44:56.897087 l2ss-py v1.1.0 (bbox=[[-97.77667, -83.05197], [21.20806, 30.16605]] cut=True)comment :L2P Core without DT analysis or other ancillary fields; Night, Start Node:Descending, End Node:Descending; WARNING Some applications are unable to properly handle signed byte values. If values are encountered &gt; 127, please subtract 256 from this reported value; Quicklooklicense :GHRSST and PO.DAAC protocol allow data use as free and open.id :MODIS_A-JPL-L2P-v2019.0naming_authority :org.ghrsstproduct_version :2019.0uuid :f6e1f61d-c4a4-4c17-8354-0c15e12d688bgds_version_id :2.0netcdf_version_id :4.1date_created :20210820T110619Zfile_quality_level :[3]spatial_resolution :1kmstart_time :20210820T080001Ztime_coverage_start :20210820T080001Zstop_time :20210820T080458Ztime_coverage_end :20210820T080458Znorthernmost_latitude :[35.9455]southernmost_latitude :[14.8953]easternmost_longitude :[-78.2345]westernmost_longitude :[-105.765]source :MODIS sea surface temperature observations for the OBPGplatform :Aquasensor :MODISmetadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MODIS_A-JPL-L2P-v2019.0keywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventiongeospatial_lat_units :degrees_northgeospatial_lat_resolution :[0.01]geospatial_lon_units :degrees_eastgeospatial_lon_resolution :[0.01]acknowledgment :The MODIS L2P sea surface temperature data are sponsored by NASAcreator_name :Ed Armstrong, JPL PO.DAACcreator_email :edward.m.armstrong@jpl.nasa.govcreator_url :http://podaac.jpl.nasa.govproject :Group for High Resolution Sea Surface Temperaturepublisher_name :The GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L2Pcdm_data_type :swathstartDirection :DescendingendDirection :Descendingday_night_flag :Nighthistory_json :[{\"date_time\": \"2021-11-19T17:44:56.897126+00:00\", \"derived_from\": \"https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20210820080001-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0.nc\", \"program\": \"l2ss-py\", \"version\": \"1.1.0\", \"parameters\": \"bbox=[[-97.77667, -83.05197], [21.20806, 30.16605]] cut=True\", \"program_ref\": \"https://cmr.earthdata.nasa.gov:443/search/concepts/S1962070864-POCLOUD\", \"$schema\": \"https://harmony.earthdata.nasa.gov/schemas/history/0.1.0/history-v0.1.0.json\"}]\n\n\n\n\nPlot the data\nUse the xarray built in plotting function to create a simple plot along the x and y dimensions of the dataset:\n\nds.sea_surface_temperature.plot() ;",
    "crumbs": [
      "Tutorials",
      "07. Data Subsetting and Transformation Services in the Cloud"
    ]
  },
  {
    "objectID": "tutorials/07_Harmony_Subsetting.html#resources",
    "href": "tutorials/07_Harmony_Subsetting.html#resources",
    "title": "07. Data Subsetting and Transformation Services in the Cloud",
    "section": "Resources",
    "text": "Resources\n\nIn-depth exploration of the MODIS_A-JPL-L2P-v2019.0 data set, co-locating in-situ and remote sensing data: https://github.com/podaac/tutorials/blob/master/notebooks/SWOT-EA-2021/Colocate_satellite_insitu_ocean.ipynb\nHarmony-Py library introduction tutorial: https://github.com/nasa/harmony-py/blob/main/examples/intro_tutorial.ipynb",
    "crumbs": [
      "Tutorials",
      "07. Data Subsetting and Transformation Services in the Cloud"
    ]
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__gdalvrt.html#timing",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__gdalvrt.html#timing",
    "title": "Direct S3 Data Access with GDAL Virtual Raster Format (VRT)",
    "section": "Timing:",
    "text": "Timing:\n\nExercise: 20 minutes"
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__gdalvrt.html#summary",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__gdalvrt.html#summary",
    "title": "Direct S3 Data Access with GDAL Virtual Raster Format (VRT)",
    "section": "Summary",
    "text": "Summary\nHello World"
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__gdalvrt.html#exercise",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__gdalvrt.html#exercise",
    "title": "Direct S3 Data Access with GDAL Virtual Raster Format (VRT)",
    "section": "Exercise",
    "text": "Exercise\n\nImport Required Packages\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport os\nimport subprocess\nimport requests\nimport boto3\nfrom pystac_client import Client\nfrom collections import defaultdict\nimport numpy as np\nimport xarray as xr\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nfrom rasterio.plot import show\nimport rioxarray\nimport geopandas\nimport pyproj\nfrom pyproj import Proj\nfrom shapely.ops import transform\nimport geoviews as gv\nfrom cartopy import crs\nimport hvplot.xarray\nimport holoviews as hv\ngv.extension('bokeh', 'matplotlib')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n  \n  \n\n\n\n\n\n\n\nGet Temporary Credentials and Configure Local Environment\nTo perform direct S3 data access one needs to acquire temporary S3 credentials. The credentials give users direct access to S3 buckets in NASA Earthdata Cloud. AWS credentials should not be shared, so take precautions when using them in notebooks our scripts. Note, these temporary credentials are valid for only 1 hour. For more information regarding the temporary credentials visit https://data.lpdaac.earthdatacloud.nasa.gov/s3credentialsREADME.\n\ndef get_temp_creds():\n    temp_creds_url = 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\n    return requests.get(temp_creds_url).json()\n\n\ntemp_creds_req = get_temp_creds()\n#temp_creds_req                      # !!! BEWARE, removing the # on this line will print your temporary S3 credentials.\n\n\nInsert the credentials into our boto3 session and configure out rasterio environment for data access\nCreate a boto3 Session object using your temporary credentials. This Session can then be used to pass those credentials and get S3 objects from applicable buckets.\n\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nGDAL environment variables must be configured to access Earthdata Cloud data assets. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\n&lt;rasterio.env.Env at 0x7fdb42409c10&gt;\n\n\n\n\n\nRead In and Process STAC Asset Links\nIn the previous section, we used the NASA CMR-STAC API to discover HLS assets the intersect with our search criteria, i.e., ROI, Date range, and collections. The search results were filtered and saved as text files by individual bands for each tile. We will read in the text files for tile T13TGF for the RED (L30: B04 & S30: B04), NIR (L30: B05 & S30: B8A), and Fmask bands.\n\nList text files with HLS links\n\n[t for t in os.listdir('./data') if '.txt' in t]\n\n['HTTPS_T13TGF_B02_Links.txt',\n 'S3_T13TGF_B05_Links.txt',\n 'HTTPS_T13TGF_Fmask_Links.txt',\n 'S3_T13TGF_B8A_Links.txt',\n 'HTTPS_T13TGF_B04_Links.txt',\n 'S3_T13TGF_B04_Links.txt',\n 'S3_T13TGF_Fmask_Links.txt',\n 'HTTPS_T13TGF_B8A_Links.txt',\n 'HTTPS_T13TGF_B05_Links.txt',\n 'S3_T13TGF_B02_Links.txt']\n\n\n\n\nRead in our asset links for BO4 (RED)\n\nred_s3_links = open('./data/S3_T13TGF_B04_Links.txt').read().splitlines()\nred_s3_links\n\n['s3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021133T173859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021140T173021.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021140T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021145T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021156T173029.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021163T173909.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021165T172422.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021165T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021185T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021188T173037.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021190T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021198T173911.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021200T172859.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021203T173909.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021204T173042.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021215T172901.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021220T173049.v1.5.B04.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021229T172441.v1.5.B04.tif']\n\n\n\n\nRead in and combine our asset links for BO5 (Landsat NIR) and B8A (Sentinel-2 NIR)\nThe near-infrared (NIR) band for Landsat is B05 while the NIR band for Sentinel-2 is B8A. In the next step we will read in and combine the lists into a single NIR list.\n\nnir_bands = ['B05', 'B8A']\nnir_link_text = [x for x in os.listdir('./data') if any(b in x for b in nir_bands) and 'S3' in x]\nnir_s3_links = []\nfor file in nir_link_text:\n    nir_s3_links.extend(open(f'./data/{file}').read().splitlines())\nnir_s3_links\n\n['s3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021133T172406.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021140T173021.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021156T173029.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021165T172422.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021188T173037.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021204T173042.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021220T173049.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSL30.015/HLS.L30.T13TGF.2021229T172441.v1.5.B05.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021133T173859.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021140T172859.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021145T172901.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021163T173909.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021165T172901.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021185T172901.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021190T172859.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021198T173911.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021200T172859.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021203T173909.v1.5.B8A.tif',\n 's3://lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2021215T172901.v1.5.B8A.tif']\n\n\n\n\nRead in our asset links for Fmask\n\nfmask_s3_links = open('./data/S3_T13TGF_Fmask_Links.txt').read().splitlines()\n#fmask_s3_links\n\nIn this example we will use the gdalbuildvrt.exe utility to create a time series virtual raster format (VRT) file. The utility, however, expects the links to be formated with the GDAL virtual file system (VSI) path, rather than the actual asset links. We will therefore use the VSI path to access our assets. The examples below show the VSI path substitution for S3 (vsis3) links.\n/vsis3/lp-prod-protected/HLSS30.015/HLS.S30.T13TGF.2020191T172901.v1.5.B04.tif\nSee the GDAL Virtual File Systems for more information regarding GDAL VSI.\n\n\nWrite out a new text file containing the vsis3 path\n\nwith open('./data/S3_T13TGF_RED_VSI_Links.txt', 'w') as f:\n    links_vsi = [r.replace('s3://', '/vsis3/' ) + '\\n' for r in red_s3_links]\n    for link in links_vsi:\n        f.write(link)\n\n\nwith open('./data/S3_T13TGF_NIR_VSI_Links.txt', 'w') as f:\n    links_vsi = [r.replace('s3://', '/vsis3/' ) + '\\n' for r in nir_s3_links]\n    for link in links_vsi:\n        f.write(link)\n\n\nwith open('./data/S3_T13TGF_FMASK_VSI_Links.txt', 'w') as f:\n    links_vsi = [r.replace('s3://', '/vsis3/' ) + '\\n' for r in fmask_s3_links]\n    for link in links_vsi:\n        f.write(link)\n\n\n\n\nRead in geoJSON for subsetting\nWe will use the input geoJSON file to clip the source data to our desired region of interest.\n\nfield = geopandas.read_file('./data/ne_w_agfields.geojson')\nfieldShape = field['geometry'][0]  \n\nTo clip the source data to our input feature boundary, we need to transform the feature boundary from its original WGS84 coordinate reference system to the projected reference system of the source HLS file (i.e., UTM Zone 13).\n\nfoa_url = red_s3_links[0]\nwith rio.open(foa_url) as src:\n    hls_proj = src.crs.to_string()\n\nhls_proj    \n\n'EPSG:32613'\n\n\n\nTransform geoJSON feature from WGS84 to UTM\n\ngeo_CRS = Proj('+proj=longlat +datum=WGS84 +no_defs', preserve_units=True)   # Source coordinate system of the ROI\nproject = pyproj.Transformer.from_proj(geo_CRS, hls_proj)                    # Set up the transformation\nfsUTM = transform(project.transform, fieldShape)\n\n\n\n\nDirect S3 Data Access\n\nStart up a dask client\n\n#from dask.distributed import Client\n\n\n#client = Client(n_workers=2)\n#client\n\nThere are multiple way to read COG data in as a time series. The subprocess package is used in this example to run GDAL’s build virtual raster file (gdalbuildvrt) executable outside our python session. First we’ll need to construct a string object with the command and it’s parameter parameters (including our temporary credentials). Then, we run the command using the subprocess.call() function.\n\n\nBuild GDAL VRT Files\n\nConstruct the GDAL VRT call\n\nbuild_red_vrt = f\"gdalbuildvrt ./data/red_stack.vrt -separate -input_file_list ./data/S3_T13TGF_RED_VSI_Links.txt --config AWS_ACCESS_KEY_ID {temp_creds_req['accessKeyId']} --config AWS_SECRET_ACCESS_KEY {temp_creds_req['secretAccessKey']} --config AWS_SESSION_TOKEN {temp_creds_req['sessionToken']} --config GDAL_DISABLE_READDIR_ON_OPEN TRUE\"\n#build_red_vrt    # !!! BEWARE, removing the # on this line will print your temporary S3 credentials.\n\nWe now have a fully configured gdalbuildvrt string that we can pass to Python’s subprocess module to run the gdalbuildvrt executable outside our Python environment.\n\n\n\nExecute gdalbuildvrt to construct a VRT on disk from the S3 links\n\n%%time\n\nsubprocess.call(build_red_vrt, shell=True)\n\nCPU times: user 2.1 ms, sys: 3.83 ms, total: 5.93 ms\nWall time: 3.74 s\n\n\n0\n\n\n0 means success! We’ll have some troubleshooting to do you get any other value. In this tutorial, the path for the output VRT file or the input file list are the first things to check.\nWhile we’re here, we’ll build the VRT files for the NIR layers and the Fmask layers.\n\nbuild_nir_vrt = f\"gdalbuildvrt ./data/nir_stack.vrt -separate -input_file_list ./data/S3_T13TGF_NIR_VSI_Links.txt --config AWS_ACCESS_KEY_ID {temp_creds_req['accessKeyId']} --config AWS_SECRET_ACCESS_KEY {temp_creds_req['secretAccessKey']} --config AWS_SESSION_TOKEN {temp_creds_req['sessionToken']} --config GDAL_DISABLE_READDIR_ON_OPEN TRUE\"\nsubprocess.call(build_nir_vrt, shell=True)\n\n0\n\n\n\nbuild_fmask_vrt = f\"gdalbuildvrt ./data/fmask_stack.vrt -separate -input_file_list ./data/S3_T13TGF_FMASK_VSI_Links.txt --config AWS_ACCESS_KEY_ID {temp_creds_req['accessKeyId']} --config AWS_SECRET_ACCESS_KEY {temp_creds_req['secretAccessKey']} --config AWS_SESSION_TOKEN {temp_creds_req['sessionToken']} --config GDAL_DISABLE_READDIR_ON_OPEN TRUE\"\nsubprocess.call(build_fmask_vrt, shell=True)\n\n0\n\n\n\n\n\nReading in an HLS time series\nWe can now read the VRT files into our Python session. A drawback of reading VRTs into Python is that the time coordinate variable needs to be contructed. Below we not only read in the VRT file using rioxarray, but we also repurpose the band variable, which is generated automatically, to hold out time information.\n\nRead the RED VRT in as xarray with Dask backing\n\n%%time\n\nchunks=dict(band=1, x=1024, y=1024)\n#chunks=dict(band=1, x=512, y=512)\nred = rioxarray.open_rasterio('./data/red_stack.vrt', chunks=chunks)                    # Read in VRT\nred = red.rename({'band':'time'})                                                       # Rename the 'band' coordinate variable to 'time' \nred['time'] = [datetime.strptime(x.split('.')[-5], '%Y%jT%H%M%S') for x in links_vsi]   # Extract the time information from the input file names and assign them to the time coordinate variable\nred = red.sortby('time')                                                                # Sort by the time coordinate variable\nred\n\nCPU times: user 219 ms, sys: 20.3 ms, total: 239 ms\nWall time: 246 ms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (time: 19, y: 3660, x: 3660)&gt;\ndask.array&lt;getitem, shape=(19, 3660, 3660), dtype=int16, chunksize=(1, 1024, 1024), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0xarray.DataArraytime: 19y: 3660x: 3660dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n485.45 MiB\n2.00 MiB\n\n\nShape\n(19, 3660, 3660)\n(1, 1024, 1024)\n\n\nCount\n609 Tasks\n304 Chunks\n\n\nType\nint16\nnumpy.ndarray\n\n\n\n\n                                                                         3660 3660 19\n\n\n\n\nCoordinates: (4)time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (3)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0\n\n\nAbove we use the parameter chunk in the rioxarray.open_rasterio() function to enable the Dask backing. What this allows is lazy reading of the data, which means the data is not actually read in into memory at this point. What we have is an object with some metadata and pointer to the source data. The data will be streamed to us when we call for it, but not stored in memory until with call the Dask compute() or persist() methods.\n\n\nPrint out the time coordinate\n\nred.time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'time' (time: 19)&gt;\narray(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')\nCoordinates:\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n    spatial_ref  int64 0xarray.DataArray'time'time: 192021-05-13T17:24:06 2021-05-13T17:38:59 ... 2021-08-17T17:24:41array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')Coordinates: (2)time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (0)\n\n\n\n\nClip out the ROI and persist the result in memory\nUp until now, we haven’t read any of the HLS data into memory. Now we will use the persist() method to load the data into memory.\n\nred_clip = red.rio.clip([fsUTM]).persist()\nred_clip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (time: 19, y: 56, x: 56)&gt;\ndask.array&lt;astype, shape=(19, 56, 56), dtype=int16, chunksize=(1, 56, 56), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * y            (y) float64 4.551e+06 4.551e+06 ... 4.549e+06 4.549e+06\n  * x            (x) float64 7.796e+05 7.796e+05 ... 7.812e+05 7.812e+05\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n    spatial_ref  int64 0\nAttributes:\n    scale_factor:  0.0001\n    add_offset:    0.0\n    _FillValue:    -9999xarray.DataArraytime: 19y: 56x: 56dask.array&lt;chunksize=(1, 56, 56), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n116.38 kiB\n6.12 kiB\n\n\nShape\n(19, 56, 56)\n(1, 56, 56)\n\n\nCount\n19 Tasks\n19 Chunks\n\n\nType\nint16\nnumpy.ndarray\n\n\n\n\n                                                             56 56 19\n\n\n\n\nCoordinates: (4)y(y)float644.551e+06 4.551e+06 ... 4.549e+06axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metrearray([4551045., 4551015., 4550985., 4550955., 4550925., 4550895., 4550865.,\n       4550835., 4550805., 4550775., 4550745., 4550715., 4550685., 4550655.,\n       4550625., 4550595., 4550565., 4550535., 4550505., 4550475., 4550445.,\n       4550415., 4550385., 4550355., 4550325., 4550295., 4550265., 4550235.,\n       4550205., 4550175., 4550145., 4550115., 4550085., 4550055., 4550025.,\n       4549995., 4549965., 4549935., 4549905., 4549875., 4549845., 4549815.,\n       4549785., 4549755., 4549725., 4549695., 4549665., 4549635., 4549605.,\n       4549575., 4549545., 4549515., 4549485., 4549455., 4549425., 4549395.])x(x)float647.796e+05 7.796e+05 ... 7.812e+05axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metrearray([779595., 779625., 779655., 779685., 779715., 779745., 779775., 779805.,\n       779835., 779865., 779895., 779925., 779955., 779985., 780015., 780045.,\n       780075., 780105., 780135., 780165., 780195., 780225., 780255., 780285.,\n       780315., 780345., 780375., 780405., 780435., 780465., 780495., 780525.,\n       780555., 780585., 780615., 780645., 780675., 780705., 780735., 780765.,\n       780795., 780825., 780855., 780885., 780915., 780945., 780975., 781005.,\n       781035., 781065., 781095., 781125., 781155., 781185., 781215., 781245.])time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :779580.0 30.0 0.0 4551060.0 0.0 -30.0array(0)Attributes: (3)scale_factor :0.0001add_offset :0.0_FillValue :-9999\n\n\nAbove, we persisted the clipped results to memory using the persist() method. This doesn’t necessarily need to be done, but it will substantially improve the performance of the visualization of the time series below.\n\n\nPlot red_clip with hvplot\n\nred_clip.hvplot.image(x='x', y='y', width=800, height=600, colorbar=True, cmap='Reds').opts(clim=(0.0, red_clip.values.max()))\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\nRead in the NIR and Fmask VRT files\n\n%%time\nchunks=dict(band=1, x=1024, y=1024)\nnir = rioxarray.open_rasterio('./data/nir_stack.vrt', chunks=chunks)                    # Read in VRT\nnir = nir.rename({'band':'time'})                                                       # Rename the 'band' coordinate variable to 'time' \nnir['time'] = [datetime.strptime(x.split('.')[-5], '%Y%jT%H%M%S') for x in links_vsi]   # Extract the time information from the input file names and assign them to the time coordinate variable\nnir = nir.sortby('time')                                                                # Sort by the time coordinate variable\nnir\n\nCPU times: user 69.9 ms, sys: 216 µs, total: 70.1 ms\nWall time: 81.9 ms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (time: 19, y: 3660, x: 3660)&gt;\ndask.array&lt;getitem, shape=(19, 3660, 3660), dtype=int16, chunksize=(1, 1024, 1024), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0xarray.DataArraytime: 19y: 3660x: 3660dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n485.45 MiB\n2.00 MiB\n\n\nShape\n(19, 3660, 3660)\n(1, 1024, 1024)\n\n\nCount\n609 Tasks\n304 Chunks\n\n\nType\nint16\nnumpy.ndarray\n\n\n\n\n                                                                         3660 3660 19\n\n\n\n\nCoordinates: (4)time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (3)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0\n\n\n\n%%time\nchunks=dict(band=1, x=1024, y=1024)\nfmask = rioxarray.open_rasterio('./data/fmask_stack.vrt', chunks=chunks)                    # Read in VRT\nfmask = fmask.rename({'band':'time'})                                                       # Rename the 'band' coordinate variable to 'time' \nfmask['time'] = [datetime.strptime(x.split('.')[-5], '%Y%jT%H%M%S') for x in links_vsi]     # Extract the time information from the input file names and assign them to the time coordinate variable\nfmask = fmask.sortby('time')                                                                # Sort by the time coordinate variable\nfmask\n\nCPU times: user 64.6 ms, sys: 85 µs, total: 64.7 ms\nWall time: 74.8 ms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (time: 19, y: 3660, x: 3660)&gt;\ndask.array&lt;getitem, shape=(19, 3660, 3660), dtype=uint8, chunksize=(1, 1024, 1024), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    255.0\n    scale_factor:  1.0\n    add_offset:    0.0xarray.DataArraytime: 19y: 3660x: 3660dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n242.73 MiB\n1.00 MiB\n\n\nShape\n(19, 3660, 3660)\n(1, 1024, 1024)\n\n\nCount\n609 Tasks\n304 Chunks\n\n\nType\nuint8\nnumpy.ndarray\n\n\n\n\n                                                                         3660 3660 19\n\n\n\n\nCoordinates: (4)time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (3)_FillValue :255.0scale_factor :1.0add_offset :0.0\n\n\n\n\nCreate an xarray dataset\nWe will now combine the RED, NIR, and Fmask arrays into a dataset and create/add a new NDVI variable.\n\nhls_ndvi = xr.Dataset({'red': red, 'nir': nir, 'fmask': fmask, 'ndvi': (nir - red) / (nir + red)})\nhls_ndvi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:      (time: 19, x: 3660, y: 3660)\nCoordinates:\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nData variables:\n    red          (time, y, x) int16 dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;\n    nir          (time, y, x) int16 dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;\n    fmask        (time, y, x) uint8 dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;\n    ndvi         (time, y, x) float64 dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;xarray.DatasetDimensions:time: 19x: 3660y: 3660Coordinates: (4)time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Data variables: (4)red(time, y, x)int16dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;_FillValue :-9999.0scale_factor :0.0001add_offset :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n485.45 MiB\n2.00 MiB\n\n\nShape\n(19, 3660, 3660)\n(1, 1024, 1024)\n\n\nCount\n609 Tasks\n304 Chunks\n\n\nType\nint16\nnumpy.ndarray\n\n\n\n\n                                                                         3660 3660 19\n\n\n\n\nnir(time, y, x)int16dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;_FillValue :-9999.0scale_factor :0.0001add_offset :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n485.45 MiB\n2.00 MiB\n\n\nShape\n(19, 3660, 3660)\n(1, 1024, 1024)\n\n\nCount\n609 Tasks\n304 Chunks\n\n\nType\nint16\nnumpy.ndarray\n\n\n\n\n                                                                         3660 3660 19\n\n\n\n\nfmask(time, y, x)uint8dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;_FillValue :255.0scale_factor :1.0add_offset :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n242.73 MiB\n1.00 MiB\n\n\nShape\n(19, 3660, 3660)\n(1, 1024, 1024)\n\n\nCount\n609 Tasks\n304 Chunks\n\n\nType\nuint8\nnumpy.ndarray\n\n\n\n\n                                                                         3660 3660 19\n\n\n\n\nndvi(time, y, x)float64dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.90 GiB\n8.00 MiB\n\n\nShape\n(19, 3660, 3660)\n(1, 1024, 1024)\n\n\nCount\n2130 Tasks\n304 Chunks\n\n\nType\nfloat64\nnumpy.ndarray\n\n\n\n\n                                                                         3660 3660 19\n\n\n\n\nAttributes: (0)\n\n\nAbove, we created a new NDVI variable. Now, we will clip and plot our results.\n\nndvi_clip = hls_ndvi.ndvi.rio.clip([fsUTM]).persist()\nndvi_clip\n\n/srv/conda/envs/notebook/lib/python3.7/site-packages/dask/core.py:119: RuntimeWarning: divide by zero encountered in true_divide\n  return func(*(_execute_task(a, cache) for a in args))\n/srv/conda/envs/notebook/lib/python3.7/site-packages/dask/core.py:119: RuntimeWarning: invalid value encountered in true_divide\n  return func(*(_execute_task(a, cache) for a in args))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'ndvi' (time: 19, y: 56, x: 56)&gt;\ndask.array&lt;getitem, shape=(19, 56, 56), dtype=float64, chunksize=(1, 56, 56), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * y            (y) float64 4.551e+06 4.551e+06 ... 4.549e+06 4.549e+06\n  * x            (x) float64 7.796e+05 7.796e+05 ... 7.812e+05 7.812e+05\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-17T17:...\n    spatial_ref  int64 0xarray.DataArray'ndvi'time: 19y: 56x: 56dask.array&lt;chunksize=(1, 56, 56), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n465.50 kiB\n24.50 kiB\n\n\nShape\n(19, 56, 56)\n(1, 56, 56)\n\n\nCount\n19 Tasks\n19 Chunks\n\n\nType\nfloat64\nnumpy.ndarray\n\n\n\n\n                                                             56 56 19\n\n\n\n\nCoordinates: (4)y(y)float644.551e+06 4.551e+06 ... 4.549e+06axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metrearray([4551045., 4551015., 4550985., 4550955., 4550925., 4550895., 4550865.,\n       4550835., 4550805., 4550775., 4550745., 4550715., 4550685., 4550655.,\n       4550625., 4550595., 4550565., 4550535., 4550505., 4550475., 4550445.,\n       4550415., 4550385., 4550355., 4550325., 4550295., 4550265., 4550235.,\n       4550205., 4550175., 4550145., 4550115., 4550085., 4550055., 4550025.,\n       4549995., 4549965., 4549935., 4549905., 4549875., 4549845., 4549815.,\n       4549785., 4549755., 4549725., 4549695., 4549665., 4549635., 4549605.,\n       4549575., 4549545., 4549515., 4549485., 4549455., 4549425., 4549395.])x(x)float647.796e+05 7.796e+05 ... 7.812e+05axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metrearray([779595., 779625., 779655., 779685., 779715., 779745., 779775., 779805.,\n       779835., 779865., 779895., 779925., 779955., 779985., 780015., 780045.,\n       780075., 780105., 780135., 780165., 780195., 780225., 780255., 780285.,\n       780315., 780345., 780375., 780405., 780435., 780465., 780495., 780525.,\n       780555., 780585., 780615., 780645., 780675., 780705., 780735., 780765.,\n       780795., 780825., 780855., 780885., 780915., 780945., 780975., 781005.,\n       781035., 781065., 781095., 781125., 781155., 781185., 781215., 781245.])time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:28:59.000000000', '2021-05-20T17:30:21.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-05T17:30:29.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-07-04T17:29:01.000000000',\n       '2021-07-07T17:30:37.000000000', '2021-07-09T17:28:59.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-08T17:30:49.000000000',\n       '2021-08-17T17:24:41.000000000'], dtype='datetime64[ns]')spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :779580.0 30.0 0.0 4551060.0 0.0 -30.0array(0)Attributes: (0)\n\n\n\nPlot NDVI\n\nndvi_clip.hvplot.image(x='x', y='y', groupby='time', width=800, height=600, colorbar=True, cmap='YlGn').opts(clim=(0.0, 1.0))\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nYou may have notices that some images for some of the time step are ‘blurrier’ than other. This is because they are contaminated in some way, be it clouds, cloud shadows, snow, ice.\n\n\n\nApply quality filter\nWe want to keep NDVI data values where Fmask equals 0 (no clouds, no cloud shadow, no snow/ice, no water.\n\nndvi_clip_filter = hls_ndvi.ndvi.where(fmask==0, np.nan).rio.clip([fsUTM]).persist()\n\n/srv/conda/envs/notebook/lib/python3.7/site-packages/dask/core.py:119: RuntimeWarning: divide by zero encountered in true_divide\n  return func(*(_execute_task(a, cache) for a in args))\n/srv/conda/envs/notebook/lib/python3.7/site-packages/dask/core.py:119: RuntimeWarning: invalid value encountered in true_divide\n  return func(*(_execute_task(a, cache) for a in args))\n\n\n\nndvi_clip_filter.hvplot.image(x='x', y='y', groupby='time', width=800, height=600, colorbar=True, cmap='YlGn').opts(clim=(0.0, 1.0))\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nAggregate by month\nFinally, we will use xarray’s groupby operation to aggregate by month.\n\nndvi_clip_filter.groupby('time.month').mean('time').hvplot.image(x = 'x', y = 'y', crs = hls_proj, groupby='month', cmap='YlGn', width=800, height=600, colorbar=True).opts(clim=(0.0, 1.0))\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nrio_env.__exit__()"
  },
  {
    "objectID": "tutorials/Additional_Resources__Direct_S3_Access__gdalvrt.html#references",
    "href": "tutorials/Additional_Resources__Direct_S3_Access__gdalvrt.html#references",
    "title": "Direct S3 Data Access with GDAL Virtual Raster Format (VRT)",
    "section": "References",
    "text": "References\n\nhttps://rasterio.readthedocs.io/en/latest/\nhttps://corteva.github.io/rioxarray/stable/index.html\nhttps://tutorial.dask.org/index.html\nhttps://examples.dask.org/applications/satellite-imagery-geotiff.html"
  },
  {
    "objectID": "tutorials/08_On-Prem_Cloud.html#timing",
    "href": "tutorials/08_On-Prem_Cloud.html#timing",
    "title": "08. Pairing Cloud and non-Cloud Data",
    "section": "Timing",
    "text": "Timing\n\nExercise: 45 min",
    "crumbs": [
      "Tutorials",
      "08. Pairing Cloud and non-Cloud Data"
    ]
  },
  {
    "objectID": "tutorials/08_On-Prem_Cloud.html#summary",
    "href": "tutorials/08_On-Prem_Cloud.html#summary",
    "title": "08. Pairing Cloud and non-Cloud Data",
    "section": "Summary",
    "text": "Summary\nThis tutorial will combine several workflow steps and components from the previous days, demonstrating the process of using the geolocation of data available outside of the Earthdata Cloud to then access coincident variables of cloud-accessible data. This may be a common use case as NASA Earthdata continues to migrate to the cloud, producing a “hybrid” data archive across Amazon Web Services (AWS) and original on-premise data storage systems. Additionally, you may also want to combine field measurements with remote sensing data available on the Earthdata Cloud.\nThis specific example explores the pairing of the ICESat-2 ATL07 Sea Ice Height data product, currently (as of November 2021) available publicly via direct download at the NSIDC DAAC, along with Sea Surface Temperature (SST) from the GHRSST MODIS L2 dataset (MODIS_A-JPL-L2P-v2019.0) available from PO.DAAC on the Earthdata Cloud.\nThe use case we’re looking at today centers over an area north of Greenland for a single day in June, where a melt pond was observed using the NASA OpenAltimetry application. Melt ponds are an important feature of Arctic sea ice dynamics, leading to an decrease in sea ice albedo and other changes in heat balance. Many NASA Earthdata datasets produce variables including sea ice albedo, sea surface temperature, air temperature, and sea ice height, which can be used to better understand these dynamics.\n\nObjectives\n\nPractice skills searching for data in CMR, determining granule coverage across two datasets over an area of interest.\nDownload data from an on-premise storage system to our cloud environment.\nRead in 1-dimensional trajectory data (ICESat-2 ATL07) into xarray and perform attribute conversions.\nSelect and read in sea surface temperature (SST) data (MODIS_A-JPL-L2P-v2019.0) from the Earthdata Cloud into xarray.\nExtract, resample, and plot coincident SST data based on ICESat-2 geolocation.",
    "crumbs": [
      "Tutorials",
      "08. Pairing Cloud and non-Cloud Data"
    ]
  },
  {
    "objectID": "tutorials/08_On-Prem_Cloud.html#import-packages",
    "href": "tutorials/08_On-Prem_Cloud.html#import-packages",
    "title": "08. Pairing Cloud and non-Cloud Data",
    "section": "Import packages",
    "text": "Import packages\n\nimport os\nfrom pathlib import Path\nfrom pprint import pprint\n\n# Access EDS\nimport requests\n\n# Access AWS S3\nimport s3fs\n\n# Read and work with datasets\nimport xarray as xr\nimport numpy as np\nimport h5py\n\n# For plotting\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nfrom shapely.geometry import box\n\n# For resampling\nimport pyresample",
    "crumbs": [
      "Tutorials",
      "08. Pairing Cloud and non-Cloud Data"
    ]
  },
  {
    "objectID": "tutorials/08_On-Prem_Cloud.html#specify-data-time-range-and-area-of-interest",
    "href": "tutorials/08_On-Prem_Cloud.html#specify-data-time-range-and-area-of-interest",
    "title": "08. Pairing Cloud and non-Cloud Data",
    "section": "Specify data, time range, and area of interest",
    "text": "Specify data, time range, and area of interest\nWe are going to focus on getting data for an area north of Greenland for a single day in June.\nThese bounding_box and temporal variables will be used for data search, subset, and access below:\n\n# Bounding Box spatial parameter in decimal degree 'W,S,E,N' format.\nbounding_box = '-62.8,81.7,-56.4,83'\n\n# Each date in yyyy-MM-ddTHH:mm:ssZ format; date range in start,end format\ntemporal = '2019-06-22T00:00:00Z,2019-06-22T23:59:59Z'\n\nSince we’ve already demonstrated how to locate a dataset’s collection_id and use the cloud_hosted parameter to determine whether a dataset resides in the Earthdata Cloud, we are going to skip forward and declare these variables:\n\nmodis_concept_id = 'C1940473819-POCLOUD'\nicesat2_concept_id = 'C2003771980-NSIDC_ECS'",
    "crumbs": [
      "Tutorials",
      "08. Pairing Cloud and non-Cloud Data"
    ]
  },
  {
    "objectID": "tutorials/08_On-Prem_Cloud.html#search-and-download-icesat-2-atl07-files",
    "href": "tutorials/08_On-Prem_Cloud.html#search-and-download-icesat-2-atl07-files",
    "title": "08. Pairing Cloud and non-Cloud Data",
    "section": "Search and download ICESat-2 ATL07 files",
    "text": "Search and download ICESat-2 ATL07 files\nPerform a granule search over our time and area of interest. How many granules are returned?\n\ngranule_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n\n\nresponse = requests.get(granule_url,\n                       params={\n                           'concept_id': icesat2_concept_id,\n                           'temporal': temporal,\n                           'bounding_box': bounding_box,\n                           'page_size': 200,\n                       },\n                       headers={\n                           'Accept': 'application/json'\n                       }\n                      )\nprint(response.headers['CMR-Hits'])\n\n2\n\n\nPrint the file names, size, and links:\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"producer_granule_id\"]} {granule[\"granule_size\"]} {granule[\"links\"][0][\"href\"]}')\n\nATL07-01_20190622055317_12980301_004_01.h5 237.0905504227 https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL07.004/2019.06.22/ATL07-01_20190622055317_12980301_004_01.h5\nATL07-01_20190622200154_13070301_004_01.h5 230.9151573181 https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL07.004/2019.06.22/ATL07-01_20190622200154_13070301_004_01.h5\n\n\n\nDownload ATL07 files\nAlthough several services are supported for ICESat-2 data, we are demonstrating direct access through the “on-prem” file system at NSIDC for simplicity.\nSome of these services include: - icepyx - From the icepyx documentation: “icepyx is both a software library and a community composed of ICESat-2 data users, developers, and the scientific community. We are working together to develop a shared library of resources - including existing resources, new code, tutorials, and use-cases/examples - that simplify the process of querying, obtaining, analyzing, and manipulating ICESat-2 datasets to enable scientific discovery.” - NSIDC DAAC Data Access and Service API - The API provided by the NSIDC DAAC allows you to access data programmatically using specific temporal and spatial filters. The same subsetting, reformatting, and reprojection services available on select data sets through NASA Earthdata Search can also be applied using this API. - IceFlow - The IceFlow python library simplifies accessing and combining data from several of NASA’s cryospheric altimetry missions, including ICESat/GLAS, Operation IceBridge, and ICESat-2. In particular, IceFlow harmonizes the various file formats and georeferencing parameters across several of the missions’ data sets, allowing you to analyze data across the multi-decadal time series.\nWe’ve found 2 granules. We’ll download the first one and write it to a file with the same name as the producer_granule_id.\nWe need the url for the granule as well. This is href links we printed out above.\n\nicesat_id = granules[0]['producer_granule_id']\nicesat_url = granules[0]['links'][0]['href']\n\nTo retrieve the granule data, we use the requests.get() method, which will utilize the .netrc file on the backend to authenticate the request against Earthdata Login.\n\nr = requests.get(icesat_url)\n\nThe response returned by requests has the same structure as all the other responses: a header and contents. The header information has information about the response, including the size of the data we downloaded in bytes.\n\nfor k, v in r.headers.items():\n    print(f'{k}: {v}')\n\nDate: Thu, 18 Nov 2021 04:02:03 GMT\nServer: Apache\nVary: User-Agent\nContent-Disposition: attachment\nContent-Length: 248607461\nKeep-Alive: timeout=15, max=100\nConnection: Keep-Alive\n\n\nThe contents needs to be saved to a file. To keep the directory clean, we will create a downloads directory to store the file. We can use a shell command to do this or use the makedirs method from the os package.\n\nos.makedirs(\"downloads\", exist_ok=True)\n\nYou should see a downloads directory in the file browser.\nTo write the data to a file, we use open to open a file. We need to specify that the file is open for writing by using the write-mode w. We also need to specify that we want to write bytes by setting the binary-mode b. This is important because the response contents are bytes. The default mode for open is text-mode. So make sure you use b.\nWe’ll use the with statement context-manager to open the file, write the contents of the response, and then close the file. Once the data in r.content is written sucessfully to the file, or if there is an error, the file is closed by the context-manager.\nWe also need to prepend the downloads path to the filename. We do this using Path from the pathlib package in the standard library.\n\noutfile = Path('downloads', icesat_id)\n\n\nif not outfile.exists():\n    with open(outfile, 'wb') as f:\n        f.write(r.content)\n\nATL07-01_20190622055317_12980301_004_01.h5 is an HDF5 file. xarray can open this but you need to tell it which group to read the data from. In this case we read the sea ice segment height data for ground-track 1 left-beam. You can explore the variable hierarchy in Earthdata Search, by selecting the Customize option under Download Data.\nThis code block performs the following operations: - Extracts the height_segment_height variable from the heights group, along with the dimension variables contained in the higher level sea_ice_segments group, - Convert attributes from bytestrings to strings, - Drops the HDF attribute DIMENSION_LIST, - Sets _FillValue to NaN\n\nvariable_names = [\n    '/gt1l/sea_ice_segments/latitude',\n    '/gt1l/sea_ice_segments/longitude',\n    '/gt1l/sea_ice_segments/delta_time',\n    '/gt1l/sea_ice_segments/heights/height_segment_height'\n    ]\nwith h5py.File(outfile, 'r') as h5:\n    data_vars = {}\n    for varname in variable_names:\n        var = h5[varname]\n        name = varname.split('/')[-1]\n        # Convert attributes\n        attrs = {}\n        for k, v in var.attrs.items():\n            if k != 'DIMENSION_LIST':\n                if isinstance(v, bytes):\n                    attrs[k] = v.decode('utf-8')\n                else:\n                    attrs[k] = v\n        data = var[:]\n        if '_FillValue' in attrs:\n            data = np.where(data &lt; attrs['_FillValue'], data, np.nan)\n        data_vars[name] = (['segment'], data, attrs)\n    is2_ds = xr.Dataset(data_vars)\n    \nis2_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:                (segment: 235584)\nDimensions without coordinates: segment\nData variables:\n    latitude               (segment) float64 82.38 82.38 82.38 ... 72.61 72.61\n    longitude              (segment) float64 -55.11 -55.11 ... 145.1 145.1\n    delta_time             (segment) float64 4.642e+07 4.642e+07 ... 4.642e+07\n    height_segment_height  (segment) float32 nan nan nan ... -0.4335 -0.4463xarray.DatasetDimensions:segment: 235584Coordinates: (0)Data variables: (4)latitude(segment)float6482.38 82.38 82.38 ... 72.61 72.61contentType :referenceInformationcoordinates :delta_time longitudedescription :Latitude, WGS84, North=+, Lat of segment centerlong_name :Latitudesource :ATBD, section 4.4standard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0array([82.38431982, 82.38431982, 82.38431982, ..., 72.60984638,\n       72.60977493, 72.60970985])longitude(segment)float64-55.11 -55.11 ... 145.1 145.1contentType :referenceInformationcoordinates :delta_time latitudedescription :Longitude, WGS84, East=+,Lon of segment centerlong_name :Longitudesource :ATBD, section 4.4standard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0array([-55.10896068, -55.10896068, -55.10896068, ..., 145.05396164,\n       145.05392851, 145.05389832])delta_time(segment)float644.642e+07 4.642e+07 ... 4.642e+07CLASS :DIMENSION_SCALENAME :gt1l/sea_ice_segments/delta_timeREFERENCE_LIST :[(&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)]contentType :physicalMeasurementcoordinates :latitude longitudedescription :Number of GPS seconds since the ATLAS SDP epoch. The ATLAS Standard Data Products (SDP) epoch offset is defined within /ancillary_data/atlas_sdp_gps_epoch as the number of GPS seconds between the GPS epoch (1980-01-06T00:00:00.000000Z UTC) and the ATLAS SDP epoch. By adding the offset contained within atlas_sdp_gps_epoch to delta time parameters, the time in gps_seconds relative to the GPS epoch can be computed.long_name :Elapsed GPS secondssource :telemetrystandard_name :timeunits :seconds since 2018-01-01array([46419293.64266939, 46419293.64266939, 46419293.64266939, ...,\n       46419681.87646231, 46419681.87759533, 46419681.87862704])height_segment_height(segment)float32nan nan nan ... -0.4335 -0.4463_FillValue :3.4028235e+38contentType :referenceInformationcoordinates :../delta_time ../latitude ../longitudedescription :Mean height from along-track segment fit detremined by the sea ice algorithm. The sea ice height is relative to the tide-free MSS.long_name :height of segment surfacesource :ATBD, section 4.2.2.4units :metersarray([        nan,         nan,         nan, ..., -0.46550068,\n       -0.43347716, -0.4462675 ], dtype=float32)Attributes: (0)\n\n\n\nis2_ds.height_segment_height.plot() ;",
    "crumbs": [
      "Tutorials",
      "08. Pairing Cloud and non-Cloud Data"
    ]
  },
  {
    "objectID": "tutorials/08_On-Prem_Cloud.html#determine-the-ghrsst-modis-l2-granules-returned-from-our-time-and-area-of-interest",
    "href": "tutorials/08_On-Prem_Cloud.html#determine-the-ghrsst-modis-l2-granules-returned-from-our-time-and-area-of-interest",
    "title": "08. Pairing Cloud and non-Cloud Data",
    "section": "Determine the GHRSST MODIS L2 granules returned from our time and area of interest",
    "text": "Determine the GHRSST MODIS L2 granules returned from our time and area of interest\n\nresponse = requests.get(granule_url, \n                        params={\n                            'concept_id': modis_concept_id,\n                            'temporal': temporal,\n                            'bounding_box': bounding_box,\n                            'page_size': 200,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nprint(response.headers['CMR-Hits'])\n\n14\n\n\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"title\"]} {granule[\"granule_size\"]} {granule[\"links\"][0][\"href\"]}')\n\n20190622000501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.71552562713623 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622000501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622014501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622014501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622032501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 21.307741165161133 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622032501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622050001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622050001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622050501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.065649032592773 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622050501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622064001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0 18.602201461791992 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0.nc\n20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 18.665077209472656 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622082001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.782299995422363 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622082001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.13440227508545 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622113501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.3239164352417 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622113501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622114001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622114001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622163001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.257243156433105 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622163001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622181001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.93498420715332 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622181001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc",
    "crumbs": [
      "Tutorials",
      "08. Pairing Cloud and non-Cloud Data"
    ]
  },
  {
    "objectID": "tutorials/08_On-Prem_Cloud.html#load-data-into-xarray-via-s3-direct-access",
    "href": "tutorials/08_On-Prem_Cloud.html#load-data-into-xarray-via-s3-direct-access",
    "title": "08. Pairing Cloud and non-Cloud Data",
    "section": "Load data into xarray via S3 direct access",
    "text": "Load data into xarray via S3 direct access\nOur CMR granule search returned 14 files for our time and area of interest. However, not all granules will be suitable for analysis.\nI’ve identified the image with granule id G1956158784-POCLOUD as a good candidate, this is the 9th granule. In this image, our area of interest is close to nadir. This means that the instantaneous field of view over the area of interest cover a smaller area than at the edge of the image.\nWe are looking for the link for direct download access via s3. This is a url but with a prefix s3://. This happens to be the first href link in the metadata.\nFor a single granule we can cut and paste the s3 link. If we have several granules, the s3 links can be extracted with some simple code.\n\ngranule = granules[9]\n\nfor link in granule['links']:\n    if link['href'].startswith('s3://'):\n        s3_link = link['href']\n        \ns3_link\n\n's3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc'\n\n\n\nGet S3 credentials\nAs with the previous S3 download tutorials we need credentials to access data from s3: access keys and tokens.\n\ns3_credentials = requests.get('https://archive.podaac.earthdata.nasa.gov/s3credentials').json()\n\nEssentially, what we are doing in this step is to “mount” the s3 bucket as a file system. This allows us to treat the S3 bucket in a similar way to a local file system.\n\ns3_fs = s3fs.S3FileSystem(\n    key=s3_credentials[\"accessKeyId\"],\n    secret=s3_credentials[\"secretAccessKey\"],\n    token=s3_credentials[\"sessionToken\"],\n)\n\n\n\nOpen a s3 file\nNow we have the S3FileSystem set up, we can access the granule. xarray cannot open a S3File directly, so we use the open method for the S3FileSystem to open the granule using the endpoint url we extracted from the metadata. We also have to set the mode='rb'. This opens the granule in read-only mode and in byte-mode. Byte-mode is important. By default, open opens a file as text - in this case it would just be a string of characters - and xarray doesn’t know what to do with that.\nWe then pass the S3File object f to xarray.open_dataset. For this dataset, we also have to set decode_cf=False. This switch tells xarray not to use information contained in variable attributes to generate human readable coordinate variables. Normally, this should work for netcdf files but for this particular cloud-hosted dataset, variable attribute data is not in the form expected by xarray. We’ll fix this.\n\nf = s3_fs.open(s3_link, mode='rb')\nmodis_ds = xr.open_dataset(f, decode_cf=False)\n\nIf you click on the Show/Hide Attributes icon (the first document-like icon to the right of coordinate variable metadata) you can see that attributes are one-element arrays containing bytestrings.\n\nmodis_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:                  (nj: 2030, ni: 1354, time: 1)\nCoordinates:\n  * time                     (time) int32 1214042401\nDimensions without coordinates: nj, ni\nData variables:\n    lat                      (nj, ni) float32 ...\n    lon                      (nj, ni) float32 ...\n    sea_surface_temperature  (time, nj, ni) int16 ...\n    sst_dtime                (time, nj, ni) int16 ...\n    quality_level            (time, nj, ni) int8 ...\n    sses_bias                (time, nj, ni) int8 ...\n    sses_standard_deviation  (time, nj, ni) int8 ...\n    l2p_flags                (time, nj, ni) int16 ...\n    chlorophyll_a            (time, nj, ni) float32 ...\n    K_490                    (time, nj, ni) int16 ...\n    wind_speed               (time, nj, ni) int8 ...\n    dt_analysis              (time, nj, ni) int8 ...\nAttributes: (12/49)\n    Conventions:                [b'CF-1.7, ACDD-1.3']\n    title:                      [b'MODIS Aqua L2P SST']\n    summary:                    [b'Sea surface temperature retrievals produce...\n    references:                 [b'GHRSST Data Processing Specification v2r5']\n    institution:                [b'NASA/JPL/OBPG/RSMAS']\n    history:                    [b'MODIS L2P created at JPL PO.DAAC']\n    ...                         ...\n    publisher_email:            [b'ghrsst-po@nceo.ac.uk']\n    processing_level:           [b'L2P']\n    cdm_data_type:              [b'swath']\n    startDirection:             [b'Ascending']\n    endDirection:               [b'Descending']\n    day_night_flag:             [b'Day']xarray.DatasetDimensions:nj: 2030ni: 1354time: 1Coordinates: (1)time(time)int321214042401long_name :[b'reference time of sst file']standard_name :[b'time']units :[b'seconds since 1981-01-01 00:00:00']comment :[b'time of first sensor observation']coverage_content_type :[b'coordinate']array([1214042401], dtype=int32)Data variables: (12)lat(nj, ni)float32...long_name :[b'latitude']standard_name :[b'latitude']units :[b'degrees_north']_FillValue :[-999.]valid_min :[-90.]valid_max :[90.]comment :[b'geographical coordinates, WGS84 projection']coverage_content_type :[b'coordinate'][2748620 values with dtype=float32]lon(nj, ni)float32...long_name :[b'longitude']standard_name :[b'longitude']units :[b'degrees_east']_FillValue :[-999.]valid_min :[-180.]valid_max :[180.]comment :[b'geographical coordinates, WGS84 projection']coverage_content_type :[b'coordinate'][2748620 values with dtype=float32]sea_surface_temperature(time, nj, ni)int16...long_name :[b'sea surface temperature']standard_name :[b'sea_surface_skin_temperature']units :[b'kelvin']_FillValue :[-32767]valid_min :[-1000]valid_max :[10000]comment :[b'sea surface temperature from thermal IR (11 um) channels']scale_factor :[0.005]add_offset :[273.15]source :[b'NASA and University of Miami']coordinates :[b'lon lat']coverage_content_type :[b'physicalMeasurement'][2748620 values with dtype=int16]sst_dtime(time, nj, ni)int16...long_name :[b'time difference from reference time']units :[b'seconds']_FillValue :[-32768]valid_min :[-32767]valid_max :[32767]comment :[b'time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981']coordinates :[b'lon lat']coverage_content_type :[b'referenceInformation'][2748620 values with dtype=int16]quality_level(time, nj, ni)int8...long_name :[b'quality level of SST pixel']_FillValue :[-128]valid_min :[0]valid_max :[5]comment :[b'thermal IR SST proximity confidence value; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered &gt; 127, please subtract 256 from this reported value']coordinates :[b'lon lat']flag_values :[0 1 2 3 4 5]flag_meanings :[b'no_data bad_data worst_quality low_quality acceptable_quality best_quality']coverage_content_type :[b'qualityInformation'][2748620 values with dtype=int8]sses_bias(time, nj, ni)int8...long_name :[b'SSES bias error based on proximity confidence flags']units :[b'kelvin']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'thermal IR SST bias error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered &gt; 127, please subtract 256 from this reported value']scale_factor :[0.15748031]add_offset :[0.]coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]sses_standard_deviation(time, nj, ni)int8...long_name :[b'SSES standard deviation error based on proximity confidence flags']units :[b'kelvin']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'thermal IR SST standard deviation error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered &gt; 127, please subtract 256 from this reported value']scale_factor :[0.07874016]add_offset :[10.]coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]l2p_flags(time, nj, ni)int16...long_name :[b'L2P flags']valid_min :[0]valid_max :[16]comment :[b'These flags can be used to further filter data variables']coordinates :[b'lon lat']flag_meanings :[b'microwave land ice lake river']flag_masks :[ 1  2  4  8 16]coverage_content_type :[b'qualityInformation'][2748620 values with dtype=int16]chlorophyll_a(time, nj, ni)float32...long_name :[b'Chlorophyll Concentration, OC3 Algorithm']units :[b'mg m^-3']_FillValue :[-32767.]valid_min :[0.001]valid_max :[100.]comment :[b'non L2P core field']coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=float32]K_490(time, nj, ni)int16...long_name :[b'Diffuse attenuation coefficient at 490 nm (OBPG)']units :[b'm^-1']_FillValue :[-32767]valid_min :[50]valid_max :[30000]comment :[b'non L2P core field']scale_factor :[0.0002]add_offset :[0.]coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int16]wind_speed(time, nj, ni)int8...long_name :[b'10m wind speed']standard_name :[b'wind_speed']units :[b'm s-1']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'Wind at 10 meters above the sea surface']scale_factor :[0.2]add_offset :[25.]source :[b'TBD.  Placeholder.  Currently empty']coordinates :[b'lon lat']grid_mapping :[b'TBD']time_offset :[2.]height :[b'10 m']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]dt_analysis(time, nj, ni)int8...long_name :[b'deviation from SST reference climatology']units :[b'kelvin']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'TBD']scale_factor :[0.1]add_offset :[0.]source :[b'TBD. Placeholder.  Currently empty']coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]Attributes: (49)Conventions :[b'CF-1.7, ACDD-1.3']title :[b'MODIS Aqua L2P SST']summary :[b'Sea surface temperature retrievals produced at the NASA OBPG for the MODIS Aqua sensor.  These have been reformatted to GHRSST GDS specifications by the JPL PO.DAAC']references :[b'GHRSST Data Processing Specification v2r5']institution :[b'NASA/JPL/OBPG/RSMAS']history :[b'MODIS L2P created at JPL PO.DAAC']comment :[b'L2P Core without DT analysis or other ancillary fields; Day, Start Node:Ascending, End Node:Descending; WARNING Some applications are unable to properly handle signed byte values. If values are encountered &gt; 127, please subtract 256 from this reported value; Refined']license :[b'GHRSST and PO.DAAC protocol allow data use as free and open.']id :[b'MODIS_A-JPL-L2P-v2019.0']naming_authority :[b'org.ghrsst']product_version :[b'2019.0']uuid :[b'f6e1f61d-c4a4-4c17-8354-0c15e12d688b']gds_version_id :[b'2.0']netcdf_version_id :[b'4.1']date_created :[b'20200221T085224Z']file_quality_level :[3]spatial_resolution :[b'1km']start_time :[b'20190622T100001Z']time_coverage_start :[b'20190622T100001Z']stop_time :[b'20190622T100459Z']time_coverage_end :[b'20190622T100459Z']northernmost_latitude :[89.9862]southernmost_latitude :[66.2723]easternmost_longitude :[-45.9467]westernmost_longitude :[152.489]source :[b'MODIS sea surface temperature observations for the OBPG']platform :[b'Aqua']sensor :[b'MODIS']metadata_link :[b'http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MODIS_A-JPL-L2P-v2019.0']keywords :[b'Oceans &gt; Ocean Temperature &gt; Sea Surface Temperature']keywords_vocabulary :[b'NASA Global Change Master Directory (GCMD) Science Keywords']standard_name_vocabulary :[b'NetCDF Climate and Forecast (CF) Metadata Convention']geospatial_lat_units :[b'degrees_north']geospatial_lat_resolution :[0.01]geospatial_lon_units :[b'degrees_east']geospatial_lon_resolution :[0.01]acknowledgment :[b'The MODIS L2P sea surface temperature data are sponsored by NASA']creator_name :[b'Ed Armstrong, JPL PO.DAAC']creator_email :[b'edward.m.armstrong@jpl.nasa.gov']creator_url :[b'http://podaac.jpl.nasa.gov']project :[b'Group for High Resolution Sea Surface Temperature']publisher_name :[b'The GHRSST Project Office']publisher_url :[b'http://www.ghrsst.org']publisher_email :[b'ghrsst-po@nceo.ac.uk']processing_level :[b'L2P']cdm_data_type :[b'swath']startDirection :[b'Ascending']endDirection :[b'Descending']day_night_flag :[b'Day']\n\n\nTo fix this, we need to extract array elements as scalars, and convert those scalars from bytestrings to strings. We use the decode method to do this. The bytestrings are encoded as utf-8, which is a unicode character format. This is the default encoding for decode but we’ve included it as an argument to be explicit.\nNot all attributes are bytestrings. Some are floats. Take a look at _FillValue, and valid_min and valid_max. To avoid an error, we use the isinstance function to check if the value of an attributes is type bytes - a bytestring. If it is, then we decode it. If not, we just extract the scalar and do nothing else.\nWe also fix the global attributes.\n\ndef fix_attributes(da):\n    '''Decodes bytestring attributes to strings'''\n    for attr, value in da.attrs.items():\n        if isinstance(value[0], bytes):\n            da.attrs[attr] = value[0].decode('utf-8')\n        else:\n            da.attrs[attr] = value[0]\n    return\n\n# Fix variable attributes\nfor var in modis_ds.variables:\n    da = modis_ds[var]\n    fix_attributes(da)\n            \n# Fix global attributes\nfix_attributes(modis_ds)\n\nWith this done, we can use the xarray function decode_cf to convert the attributes.\n\nmodis_ds = xr.decode_cf(modis_ds)\n\n\nmodis_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:                  (nj: 2030, ni: 1354, time: 1)\nCoordinates:\n    lat                      (nj, ni) float32 ...\n    lon                      (nj, ni) float32 ...\n  * time                     (time) datetime64[ns] 2019-06-22T10:00:01\nDimensions without coordinates: nj, ni\nData variables:\n    sea_surface_temperature  (time, nj, ni) float32 ...\n    sst_dtime                (time, nj, ni) timedelta64[ns] ...\n    quality_level            (time, nj, ni) float32 ...\n    sses_bias                (time, nj, ni) float32 ...\n    sses_standard_deviation  (time, nj, ni) float32 ...\n    l2p_flags                (time, nj, ni) int16 ...\n    chlorophyll_a            (time, nj, ni) float32 ...\n    K_490                    (time, nj, ni) float32 ...\n    wind_speed               (time, nj, ni) float32 ...\n    dt_analysis              (time, nj, ni) float32 ...\nAttributes: (12/49)\n    Conventions:                CF-1.7, ACDD-1.3\n    title:                      MODIS Aqua L2P SST\n    summary:                    Sea surface temperature retrievals produced a...\n    references:                 GHRSST Data Processing Specification v2r5\n    institution:                NASA/JPL/OBPG/RSMAS\n    history:                    MODIS L2P created at JPL PO.DAAC\n    ...                         ...\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L2P\n    cdm_data_type:              swath\n    startDirection:             Ascending\n    endDirection:               Descending\n    day_night_flag:             Dayxarray.DatasetDimensions:nj: 2030ni: 1354time: 1Coordinates: (3)lat(nj, ni)float32...long_name :latitudestandard_name :latitudeunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[2748620 values with dtype=float32]lon(nj, ni)float32...long_name :longitudestandard_name :longitudeunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[2748620 values with dtype=float32]time(time)datetime64[ns]2019-06-22T10:00:01long_name :reference time of sst filestandard_name :timecomment :time of first sensor observationcoverage_content_type :coordinatearray(['2019-06-22T10:00:01.000000000'], dtype='datetime64[ns]')Data variables: (10)sea_surface_temperature(time, nj, ni)float32...long_name :sea surface temperaturestandard_name :sea_surface_skin_temperatureunits :kelvinvalid_min :-1000valid_max :10000comment :sea surface temperature from thermal IR (11 um) channelssource :NASA and University of Miamicoverage_content_type :physicalMeasurement[2748620 values with dtype=float32]sst_dtime(time, nj, ni)timedelta64[ns]...long_name :time difference from reference timevalid_min :-32767valid_max :32767comment :time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981coverage_content_type :referenceInformation[2748620 values with dtype=timedelta64[ns]]quality_level(time, nj, ni)float32...long_name :quality level of SST pixelvalid_min :0valid_max :5comment :thermal IR SST proximity confidence value; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered &gt; 127, please subtract 256 from this reported valueflag_values :0flag_meanings :no_data bad_data worst_quality low_quality acceptable_quality best_qualitycoverage_content_type :qualityInformation[2748620 values with dtype=float32]sses_bias(time, nj, ni)float32...long_name :SSES bias error based on proximity confidence flagsunits :kelvinvalid_min :-127valid_max :127comment :thermal IR SST bias error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered &gt; 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]sses_standard_deviation(time, nj, ni)float32...long_name :SSES standard deviation error based on proximity confidence flagsunits :kelvinvalid_min :-127valid_max :127comment :thermal IR SST standard deviation error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered &gt; 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]l2p_flags(time, nj, ni)int16...long_name :L2P flagsvalid_min :0valid_max :16comment :These flags can be used to further filter data variablesflag_meanings :microwave land ice lake riverflag_masks :1coverage_content_type :qualityInformation[2748620 values with dtype=int16]chlorophyll_a(time, nj, ni)float32...long_name :Chlorophyll Concentration, OC3 Algorithmunits :mg m^-3valid_min :0.001valid_max :100.0comment :non L2P core fieldcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]K_490(time, nj, ni)float32...long_name :Diffuse attenuation coefficient at 490 nm (OBPG)units :m^-1valid_min :50valid_max :30000comment :non L2P core fieldcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]wind_speed(time, nj, ni)float32...long_name :10m wind speedstandard_name :wind_speedunits :m s-1valid_min :-127valid_max :127comment :Wind at 10 meters above the sea surfacesource :TBD.  Placeholder.  Currently emptygrid_mapping :TBDtime_offset :2.0height :10 mcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]dt_analysis(time, nj, ni)float32...long_name :deviation from SST reference climatologyunits :kelvinvalid_min :-127valid_max :127comment :TBDsource :TBD. Placeholder.  Currently emptycoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]Attributes: (49)Conventions :CF-1.7, ACDD-1.3title :MODIS Aqua L2P SSTsummary :Sea surface temperature retrievals produced at the NASA OBPG for the MODIS Aqua sensor.  These have been reformatted to GHRSST GDS specifications by the JPL PO.DAACreferences :GHRSST Data Processing Specification v2r5institution :NASA/JPL/OBPG/RSMAShistory :MODIS L2P created at JPL PO.DAACcomment :L2P Core without DT analysis or other ancillary fields; Day, Start Node:Ascending, End Node:Descending; WARNING Some applications are unable to properly handle signed byte values. If values are encountered &gt; 127, please subtract 256 from this reported value; Refinedlicense :GHRSST and PO.DAAC protocol allow data use as free and open.id :MODIS_A-JPL-L2P-v2019.0naming_authority :org.ghrsstproduct_version :2019.0uuid :f6e1f61d-c4a4-4c17-8354-0c15e12d688bgds_version_id :2.0netcdf_version_id :4.1date_created :20200221T085224Zfile_quality_level :3spatial_resolution :1kmstart_time :20190622T100001Ztime_coverage_start :20190622T100001Zstop_time :20190622T100459Ztime_coverage_end :20190622T100459Znorthernmost_latitude :89.9862southernmost_latitude :66.2723easternmost_longitude :-45.9467westernmost_longitude :152.489source :MODIS sea surface temperature observations for the OBPGplatform :Aquasensor :MODISmetadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MODIS_A-JPL-L2P-v2019.0keywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventiongeospatial_lat_units :degrees_northgeospatial_lat_resolution :0.01geospatial_lon_units :degrees_eastgeospatial_lon_resolution :0.01acknowledgment :The MODIS L2P sea surface temperature data are sponsored by NASAcreator_name :Ed Armstrong, JPL PO.DAACcreator_email :edward.m.armstrong@jpl.nasa.govcreator_url :http://podaac.jpl.nasa.govproject :Group for High Resolution Sea Surface Temperaturepublisher_name :The GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L2Pcdm_data_type :swathstartDirection :AscendingendDirection :Descendingday_night_flag :Day\n\n\nLet’s make a quick plot to take a look at the sea_surface_temperature variable.\n\nmodis_ds.sea_surface_temperature.plot() ;\n\n\n\n\n\n\n\n\n\n\nPlot MODIS and ICESat-2 data on a map\n\nmap_proj = ccrs.NorthPolarStereo()\n\nfig = plt.figure(figsize=(10,5))\nax = fig.add_subplot(projection=map_proj)\nax.coastlines()\n\n# Plot MODIS sst, save object as sst_img, so we can add colorbar\nsst_img = ax.pcolormesh(modis_ds.lon, modis_ds.lat, modis_ds.sea_surface_temperature[0,:,:], \n                        vmin=240, vmax=270,  # Set max and min values for plotting\n                        cmap='viridis', shading='auto',   # shading='auto' to avoid warning\n                        transform=ccrs.PlateCarree())  # coords are lat,lon but map if NPS \n\n# Plot IS2 surface height \nis2_img = ax.scatter(is2_ds.longitude, is2_ds.latitude,\n                     c=is2_ds.height_segment_height, \n                     vmax=1.5,  # Set max height to plot\n                     cmap='Reds', alpha=0.6, s=2,\n                     transform=ccrs.PlateCarree())\n\n# Add colorbars\nfig.colorbar(sst_img, label='MODIS SST (K)')\nfig.colorbar(is2_img, label='ATL07 Height (m)')\n\n\n\n\n\n\n\n\n\n\n\nExtract SST coincident with ICESat-2 track\nThe MODIS SST is swath data, not a regularly-spaced grid of sea surface temperatures. ICESat-2 sea surface heights are irregularly spaced segments along one ground-track traced by the ATLAS instrument on-board ICESat-2. Fortunately, pyresample allows us to resample swath data.\npyresample has many resampling methods. We’re going to use the nearest neighbour resampling method, which is implemented using a k-dimensional tree algorithm or K-d tree. K-d trees are data structures that improve search efficiency for large data sets.\nThe first step is to define the geometry of the ICESat-2 and MODIS data. To do this we use the latitudes and longitudes of the datasets.\n\nis2_geometry = pyresample.SwathDefinition(lons=is2_ds.longitude,\n                                          lats=is2_ds.latitude)\n\n\nmodis_geometry = pyresample.SwathDefinition(lons=modis_ds.lon, lats=modis_ds.lat)\n\nWe then implement the resampling method, passing the two geometries we have defined, the data array we want to resample - in this case sea surface temperature, and a search radius. The resampling method expects a numpy.Array rather than an xarray.DataArray, so we use values to get the data as a numpy.Array.\nWe set the search radius to 1000 m. The MODIS data is nominally 1km spacing.\n\nsearch_radius=1000.\nfill_value = np.nan\nis2_sst = pyresample.kd_tree.resample_nearest(\n    modis_geometry,\n    modis_ds.sea_surface_temperature.values,\n    is2_geometry,\n    search_radius,\n    fill_value=fill_value\n)\n\n\nis2_sst\n\narray([263.375, 263.375, 263.375, ...,     nan,     nan,     nan],\n      dtype=float32)\n\n\n\nis2_ds['sea_surface_temperature'] = xr.DataArray(is2_sst, dims='segment')\nis2_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:                  (segment: 235584)\nDimensions without coordinates: segment\nData variables:\n    latitude                 (segment) float64 82.38 82.38 82.38 ... 72.61 72.61\n    longitude                (segment) float64 -55.11 -55.11 ... 145.1 145.1\n    delta_time               (segment) float64 4.642e+07 4.642e+07 ... 4.642e+07\n    height_segment_height    (segment) float32 nan nan nan ... -0.4335 -0.4463\n    sea_surface_temperature  (segment) float32 263.4 263.4 263.4 ... nan nan nanxarray.DatasetDimensions:segment: 235584Coordinates: (0)Data variables: (5)latitude(segment)float6482.38 82.38 82.38 ... 72.61 72.61contentType :referenceInformationcoordinates :delta_time longitudedescription :Latitude, WGS84, North=+, Lat of segment centerlong_name :Latitudesource :ATBD, section 4.4standard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0array([82.38431982, 82.38431982, 82.38431982, ..., 72.60984638,\n       72.60977493, 72.60970985])longitude(segment)float64-55.11 -55.11 ... 145.1 145.1contentType :referenceInformationcoordinates :delta_time latitudedescription :Longitude, WGS84, East=+,Lon of segment centerlong_name :Longitudesource :ATBD, section 4.4standard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0array([-55.10896068, -55.10896068, -55.10896068, ..., 145.05396164,\n       145.05392851, 145.05389832])delta_time(segment)float644.642e+07 4.642e+07 ... 4.642e+07CLASS :DIMENSION_SCALENAME :gt1l/sea_ice_segments/delta_timeREFERENCE_LIST :[(&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)\n (&lt;HDF5 object reference&gt;, 0) (&lt;HDF5 object reference&gt;, 0)]contentType :physicalMeasurementcoordinates :latitude longitudedescription :Number of GPS seconds since the ATLAS SDP epoch. The ATLAS Standard Data Products (SDP) epoch offset is defined within /ancillary_data/atlas_sdp_gps_epoch as the number of GPS seconds between the GPS epoch (1980-01-06T00:00:00.000000Z UTC) and the ATLAS SDP epoch. By adding the offset contained within atlas_sdp_gps_epoch to delta time parameters, the time in gps_seconds relative to the GPS epoch can be computed.long_name :Elapsed GPS secondssource :telemetrystandard_name :timeunits :seconds since 2018-01-01array([46419293.64266939, 46419293.64266939, 46419293.64266939, ...,\n       46419681.87646231, 46419681.87759533, 46419681.87862704])height_segment_height(segment)float32nan nan nan ... -0.4335 -0.4463_FillValue :3.4028235e+38contentType :referenceInformationcoordinates :../delta_time ../latitude ../longitudedescription :Mean height from along-track segment fit detremined by the sea ice algorithm. The sea ice height is relative to the tide-free MSS.long_name :height of segment surfacesource :ATBD, section 4.2.2.4units :metersarray([        nan,         nan,         nan, ..., -0.46550068,\n       -0.43347716, -0.4462675 ], dtype=float32)sea_surface_temperature(segment)float32263.4 263.4 263.4 ... nan nan nanarray([263.375, 263.375, 263.375, ...,     nan,     nan,     nan],\n      dtype=float32)Attributes: (0)\n\n\n\n\nPlot SST and Height along track\nThis is a quick plot of the extracted data. We’re using matplotlib so we can use latitude as the x-value:\n\nis2_ds = is2_ds.set_coords(['latitude'])\n\nfig, ax1 = plt.subplots(figsize=(15, 7))\nax1.set_xlim(82.,88.)\nax1.plot(is2_ds.latitude, is2_ds.sea_surface_temperature, \n         color='orange', label='SST', zorder=3)\nax1.set_ylabel('SST (K)')\n\nax2 = ax1.twinx()\nax2.plot(is2_ds.latitude, is2_ds.height_segment_height, label='Height')\nax2.set_ylabel('Height (m)')\n\nfig.legend()",
    "crumbs": [
      "Tutorials",
      "08. Pairing Cloud and non-Cloud Data"
    ]
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials Overview",
    "section": "",
    "text": "These tutorials are a combination of narrative, links, code, and outputs. They have been developed for live-coding during the Hackathon, and are available for self-paced learning.\nTutorials are markdown (.md) and Jupyter (.ipynb) notebooks, and are available on GitHub:\nhttps://github.com/NASA-Openscapes/2021-Cloud-Hackathon/tree/main/tutorials.",
    "crumbs": [
      "Tutorials",
      "Tutorials Overview"
    ]
  },
  {
    "objectID": "logistics/schedule.html",
    "href": "logistics/schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The 2021 Cloud Hackathon will take place November 15-19 from 9am-1pm PST (UTC-8) each day.\nAn optional pre-hackathon clinic will be held on November 9 from 8-10am PST, and the recording will be made available to all hackathon participants to review on their own.\nAll Cloud Hackathon activities are virtual. We will engage primarily with this group and schedule via Zoom and Slack; see prerequisites & getting help.\nZoom links will be posted on the 2021-nasacloudhack-general Slack channel",
    "crumbs": [
      "Logistics",
      "Schedule"
    ]
  },
  {
    "objectID": "logistics/schedule.html#hackathon-day-1-november-15",
    "href": "logistics/schedule.html#hackathon-day-1-november-15",
    "title": "Schedule",
    "section": "Hackathon Day 1: November 15",
    "text": "Hackathon Day 1: November 15\n\n\n\nTime, PST (UTC-8)\nEvent\nLeads/Instructors\n\n\n\n\n9:00 am\nWelcome\nErin Robinson & Julie Lowndes, Openscapes\n\n\n9:25 am\nCloud Paradigm Overview\nCatalina Oaida, PO.DAAC\n\n\n9:45 am\nTutorial 0: Getting set up and connected\nLuis Lopez, NSIDC and Makhan Virdi, ASDC\n\n\n10:45 am\nQ&A and Break\n\n\n\n11:00 am\nTutorial 1: Data discovery with CMR\nAndy Barrett and Luis Lopez, NSIDC\n\n\n11:30 am\nTutorial 2: Data discovery with CMR-STAC API\nAaron Friesz, LP DAAC\n\n\n12:00 pm\nQ&A and Break\n\n\n\n12:15 pm\nProjects Pitchfest\nCatalina Oaida, PO.DAAC\n\n\n12:55 pm\nClosing\nErin Robinson, Openscapes\n\n\n\n\nWelcome Day 1\nPlease see the CloudHackathon_Notes Google Doc:\n\nWelcome and Code of Conduct\nLogistics\nMeet your neighbors\n\n\n\nClosing Day 1\n\nThank you!\nJupyterHub: close out.\n\nclose out your JupyterHub instance if you are finished for the day, following these instructions.\n\nContinued work.\n\nYou’re welcome to continue working beyond the hackathon daily scheduled session, using JupyterHub and Slack. Note that questions in Slack, including 2021-nasacloudhack-help might have delayed Helper response outside of the daily scheduled cloud hackathon times.\n\nAgenda for tomorrow: what’s coming next.",
    "crumbs": [
      "Logistics",
      "Schedule"
    ]
  },
  {
    "objectID": "logistics/schedule.html#hackathon-day-2-november-16",
    "href": "logistics/schedule.html#hackathon-day-2-november-16",
    "title": "Schedule",
    "section": "Hackathon Day 2: November 16",
    "text": "Hackathon Day 2: November 16\n\n\n\nTime, PST (UTC-8)\nEvent\nLeads/Instructors\n\n\n\n\n8:00 am\nOptional Catch-up/help\n\n\n\n9:00 am\nWelcome\nJulie Lowndes, Openscapes\n\n\n9:15 am\nDemo: the power of Earthdata Cloud and Q&A\nMarisol Garcia Reyes, Farallon Institute\n\n\n9:35 am\nTutorial 3: Introduction to Xarray\nAndy Barrett, NSIDC, Aaron Friesz, LP DAAC\n\n\n10:30 am\nQ&A and Break\n\n\n\n10:50 am\nTutorial 4: Authentication for NASA Earthdata\nAaron Friesz, LP DAAC\n\n\n11:05 am\nTutorial 5: Direct S3 Access\nAaron Friesz, LP DAAC\n\n\n11:35 am\nTeam Hack Time\nAll\n\n\n12:55 pm\nClosing\nJulie Lowndes, Openscapes\n\n\n\n\nWelcome Day 2\n\nJupyterHub: Log in.\n\nLog into 2i2c. This takes a few minutes so please start this as soon we reconvene each day\n\nGitHub: Get the latest.\n\nYou’ll need to git fetch, merge, and pull: follow the github workflows daily setup.\n\n\n\n\nClosing Day 2\n\nThank you!\nJupyterHub: close out.\n\nclose out your JupyterHub instance if you are finished for the day, following these instructions.\n\nContinued work.\n\nYou’re welcome to continue working beyond the hackathon daily scheduled session, using JupyterHub and Slack. Note that questions in Slack, including 2021-nasacloudhack-help might have delayed Helper response outside of the daily scheduled cloud hackathon times.\n\nAgenda for tomorrow: what’s coming next.",
    "crumbs": [
      "Logistics",
      "Schedule"
    ]
  },
  {
    "objectID": "logistics/schedule.html#hackathon-day-3-november-17",
    "href": "logistics/schedule.html#hackathon-day-3-november-17",
    "title": "Schedule",
    "section": "Hackathon Day 3: November 17",
    "text": "Hackathon Day 3: November 17\n\n\n\nTime, PST (UTC-8)\nEvent\nLeads/Instructors\n\n\n\n\n8:00 am\nOptional Catch-up/help\n\n\n\n9:00 am\nWelcome\nCatalina Oaida, PO.DAAC\n\n\n9:15 am\nTutorial 6: Sentinel-6 MF L2 Altimetry Data Access (OPeNDAP) & Gridding\nJack McNelis, PO.DAAC\n\n\n9:45 am\nQ&A and Break\n\n\n\n10:00 am\nTutorial 7: Data Subsetting and Transformation Services in the Cloud\nAmy Steiker, NSIDC\n\n\n10:40 am\nQ&A and Break\n\n\n\n11:00 am\nTeam Hack Time\nAll\n\n\n12:55 pm\nClosing\nCatalina Oaida, PO.DAAC\n\n\n\n\nWelcome Day 3\n\nJupyterHub: Log in.\n\nLog into 2i2c. This takes a few minutes so please start this as soon we reconvene each day\n\nGitHub: Get the latest.\n\nYou’ll need to git fetch, merge, and pull: follow the github workflows daily setup.\n\n\n\n\nClosing Day 3\n\nThank you!\nJupyterHub: close out.\n\nclose out your JupyterHub instance if you are finished for the day, following these instructions.\n\nContinued work.\n\nYou’re welcome to continue working beyond the hackathon daily scheduled session, using JupyterHub and Slack. Note that questions in Slack, including 2021-nasacloudhack-help might have delayed Helper response outside of the daily scheduled cloud hackathon times.\n\nAgenda for tomorrow: what’s coming next.",
    "crumbs": [
      "Logistics",
      "Schedule"
    ]
  },
  {
    "objectID": "logistics/schedule.html#hackathon-day-4-november-18",
    "href": "logistics/schedule.html#hackathon-day-4-november-18",
    "title": "Schedule",
    "section": "Hackathon Day 4: November 18",
    "text": "Hackathon Day 4: November 18\n\n\n\nTime, PST (UTC-8)\nEvent\nLeads/Instructors\n\n\n\n\n8:00 am\nOptional Catch-up/help\n\n\n\n9:00 am\nWelcome\nErin Robinson, Openscapes\n\n\n9:15 am\nTutorial 9: Access COF data vis Zarr EOSDIS Store\nPatrick Quinn, Element84\n\n\n10:00 am\nQ&A and Break\n\n\n\n10:15 am\nTutorial 8: EDC and on-prem DAAC hybrid use case\nAmy Steiker, NSIDC\n\n\n11:00 am\nQ&A and Break\n\n\n\n11:20 am\nTeam Hack Time\nAll\n\n\n12:55 pm\nClosing\nErin Robinson, Openscapes\n\n\n\n\nWelcome Day 4\n\nJupyterHub: Log in.\n\nLog into 2i2c. This takes a few minutes so please start this as soon we reconvene each day\n\nGitHub: Get the latest.\n\nYou’ll need to git fetch, merge, and pull: follow the github workflows daily setup.\n\n\n\n\nClosing Day 4\n\nThank you!\nJupyterHub: close out.\n\nclose out your JupyterHub instance if you are finished for the day, following these instructions.\n\nContinued work.\n\nYou’re welcome to continue working beyond the hackathon daily scheduled session, using JupyterHub and Slack. Note that questions in Slack, including 2021-nasacloudhack-help might have delayed Helper response outside of the daily scheduled cloud hackathon times.\n\nAgenda for tomorrow: what’s coming next.",
    "crumbs": [
      "Logistics",
      "Schedule"
    ]
  },
  {
    "objectID": "logistics/schedule.html#hackathon-day-5-november-19",
    "href": "logistics/schedule.html#hackathon-day-5-november-19",
    "title": "Schedule",
    "section": "Hackathon Day 5: November 19",
    "text": "Hackathon Day 5: November 19\n\n\n\nTime, PST (UTC-8)\nEvent\nLeads/Instructors\n\n\n\n\n8:00 am\nOptional Catch-up/help\n\n\n\n9:00 am\nWelcome\nCatalina Oaida, PO.DAAC\n\n\n9:05 am\nTeam Hack Time\nAll\n\n\n10:00 am\nTeam Report-outs Part 1\nTeams 1-6\n\n\n11:00 am\nBreak\n\n\n\n11:15 am\nTeam Report-outs Part 2\nTeams 7-12\n\n\n12:15 pm\nSurvey\n\n\n\n12:30 pm\nWhat’s next\nJulie Lowndes, Openscapes\n\n\n12:55 pm\nClosing\nCatalina Oaida, PO.DAAC\n\n\n\n\nWelcome Day 5\n\nJupyterHub: Log in.\n\nLog into 2i2c. This takes a few minutes so please start this as soon we reconvene each day\n\nGroup Photo!\n\n\n\nWhat’s next\nSlides sharing about upcoming opportunities with the NASA Openscapes project.\n\n\nClosing Day 5\n\nThank you\nContinued hacking on the cloud - next 3 months. You will continue to have access to the 2i2c JupyterHub in AWS for three months following the Cloud Hackathon so you can continue to work and we all learn more about what is involved with migrating data access and science workflows to the Cloud. This cloud compute environment is supported by the NASA Openscapes project.\nUpcoming events - all virtual\n\nAGU Fall Meeting Workshop - December 12 2021. The DAAC mentors will hold a short, half-day, virtual workshop at AGU on Sunday, Dec 12.\nAGU Open science in action session - December 17, 2021. Talks and tutorials by Hackathon Mentors, among other leaders in open science.\nNASA Openscapes Champions Cohort - March-April 2022. Openscapes will lead a NASA Champions Cohort for 7 research teams. This is a professional development and leadership opportunity for scientists that use data from NASA DAACs and are interested in collaborative open data science practices and migrating their workflows to the cloud. Nominate your team by February 1.",
    "crumbs": [
      "Logistics",
      "Schedule"
    ]
  },
  {
    "objectID": "logistics/schedule.html#pre-hackathon-clinic-november-9",
    "href": "logistics/schedule.html#pre-hackathon-clinic-november-9",
    "title": "Schedule",
    "section": "Pre-Hackathon Clinic: November 9",
    "text": "Pre-Hackathon Clinic: November 9\nThis Clinic is optional and we will share a recording that participants can review ahead of time.\n\n\n\nTime, PST (UTC-8)\nEvent\nLeads/Instructors\n\n\n\n\n8:00 am\nWelcome\nJulie Lowndes, Openscapes\n\n\n8:05 am\nJupyterHub, repos, environments\nLuis Lopez, NSIDC\n\n\n9:00 am\nBreak\n\n\n\n9:05 am\nNotebooks, python, syncing\nMakhan Virdi, ASDC\n\n\n10:00 am\nClosing",
    "crumbs": [
      "Logistics",
      "Schedule"
    ]
  },
  {
    "objectID": "logistics/github-workflows.html",
    "href": "logistics/github-workflows.html",
    "title": "GitHub/Jupyter workflows",
    "section": "",
    "text": "We will be live-coding during the tutorials and collaborating during the project hack-time, using GitHub in both cases. Here is how to setup and work for each.\nNote: we’ll be using Git together with GitHub and will talk about them interchangeably.",
    "crumbs": [
      "Logistics",
      "GitHub/Jupyter workflows"
    ]
  },
  {
    "objectID": "logistics/github-workflows.html#first-time-setup",
    "href": "logistics/github-workflows.html#first-time-setup",
    "title": "GitHub/Jupyter workflows",
    "section": "First-Time Setup",
    "text": "First-Time Setup\n\nFork the hackathon repo\nGo to https://github.com/nasa-openscapes/2021-Cloud-Hackathon and fork the repository. This will enable you to you can edit your own copy and live-code with us\n\nNote: The term fork means that you are going to copy the project into your own user space in Github\n\n\n\n\nFork a copy\n\n\n\n\nClone your forked repo into JupyterHub\nOpen your terminal\ngit clone https://github.com/YOUR-USERNAME/2021-Cloud-Hackathon\ndon’t do all the credentials/token — they don’t need push access to follow along with the tutorials. We can help them via Slack/breakouts when/if they need to push with tokens",
    "crumbs": [
      "Logistics",
      "GitHub/Jupyter workflows"
    ]
  },
  {
    "objectID": "logistics/github-workflows.html#daily-setup",
    "href": "logistics/github-workflows.html#daily-setup",
    "title": "GitHub/Jupyter workflows",
    "section": "Daily Setup",
    "text": "Daily Setup\nThe daily setup has 2 steps: get the latest into your forked copy of the repo, then get the latest of your fork into your JupyterHub.\nIf you have any conflicts with the following steps, you will likely need to commit your work, or clear your work if you don’t want to keep anything you’ve done. See below for daily setup troubleshooting as well as Git update, revert, etc.\n\nGitHub: Update your fork\nFrom github.com Fetch and merge: Update your forked repo from main by clicking “Fetch upstream” beneath the big green code button, and then the green “Fetch and merge” button. You may have to refresh the page to see any recent activity.\n\n\n\nJupyterHub: Get your fork’s updates\nGo to the JupyterHub\nGo to the GitHub extension and click the “pull button”\nIf you see\n\nYou’ll need to decide if you want to keep or delete the changes you made yesterday. This will depend on the work you did and how important it was. If you’d like to delete it, please follow delete your local changes below, and then come back above and retype git status and git pull.\nIf you’d like to keep your changes, you’ll need to commit them. You can press “Cancel” and look at the files in the “Changed” category and you can hover over the file to inspect them (open, diff, discard, add) and if you’d like to add them, they will be staged and then you can commit them with a message.\nYou could also do the above in the terminal, making sure you are in the 2021-Cloud-Hackathon directory (double check with pwd and move with cd)\ngit status\ngit pull\n\n\nDaily setup troubleshooting\nNot a git repository - in your terminal if you see the following, you likely need to cd change directory into your GitHub folder.\nfatal: not a git repository (or any parent up to mount point /home)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).",
    "crumbs": [
      "Logistics",
      "GitHub/Jupyter workflows"
    ]
  },
  {
    "objectID": "logistics/github-workflows.html#git-update-revert-etc",
    "href": "logistics/github-workflows.html#git-update-revert-etc",
    "title": "GitHub/Jupyter workflows",
    "section": "Git: update, revert, etc",
    "text": "Git: update, revert, etc\nThese are some useful commands to revert/delete your local changes and update your fork with the most recent information from the main branch.\n\nDelete your local changes\nThere are several ways to delete your local changes if you were playing around and want to reset. Here are a few:\n\nUndo changes you’ve maybe saved or committed, but not pushed\nThis is less time and internet intensive (no new clone/download).\nIf you’ve got changes saved, but not yet staged, committed, or pushed, you’ll delete unstaged changes in the working directory with clean:\nYou’ll need to make sure you’re in the github repository (use pwd to check your present working directory and cd to change directory)\ngit clean -df\ngit checkout -- .\n\n\nBurn it all down\nYou’ll delete the whole repo that you have locally, and then reclone.\nYou’ll need to make sure you’re in the github repository (use pwd to check your present working directory and cd to change directory)\nrm -rf YOUR-REPO\nHere is a whole blog on how to go back in time (walk back changes), with conceptual diagrams, command line code, and screenshots from RStudio. https://ohi-science.org/news/github-going-back-in-time\n\n\n\nUpdate local branch with remote main branch\nIf while you’re working you would like to update your local your-branch with the most recent updates on the main branch on GitHub.com, there are several ways to do this. Here’s one.\ngit checkout your-branch\ngit fetch\ngit merge origin/main\n\n\nUpdate from main",
    "crumbs": [
      "Logistics",
      "GitHub/Jupyter workflows"
    ]
  },
  {
    "objectID": "logistics/github-workflows.html#project-hacktime-setup",
    "href": "logistics/github-workflows.html#project-hacktime-setup",
    "title": "GitHub/Jupyter workflows",
    "section": "Project Hacktime Setup",
    "text": "Project Hacktime Setup\nHere are some suggestions for collaborating with your project groups (and beyond!) using GitHub.\nThis means a combination of creating a place to collaborate (a github repository) and a shared workflow to contribute.\n\nCreate a repository on GitHub.com\nJust one person does this.\nYou can do this in one of your user/organization accounts, or ask someone from the Cloud Hackathon Team to create one for you in the NASA-Openscapes organization.\nHere are instructions for creating a repo on GitHub.com — remember to make it public so that other hackathon folks can see and help!\n\n\nDiscuss edit access vs branches\nThe person who created the repo will manage the permission.\nThe simplest way to collaborate on GitHub is if everyone has permission to edit the repository directly through the main branch. Talk to your team — folks that have experience using branches can do so but others can push changes directly to the main branch.\nHere are instructions for updating github repo permissions.\n\n\nClone repo into 2i2c\nEverybody does this.\nTo to the JupyterHub, go to your Terminal, then:\ngit clone https://github.com/USERNAME/REPOSITORY-NAME.\n\n\nCheck in as you push changes\nWhether you’re using branches or not, check in with each other as you push updates to avoid merge conflicts and have the latest progress.\nHere are instructions for a workflow with branches (optional).\n\n\nUploading files from your local computer to 2i2c\nDo this using the “Upload Files” button in JupyterHub in 2i2c, the UP arrow two over from the big blue + button.",
    "crumbs": [
      "Logistics",
      "GitHub/Jupyter workflows"
    ]
  },
  {
    "objectID": "logistics/github-workflows.html#github-qa",
    "href": "logistics/github-workflows.html#github-qa",
    "title": "GitHub/Jupyter workflows",
    "section": "GitHub Q&A",
    "text": "GitHub Q&A\n\nWhen should I fork+clone instead of just clone?\nFrom Mike Gangl:\nA fork becomes ‘independent’ of the repository you’re forking. So you control if/when you pull in changes. a clone on the other hand, will be linked to the original repository- so if you do “pull” you’ll get the changes from the parent repository as well.\nIf you plan on contributing to a project, a fork is usually the best way to do that, if you plan on simply consuming the project (e.g. run a tutorial) then cloning is fine. A fork can always be appropriate.\nOr, if you plan on updating and making changes that you’d want to preserve, a fork allows you to do that in your own repository, whereas cloning would need you to have permissions to write (push) to the repository.",
    "crumbs": [
      "Logistics",
      "GitHub/Jupyter workflows"
    ]
  },
  {
    "objectID": "logistics/github-workflows.html#jupyter-qa",
    "href": "logistics/github-workflows.html#jupyter-qa",
    "title": "GitHub/Jupyter workflows",
    "section": "Jupyter Q&A",
    "text": "Jupyter Q&A\n\nJupyter notebook question: is there a way to copy multiple cells at once and paste in a new notebook?\n(rather than having to go into each cell individually to copy that snippet of code)\nAnswer: while pressing shift, with the mouse or the arrow keys select the cells you want then you can press ‘C’ or right click and copy the cells and then go to a different notebook and paste them. To make this work you need to focus on the notebook not in a cell (press ESC if so)",
    "crumbs": [
      "Logistics",
      "GitHub/Jupyter workflows"
    ]
  },
  {
    "objectID": "clinic/notebooks.html",
    "href": "clinic/notebooks.html",
    "title": "Notebooks, Python, Git",
    "section": "",
    "text": "In this session, we will provide a brief introduction to:\n\nCommand line (terminal/shell)\nVersion Control (code management using git)\nProgramming in Python (using Jupyter Notebook)\nGeospatial Fundamentals (optional, self-study)\n\nYou will need a working knowledge of git and terminal for this hackathon. We will provide an overview of these topics and also share resources for self-paced learning.",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "Notebooks, Python, Git"
    ]
  },
  {
    "objectID": "clinic/notebooks.html#summary",
    "href": "clinic/notebooks.html#summary",
    "title": "Notebooks, Python, Git",
    "section": "",
    "text": "In this session, we will provide a brief introduction to:\n\nCommand line (terminal/shell)\nVersion Control (code management using git)\nProgramming in Python (using Jupyter Notebook)\nGeospatial Fundamentals (optional, self-study)\n\nYou will need a working knowledge of git and terminal for this hackathon. We will provide an overview of these topics and also share resources for self-paced learning.",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "Notebooks, Python, Git"
    ]
  },
  {
    "objectID": "clinic/notebooks.html#introduction-command-line-terminalshell",
    "href": "clinic/notebooks.html#introduction-command-line-terminalshell",
    "title": "Notebooks, Python, Git",
    "section": "Introduction :: Command Line (Terminal/Shell)",
    "text": "Introduction :: Command Line (Terminal/Shell)\n\nShell Basics\n\nWhat is Terminal or Shell?\nNavigating Files and Directories\nWorking with Files and Directories\n\n\n\nShell: More Details\nDetailed self-paced lesson on shell: Shell Lesson from Software Carpentry",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "Notebooks, Python, Git"
    ]
  },
  {
    "objectID": "clinic/notebooks.html#introduction-version-control-git-and-github",
    "href": "clinic/notebooks.html#introduction-version-control-git-and-github",
    "title": "Notebooks, Python, Git",
    "section": "Introduction :: Version Control (Git and Github)",
    "text": "Introduction :: Version Control (Git and Github)\n\nWhat is version control, git, github, and how to set it up?\nVersion control is managing and tracking changes to your documents (program source code, images, websites, data files, etc.). git is a popular tool used for version control of software code. github.com is popular platform that provides remote server hosting for git repositories. A repository is a collection of various files that you are tracking for changes and versions (think of it as a directory with files that are being tracked for changes, using git for taking snapshots of versions as you are developing).\nThis section is a step-by-step guide to set up git on your 2i2c JupyterHub instance (referred to as 2i2c JupyterHub in these instruction). We will also configure git to use your github.com account for managing your repositories hosted on github.com. There are 5 main steps with substeps, includes instruction for addressing github’s new approach for token authentication.\n\n\nStep 1: Create a github account\nTo complete the setup, you will need an account on github.com. If you don’t have an account, please visit github.com, create an account (free) and come back to this guide for setting up git.\n\n\nStep 2: Fork a repository\nA fork is a copy of a repository from another github account (for example NASA-Openscapes account) to your github account (for example, my account virdi) that then you have permission to edit. To help you finish this setup correctly, we have created a demo repository on Openscapes github account named check_github_setup. You can fork this repository into your github account following these steps:\n\nLog in to your github.com account\nGo to the demo repository at NASA-Openscapes github\n\n\n\nDemo repository on NASA-Openscapes github\n\n\nClick on the fork icon in the top right corner, as shown in the image below and click your user name if prompted to do so\n\n\n\n\nStep 3: Clone the repository that you just forked\nNow you have a fork of the demo repository in your github account that we can clone it in your 2i2c instance. In the code below, commands beginning with git is a git command for version control and synching; commands that don’t start with git are bash/linux/command line commands.\n\nStart your 2i2c JupyterHub and open a terminal\nFile &gt;&gt; New &gt;&gt; Terminal\nMake sure you are in your home directory by usingpwd command and verifying the output as below\n/home/jovyan\n\nConfigure git with your name and email address.\ngit config --global user.name \"Makhan Virdi\"\ngit config --global user.email \"Makhan.Virdi@gmail.com\"\nNote: This name and email could be different from your github.com credentials. Remember git is a program that keeps track of your changes locally (on 2i2c JupyterHub or your own computer) and github.com is a platform to host your repositories. However, since your changes are tracked by git, the email/name used in git configuration will show up next to your contributions on github.com when you push your repository to github.com (git push is discussed in a later step).\nConfigure git to store your github credentials to avoid having to enter your github username and token each time you push changes to your repository(in Step 5, we will describe how to use github token instead of a password)\ngit config --global credential.helper store\nCopy link for the demo repository from your github account. Click the green “Code” button and copy the link as shown.\n\nClone the repository using git clone command in the terminal\nTo clone a repository from github, copy the link for the repository (previous step) and use git clone:\ngit clone https://github.com/YOUR-GITHUB-USERNAME/check_github_setup\nNote: Replace YOUR-GITHUB-USERNAME here with your github.com username. For example, it is virdi for my github.com account as seen in this image.\n\nUse ls (list files) to verify the existence of the repository that you just cloned\n\nChange directory to the cloned repository using cd check_github_setup and check the current directory using pwd command (present working directory)\n\nCheck status of your git repository to confirm git set up using git status\n\nYou are all set with using git on your 2i2c JupyterHub! But the collaborative power of git through github needs some additional setup.\nIn the next step, we will create a new file in this repository, track changes to this file, and link it with your github.com account.\n\n\n\nStep 4. Creating new file and tracking changes\n\nIn the left panel on your 2i2c JupyterHub, click on the “directory” icon and then double click on “check_github_setup” directory.\n\n\nOnce you are in the check_github_setup directory, create a new file using the text editor in your 2i2c JupyterHub (File &gt;&gt; New &gt;&gt; Text File).\n\nName the file lastname.txt. For example, virdi.txt for me (use your last name). Add some content to this file (for example, I added this to my virdi.txt file: my last name is virdi).\n\nNow you should have a new file (lastname.txt) in the git repository directory check_github_setup\nCheck if git can see that you have added a new file using git status. Git reports that you have a new file that is not tracked by git yet, and suggests adding that file to the git tracking system.\n\nAs seen in this image, git suggests adding that file so it can be tracked for changes. You can add file to git for tracking changes using git add. Then, you can commit changes to this file’s content using git commit as shown in the image.\ngit add virdi.txt\ngit status\ngit commit -m \"adding a new file\"\ngit status\n\nAs seen in the image above, git is suggesting to push the change that you just committed to the remote server at github.com (so that your collaborators can also see what changes you made).\nNote: DO NOT execute push yet. Before we push to github.com, let’s configure git further and store our github.com credentials to avoid entering the credentials every time we invoke git push. For doing so, we need to create a token on github.com to be used in place of your github.com password.\n\n\n\nStep 5. Create access token on github.com\n\nGo to your github account and create a new “personal access token”: https://github.com/settings/tokens/new\n\n\n\nGenerate Personal Access Token on github.com\n\n\nEnter a description in “Note” field as seen above, select “repo” checkbox, and scroll to the bottom and click the green button “Generate Token”. Once generated, copy the token (or save it in a text file for reference).\nIMPORTANT: You will see this token only once, so be sure to copy this. If you do not copy your token at this stage, you will need to generate a new token.\n\nTo push (transfer) your changes to github, use git push in terminal. It requires you to enter your github credentials. You will be prompted to enter your github username and “password”. When prompted for your “password”, DO NOT use your github password, use the github token that was copied in the previous step.\ngit push\n\nNote: When you paste your token in the terminal window, windows users will press Ctrl+V and mac os users will press Cmd+V. If it does not work, try generating another token and use the copy icon next to the token to copy the token. Then, paste using your computer’s keyboard shortcut for paste.\nNow your password is stored in ~/.git-credentials and you will not be prompted again unless the Github token expires. You can check the presence of this git-credentials file using Terminal. Here the ~ character represents your home directory (/home/jovyan/).\nls -la ~\nThe output looks like this:\ndrwxr-xr-x 13 jovyan jovyan 6144 Oct 22 17:35 .\ndrwxr-xr-x  1 root   root   4096 Oct  4 16:21 ..\n-rw-------  1 jovyan jovyan 1754 Oct 29 18:30 .bash_history\ndrwxr-xr-x  4 jovyan jovyan 6144 Oct 29 16:38 .config\n-rw-------  1 jovyan jovyan   66 Oct 22 17:35 .git-credentials\n-rw-r--r--  1 jovyan jovyan   84 Oct 22 17:14 .gitconfig\ndrwxr-xr-x 10 jovyan jovyan 6144 Oct 21 16:19 2021-Cloud-Hackathon\nYou can also verify your git configuration\n(notebook) jovyan@jupyter-virdi:~$ git config -l\nThe output should have credential.helper = store:\nuser.email        = Makhan.Virdi@gmail.com\nuser.name         = Makhan Virdi\ncredential.helper = store\n\nNow we are all set to collaborate with github on the JupyterHub during the Cloud Hackathon!\n\n\nSummary: Git Commands\n\nCommonly used git commands (modified from source)\n\n\nGit Command\nDescription\n\n\n\n\ngit status\nShows the current state of the repository: the current working branch, files in the staging area, etc.\n\n\ngit add\nAdds a new, previously untracked file to version control and marks already tracked files to be committed with the next commit\n\n\ngit commit\nSaves the current state of the repository and creates an entry in the log\n\n\ngit log\nShows the history for the repository\n\n\ngit diff\nShows content differences between commits, branches, individual files and more\n\n\ngit clone\nCopies a repository to your local environment, including all the history\n\n\ngit pull\nGets the latest changes of a previously cloned repository\n\n\ngit push\nPushes your local changes to the remote repository, sharing them with others\n\n\n\n\n\nGit: More Details\nLesson: For a more detailed self-paced lesson on git, visit Git Lesson from Software Carpentry\nCheatsheet: Frequently used git commands\nDangit, Git!?!: If you are stuck after a git mishap, there are ready-made solutions to common problems at Dangit, Git!?!\n\n\nCloning our repository using the git Jupyter lab extension.\nIf we’re already familiar with git commands and feel more confortable using a GUI our Jupyterhub deployment comes with a git extension. This plugin allows us to operate with git using a simple user interface.\nFor example we can clone our repository using the extension.\n\n\n\ngit extension",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "Notebooks, Python, Git"
    ]
  },
  {
    "objectID": "clinic/notebooks.html#introduction-programming-in-python",
    "href": "clinic/notebooks.html#introduction-programming-in-python",
    "title": "Notebooks, Python, Git",
    "section": "Introduction :: Programming in Python",
    "text": "Introduction :: Programming in Python\nSwitch to Jupyter Notebook for an introduction to programming in Python\n\nVariables (and mathematical operations)\nData Structures (list, tuple, dict)\nFlow Control using loops (for, while)\nConditionals (if, else, elif)\nFunctions\nErrors and Exceptions (understanding and handling errors)\nUsing modules (libraries, packages)\n\npandas: high-performance, easy-to-use data structures and data analysis tools\nrioxarray: based on the rasterio package for working with rasters and xarray\n\n\n\nPython Learning Resources\nSelf-paced lesson on Programming with Python from Software Carpentry",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "Notebooks, Python, Git"
    ]
  },
  {
    "objectID": "clinic/notebooks.html#introduction-geospatial-fundamentals-optional",
    "href": "clinic/notebooks.html#introduction-geospatial-fundamentals-optional",
    "title": "Notebooks, Python, Git",
    "section": "Introduction :: Geospatial Fundamentals (Optional)",
    "text": "Introduction :: Geospatial Fundamentals (Optional)\nDetailed self-paced lesson on Fundamentals of Geospatial Raster and Vector Data with Python from Data Carpentry\nThe end!",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "Notebooks, Python, Git"
    ]
  },
  {
    "objectID": "clinic/earthdata.html",
    "href": "clinic/earthdata.html",
    "title": "Earthdata Login",
    "section": "",
    "text": "The following was borrowed from the excellent SnowEx Hackathon 2021\n\n\nNASA data are stored at one of several Distributed Active Archive Centers (DAACs). If you’re interested in available data for a given area and time of interest, the Earthdata Search portal provides a convenient web interface.\n\n\n\nEach participant will need a login. We will be teaching you ways to programmatically access NASA data from within your Python scripts. You will need to enter your Earthdata username and password in order for this to work.\n\n\n\nIf you do not already have an Earthdata login, then navigate to the Earthdata Login page, a username and password, and then record this somewhere for use during the tutorials:\n\n\n\nearthdata-login\n\n\n\n\n\nIf you use web interfaces to retrieve nasa data such as Earthdata Search you are prompted to login. We will be using software to retrieve data from NASA Servers during the hackweek, so you must store your credentials on the JupyterHub as explained in this documentation. Run the following commands on the JupyterHub in a terminal replacing your Earthdata login username and password:\necho \"machine urs.earthdata.nasa.gov login EARTHDATA_LOGIN password EARTHDATA_PASSWORD\" &gt; ~/.netrc\nchmod 0600 .netrc",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "Earthdata Login"
    ]
  },
  {
    "objectID": "clinic/earthdata.html#overview",
    "href": "clinic/earthdata.html#overview",
    "title": "Earthdata Login",
    "section": "",
    "text": "NASA data are stored at one of several Distributed Active Archive Centers (DAACs). If you’re interested in available data for a given area and time of interest, the Earthdata Search portal provides a convenient web interface.",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "Earthdata Login"
    ]
  },
  {
    "objectID": "clinic/earthdata.html#why-do-i-need-an-earthdata-login",
    "href": "clinic/earthdata.html#why-do-i-need-an-earthdata-login",
    "title": "Earthdata Login",
    "section": "",
    "text": "Each participant will need a login. We will be teaching you ways to programmatically access NASA data from within your Python scripts. You will need to enter your Earthdata username and password in order for this to work.",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "Earthdata Login"
    ]
  },
  {
    "objectID": "clinic/earthdata.html#getting-an-earthdata-login",
    "href": "clinic/earthdata.html#getting-an-earthdata-login",
    "title": "Earthdata Login",
    "section": "",
    "text": "If you do not already have an Earthdata login, then navigate to the Earthdata Login page, a username and password, and then record this somewhere for use during the tutorials:\n\n\n\nearthdata-login",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "Earthdata Login"
    ]
  },
  {
    "objectID": "clinic/earthdata.html#configure-programmatic-access-to-nasa-servers",
    "href": "clinic/earthdata.html#configure-programmatic-access-to-nasa-servers",
    "title": "Earthdata Login",
    "section": "",
    "text": "If you use web interfaces to retrieve nasa data such as Earthdata Search you are prompted to login. We will be using software to retrieve data from NASA Servers during the hackweek, so you must store your credentials on the JupyterHub as explained in this documentation. Run the following commands on the JupyterHub in a terminal replacing your Earthdata login username and password:\necho \"machine urs.earthdata.nasa.gov login EARTHDATA_LOGIN password EARTHDATA_PASSWORD\" &gt; ~/.netrc\nchmod 0600 .netrc",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "Earthdata Login"
    ]
  },
  {
    "objectID": "clinic/index.html",
    "href": "clinic/index.html",
    "title": "Overview and Agenda",
    "section": "",
    "text": "Before the Hackathon, we are hosting a 2-hour Clinic where you can build skills and practice the tooling and workflow that we will use during the Hackathon.\nThe Clinic will cover the following as a workflow:",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "Overview and Agenda"
    ]
  },
  {
    "objectID": "clinic/index.html#welcome",
    "href": "clinic/index.html#welcome",
    "title": "Overview and Agenda",
    "section": "Welcome!",
    "text": "Welcome!\nThanks for being here\n\nWho instructors & helpers are, how to ask for help.\nCode of Conduct reminder: Be respectful and value each other’s ideas, styles and viewpoints.\nIf you have issues, please direct them to: Julie - lowndes @ nceas.ucsb.edu.\nLive transcripts are available.\nWe are recording this session.\n\n\n\n\n\n\n\nText to paste into Zoom Chat\n\n\n\n\n\nWelcome to the Cloud Hackathon Clinic!\nPlease go to 2i2c Openscapes Hub - log in with your GitHub Account, and select “Small”\nClinic materials that we’ll cover today are here: https://nasa-openscapes.github.io/2021-Cloud-Hackathon/clinic/",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "Overview and Agenda"
    ]
  },
  {
    "objectID": "clinic/index.html#agenda",
    "href": "clinic/index.html#agenda",
    "title": "Overview and Agenda",
    "section": "Agenda",
    "text": "Agenda\nThe Clinic will occur in 2 halves, with a 5 minute break in-between:\n\n\n\nTime, PST (UTC-8)\nEvent\nLeads/Instructors\n\n\n\n\n8:00 am\nWelcome\nJulie Lowndes, Openscapes\n\n\n8:05 am\nJupyterHub, repos, environments\nLuis Lopez, NSIDC\n\n\n9:00 am\nBreak\n\n\n\n9:05 am\nNotebooks, python, syncing\nMakhan Virdi, ASDC\n\n\n10:00 am\nClosing",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "Overview and Agenda"
    ]
  },
  {
    "objectID": "clinic/index.html#before-the-clinic",
    "href": "clinic/index.html#before-the-clinic",
    "title": "Overview and Agenda",
    "section": "Before the Clinic",
    "text": "Before the Clinic\nPlease follow the set up prerequisites before the Clinic.",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "Overview and Agenda"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "2021 Cloud Hackathon",
    "section": "",
    "text": "Welcome to Cloud Hackathon: Transitioning Earthdata Workflows to the Cloud, co-hosted by the NASA EOSDIS Physical Oceanography Distributed Active Archive Center (PO.DAAC), National Snow and Ice Data Center DAAC (NSIDC DAAC), Land Processes Distributed Active Archive Center (LP.DAAC), with support provided by ASDC DAAC, GES DISC, IMPACT, and NASA Openscapes.\nThe Cloud Hackathon will take place virtually from November 15-19, 2021. The event is free to attend, but an application is required. The application period (September 21 - October 12, 2021) is now closed. Those who applied will be informed of the outcome on or around October 20th, 2021.\nPost-workshop resources. Learn about the 2021 Projects, and the event summary: https://earthdata.nasa.gov/learn/articles/2021-cloud-hackathon.",
    "crumbs": [
      "Welcome",
      "2021 Cloud Hackathon"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "2021 Cloud Hackathon",
    "section": "",
    "text": "Welcome to Cloud Hackathon: Transitioning Earthdata Workflows to the Cloud, co-hosted by the NASA EOSDIS Physical Oceanography Distributed Active Archive Center (PO.DAAC), National Snow and Ice Data Center DAAC (NSIDC DAAC), Land Processes Distributed Active Archive Center (LP.DAAC), with support provided by ASDC DAAC, GES DISC, IMPACT, and NASA Openscapes.\nThe Cloud Hackathon will take place virtually from November 15-19, 2021. The event is free to attend, but an application is required. The application period (September 21 - October 12, 2021) is now closed. Those who applied will be informed of the outcome on or around October 20th, 2021.\nPost-workshop resources. Learn about the 2021 Projects, and the event summary: https://earthdata.nasa.gov/learn/articles/2021-cloud-hackathon.",
    "crumbs": [
      "Welcome",
      "2021 Cloud Hackathon"
    ]
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "2021 Cloud Hackathon",
    "section": "About",
    "text": "About\nThe Cloud Hackathon: Transitioning Earthdata Workflows to the Cloud is a virtual 5-day (4 hours per day) collaborative open science learning experience aimed at exploring, creating, and promoting effective cloud-based science and applications workflows using NASA Earthdata Cloud data, tools, and services (among others), in support of Earth science data processing and analysis in the era of big data. Its goals are to:\n\nIntroduce Earth science data users to NASA Earthdata cloud-based data products, tools and services in order to increase awareness and support transition to cloud-based science and applications workflows.\nEnable science and applications workflows in the cloud that leverage NASA Earth Observations and capabilities (services) from within the NASA Earthdata Cloud, hosted in Amazon Web Services (AWS) cloud, thus increasing NASA Earthdata data utility and meaningfulness for science and applications use cases.\nFoster community engagement utilizing Earthdata cloud tools and services in support of open science and open data.\n\nOutcome: Participants prototype their science and applications workflows (via hackathon projects) that leverage Earthdata Cloud data and services (focusing on, but not limited to, oceanography, cryosphere, hydrology and land data), which supports them in their transition to cloud-based or hybrid workflows for data processing and analysis.\nThis is an opportunity for researchers that might not yet have had the opportunity to work in the Cloud to explore, learn and prototype workflows with NASA Earthdata in the Cloud, but more intermediate or advanced cloud users interested in further exploring cloud workflows with Earthdata Cloud data and service are also welcome.",
    "crumbs": [
      "Welcome",
      "2021 Cloud Hackathon"
    ]
  },
  {
    "objectID": "index.html#application",
    "href": "index.html#application",
    "title": "2021 Cloud Hackathon",
    "section": "Application",
    "text": "Application\n\nInformation for applicants\nThe Cloud Hackathon will be a virtual event held November 15-19, 2021, where participants will explore the intersection of Earth science data, cloud computing, and big data analysis through demonstration tutorials and hands-on “hacking” projects. To best benefit from the event, we recommend some familiarity or experience with:\n\nNASA Earthdata data (focusing on oceanography, cryosphere, hydrology, cryosphere and land data, including interdisciplinary applications); and\nProgramming skills using Python. We plan to accept participants with broad skill levels and backgrounds in programming. However, to best benefit from and contribute to the program, participants are expected to have some experience with Python programming.\n\nNo cloud computing experience is required, but we encourage both beginner and more experienced participants with AWS cloud to apply.\nIf selected, participants will have the option to attend a Carpentries-style github, python, shell scripting clinic ahead of the Cloud Hachathon.\n\n\nApplication Form\nIn the application form, we encourage you to think about and provide a science use case that you would like to prototype in the cloud. At the beginning of the hackathon, participants will be able to pitch their use case to support the formation of “hack” projects - by which we mean collaboratively experiment working in with NASA Earthdata data and capabilities in the Cloud. During the hackathon, participants will get into teams of their choosing, around a common use case to “hack” in the cloud. The use cases provided in the application form will also help the organizers best prepare materials tailored to those use cases.\nThe application period has now closed. Thank you for your interest.",
    "crumbs": [
      "Welcome",
      "2021 Cloud Hackathon"
    ]
  },
  {
    "objectID": "index.html#what-to-expect",
    "href": "index.html#what-to-expect",
    "title": "2021 Cloud Hackathon",
    "section": "What to expect",
    "text": "What to expect\n\nDuring the Cloud Hackathon, the selected participants will have access to cloud environments in AWS through a JupyterHub interface, provided through 2i2c.\nParticipants will be guided on how to log into the cloud environment, import needed data recipes and resources, and will have the opportunity to explore and develop science and applications workflows in a cloud environment (hosted in AWS) using example tutorials as building blocks.\nThe Cloud Hackathon is an open science event: all tutorials and examples are developed openly and will be publicly available during and following hackathon. Participants will strengthen their practice of open science, using open source code and “hacking” their projects openly to enable further discovery and contributions by the broader open community following the hackathon.\nThroughout the hackathon, participants will learn about NASA’s Earthdata move to the cloud and Earthdata APIs for data discovery, access, and transformations to enable faster, more efficient time to science.\n\nIn the two to three weeks leading up to the hackathon, participants are encouraged to review background resources that will facilitate a more effective hackathon experience. These resources will be shared here leading up to the Hackathon dates, and will be accessible to all data users, whether they attend the hackathon or not.\nThe following datasets are currently available from the NASA Earthdata Cloud. Participants can choose to prototype a cloud-based science workflow using a combination of these datasets, as well as other non-Earthdata Cloud data. If your preferred dataset is not yet available in the Earthdata Cloud, consider using a current cloud-based dataset as proxy to explore prototyping.\n\nhttps://search.earthdata.nasa.gov/search?ff=Available%20from%20AWS%20Cloud\n\nExample use cases to explore in the cloud (note these are for inspiration only, you are not limited to these workflows):\n\nUse the advanced wildcard search capabilities in Earthdata Search Client/Common Metadata Repository (CMR) to precisely search/select all cloud-archived Sentinel-6A granules\n\nfrom a specific cycle (i.e. a sequence orbits that together provide global spatial coverage), and/or\nfrom a specific pass(es) over multiple cycles (i.e. selected orbits over a series of cycles that together provide a time series coverage).\nThen, prepare the data for gridding or for local analysis at space/time scales which are appropriate for the target analysis (and limited by default given the length of S6A data record…)\n\nTime series analysis across multi-mission measurements spanning data housed both within and outside of NASA Earthdata Cloud, to develop a workflow that can accommodate different data locations, as data continue to migrate to the Cloud:\n\nProgrammatically search for a data variable (e.g. altimetry measurements) at a single point or area of interest across multiple datasets and identify whether the data are available in the Cloud\nAcquire the data based on archived location and combine in order to produce a homogenous time series\n\nExplore/leverage cloud-optimized formats (COFs) such as Zarr to compute global or regional climatology and anomalies for a large-volume dataset (e.g. 1-km MUR SST) without having to download data (in-cloud analysis).\nSubset Level 2 swath dataset of interest spatially and for specific variable and do some exploratory analysis and visualization from within the cloud.\nUse NASA’s CMR-STAC API to search and discover Harmonized Landsat Sentinel-2 (HLS) cloud assets based on cloud data products, area of interest, and date range query parameters.\nHarmonized Landsat Sentinel-2 (HLS) for land monitoring: access, explore, and visualize time series surface reflectance data in the cloud.\n\nThis event is motivated by the dawn of the era of Big Data. NASA’s Earth Observing System Data and Information System (EOSDIS) is in the process of moving EOSDIS data to the cloud, driven by a rapid rate of data ingest into the EOSDIS archive. NASA remote sensing data from both upcoming (e.g. SWOT) and existing (e.g. Terra, Aqua, ICESat-2) missions will be available in the Earthdata Cloud platform in the coming years. The paradigm shift from on-premise (local) to cloud-based data distribution, and that from “download and analyze” to “analysis in place” present opportunities and challenges. Guiding users through this transition is of the utmost importance.",
    "crumbs": [
      "Welcome",
      "2021 Cloud Hackathon"
    ]
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "2021 Cloud Hackathon",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nThe 2021 Cloud Hackathon is a safe learning space and all participants are required to abide by our Code of Conduct.",
    "crumbs": [
      "Welcome",
      "2021 Cloud Hackathon"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "2021 Cloud Hackathon",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nCloud Hackathon: Transitioning Earthdata Workflows to the Cloud is co-hosted by NASA’s PO.DAAC, NSIDC DAAC, LP.DAAC, with support from ASDC DAAC, GES DISC and the NASA Openscapes Project, and cloud computing infrastructure by 2i2c.   \nWe thank all of the additional NASA staff that have have joined as helpers.\nThank you to the open science community that has created software, teaching resources, and workflows that we have been able to build heavily from! These include:\n\neScience Institute, University of Washington:\n\nhttps://uwhackweek.github.io/hackweeks-as-a-service/intro.html\nhttps://snowex-hackweek.github.io/website/intro.html\nhttps://icesat-2hackweek.github.io/learning-resources/\n\n\nThis hackathon book is made with quarto. See the earthdata-cloud-cookbook to learn more about how we work and contributing.",
    "crumbs": [
      "Welcome",
      "2021 Cloud Hackathon"
    ]
  },
  {
    "objectID": "clinic/jupyterhub.html",
    "href": "clinic/jupyterhub.html",
    "title": "NASA Openscapes Cloud Environment",
    "section": "",
    "text": "Summary of what we’ll cover:",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "NASA Openscapes Cloud Environment"
    ]
  },
  {
    "objectID": "clinic/jupyterhub.html#why-are-we-using-a-cloud-environment",
    "href": "clinic/jupyterhub.html#why-are-we-using-a-cloud-environment",
    "title": "NASA Openscapes Cloud Environment",
    "section": "Why are we using a cloud environment?",
    "text": "Why are we using a cloud environment?\n“Anyone working with large-scale Earth System data today faces the same general problems:\n\nThe data we want to work with are huge (typical analyses involve several TB at least)\nThe data we need are produced and distributed by many different organizations (NASA, NOAA, ESGF, Copernicus, etc.)\nWe want to apply a wide range of different analysis methodologies to the data, from simple statistics to signal processing to machine learning.\n\nThe community is waking up to the idea that we can’t simply expect scientists to download all this data to their personal computers for processing.”\nRyan Abernathey, Pangeo Project.\n\n\n\nDownload-based workflow. From Abernathey, Ryan (2020): Data Access Modes in Science",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "NASA Openscapes Cloud Environment"
    ]
  },
  {
    "objectID": "clinic/jupyterhub.html#openscapes-hub-and-cloud-infrastructure",
    "href": "clinic/jupyterhub.html#openscapes-hub-and-cloud-infrastructure",
    "title": "NASA Openscapes Cloud Environment",
    "section": "Openscapes Hub and Cloud Infrastructure",
    "text": "Openscapes Hub and Cloud Infrastructure\nThere is no cloud, it’s someone else’s computer\nGo to Openscapes Jupyter Hub. You will be asked to log in with your GitHub Account\n\n\n\nOpenscapes JupyterHub Login\n\n\nOnce we are logged with our Github account we need to select our server type. There are different hardware configurations for each profile, for the duration of the Hackweek we’ll use small instances, the option at the top.\n\n\n\nMachine Profiles\n\n\nAfter we select our server type and click on start, Jupyterhub will allocate our instance using Amazon Web Services (AWS). This may take several minutes. While we wait, we’ll get set up with GitHub and a brief overview.\n\n\n\nJupyterhub Spawning",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "NASA Openscapes Cloud Environment"
    ]
  },
  {
    "objectID": "clinic/jupyterhub.html#jupyter-ecosystem",
    "href": "clinic/jupyterhub.html#jupyter-ecosystem",
    "title": "NASA Openscapes Cloud Environment",
    "section": "Jupyter Ecosystem",
    "text": "Jupyter Ecosystem\n\nSource: Project Pythia",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "NASA Openscapes Cloud Environment"
    ]
  },
  {
    "objectID": "clinic/jupyterhub.html#pythonconda-environments",
    "href": "clinic/jupyterhub.html#pythonconda-environments",
    "title": "NASA Openscapes Cloud Environment",
    "section": "Python/Conda environments",
    "text": "Python/Conda environments\nname: nsidc\nchannels:\n  - conda-forge\ndependencies:\n  - ipykernel\n  - awscli~=1.21.4\n  - requests\n  - pip\n\nHow do I get my code in and out of the Openscapes hub?\nWhen you start your own server you will have access to your own virtual drive space. No other users will be able to see or access your data files. You can easily upload files to your virtual drive space and save files from the hub back to another location, such as GitHub or your own local laptop drive.\nHere we’ll show you how to pull (copy) some files from GitHub into your virtual drive space using git. This will be a common task during the hackweek: at the start of each tutorial we’ll ask you to “fork” (create your own copy of in your GitHub account) and “clone” (make a copy of in a computing environment, such as your local computer or Openscapes instance) the GitHub repository corresponding to the specific tutorial being taught into your Openscapes drive space.\n\n\n\nterminal-button\n\n\nThis will open a new terminal tab in your JupyterLab interface:\n\n\n\nterminal-tab\n\n\nNow you can issue any Linux commands to manage your local file system.\nYou may also upload files from your local system using the upload button (up-pointing arrow) on the top left of the JupyterHub navigation panel. Similarly, you may download files to your local system by right-clicking the file and selecting download (down-pointing arrow).\nSimple, example GitHub/git/local-workspace workflows for getting a tutorial started in your Openscapes instance and working on a group project are provided. The getting started on a tutorial workflow briefly reviews much of the information in this preliminary exercise along with steps for making and saving notes or other changes as you work through the tutorial and keeping it updated with the original, master copy. The basic git workflow for a project serves as a reminder of the git workflow for working on a group project while minimizing code conflicts that could result from multiple people making changes to the same files simultaneously.",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "NASA Openscapes Cloud Environment"
    ]
  },
  {
    "objectID": "clinic/jupyterhub.html#how-do-i-end-my-openscapes-session",
    "href": "clinic/jupyterhub.html#how-do-i-end-my-openscapes-session",
    "title": "NASA Openscapes Cloud Environment",
    "section": "How do I end my Openscapes session?",
    "text": "How do I end my Openscapes session?\nWhen you are finished working for the day it is important to explicitly log out of your Openscapes session. The reason for this is it will save money and is a good habit to be in. When you keep a session active it uses up AWS resources and keeps a series of virtual machines deployed.\nStopping the server happens automatically when you log out, so navigate to “File -&gt; Log Out” and click “Log Out”!\n\n\n\nhub-control-panel-button\n\n\n\nWill I lose all of my work?\nLogging out will NOT cause any of your work to be lost or deleted. It simply shuts down some resources. It would be equivalent to turning off your desktop computer at the end of the day.",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "NASA Openscapes Cloud Environment"
    ]
  },
  {
    "objectID": "clinic/jupyterhub.html#references",
    "href": "clinic/jupyterhub.html#references",
    "title": "NASA Openscapes Cloud Environment",
    "section": "References",
    "text": "References\n\nProject Pythia\nWhy Jupyter is data scientists’ computational notebook of choice\nClosed Platforms vs. Open Architectures for Cloud-Native Earth System Analytics\nIntroduction to Geospatial Concepts\n2i2c user storage\nSnowEX Hackweek",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "NASA Openscapes Cloud Environment"
    ]
  },
  {
    "objectID": "clinic/jupyterhub.html#faq",
    "href": "clinic/jupyterhub.html#faq",
    "title": "NASA Openscapes Cloud Environment",
    "section": "FAQ",
    "text": "FAQ\nfrom participants during our first Clinic\nI have an empty ‘shared’ folder. That’s expected. There shouldn’t be anything in the ‘shared/’ folder\nAfter the 3 months are up, what do we do with our work on the server? You’ll have them since you can back everything up with GitHub. We can follow up with more details of what happens on the 2i2c side\nCan we use Matlab with JupyterHub? You can also use Octave kernel as a Matlab replacement. It is open source and free. If you want to integrate Matlab, there is a project to do so jupyter-matlab-proxy\nWhy do we have the same home directory as /home/jovyan? /home/jovyan is the default home directory for ‘jupyter’ based images/dockers. It is the historic home directory for Jupyter deployments.\n/home/jovyan is the default home directory for jupyter-based deployments\nCan other users see the .git-credentials file in my /home/jovyan folder? No, other users can not see your creds\nHow to exit 2i2c’s terminal text editor? esc to get to the command, and then :w to save, :q to quit.",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "NASA Openscapes Cloud Environment"
    ]
  },
  {
    "objectID": "clinic/github.html",
    "href": "clinic/github.html",
    "title": "GitHub",
    "section": "",
    "text": "The following was borrowed from the excellent SnowEx Hackathon 2021\n\n\nGitHub is a hosting service for Git repositories, enabling us to share code across teams in a web environment.\n\n\n\nThere are three reasons you are required to have a GitHub account for the hackweek:\n\nYour GitHub accounts will give you access to the hackweek cloud computing resources\nAll hackweek tutorials will be shared on GitHub\nAll project teams will use GitHub to collaborate and work together on their code\n\n\n\n\nGo to GitHub.\n\n\n\ngithub-signup\n\n\nNext, enter your email address and click on the green ‘Sing up for GitHub’ button. You will need to answer a few required questions in the following dialogs. Be sure to save your password somewhere safe because you will need it later! The steps for doing this are also well documented on this GitHub help page.\n\n\n\nrepos-tab\n\n\nEach repository is a container for a specific subset of material for this event. For example, there is a repository for the public-facing website you used to register for this event {{website_url}}. We’ll also create new repositories for each project.",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "GitHub"
    ]
  },
  {
    "objectID": "clinic/github.html#what-is-github",
    "href": "clinic/github.html#what-is-github",
    "title": "GitHub",
    "section": "",
    "text": "GitHub is a hosting service for Git repositories, enabling us to share code across teams in a web environment.",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "GitHub"
    ]
  },
  {
    "objectID": "clinic/github.html#why-do-i-need-a-github-account",
    "href": "clinic/github.html#why-do-i-need-a-github-account",
    "title": "GitHub",
    "section": "",
    "text": "There are three reasons you are required to have a GitHub account for the hackweek:\n\nYour GitHub accounts will give you access to the hackweek cloud computing resources\nAll hackweek tutorials will be shared on GitHub\nAll project teams will use GitHub to collaborate and work together on their code",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "GitHub"
    ]
  },
  {
    "objectID": "clinic/github.html#creating-a-github-account",
    "href": "clinic/github.html#creating-a-github-account",
    "title": "GitHub",
    "section": "",
    "text": "Go to GitHub.\n\n\n\ngithub-signup\n\n\nNext, enter your email address and click on the green ‘Sing up for GitHub’ button. You will need to answer a few required questions in the following dialogs. Be sure to save your password somewhere safe because you will need it later! The steps for doing this are also well documented on this GitHub help page.\n\n\n\nrepos-tab\n\n\nEach repository is a container for a specific subset of material for this event. For example, there is a repository for the public-facing website you used to register for this event {{website_url}}. We’ll also create new repositories for each project.",
    "crumbs": [
      "Pre-Hackathon-Clinic",
      "GitHub"
    ]
  },
  {
    "objectID": "logistics/index.html",
    "href": "logistics/index.html",
    "title": "Logistics overview",
    "section": "",
    "text": "Before the hackathon, please complete the prerequisites and be prepared for the daily setup and be familiar with the ways of getting help.",
    "crumbs": [
      "Logistics",
      "Logistics overview"
    ]
  },
  {
    "objectID": "logistics/index.html#for-hackathon-participants",
    "href": "logistics/index.html#for-hackathon-participants",
    "title": "Logistics overview",
    "section": "",
    "text": "Before the hackathon, please complete the prerequisites and be prepared for the daily setup and be familiar with the ways of getting help.",
    "crumbs": [
      "Logistics",
      "Logistics overview"
    ]
  },
  {
    "objectID": "logistics/index.html#for-hackathon-helpers---a-mentors-guide",
    "href": "logistics/index.html#for-hackathon-helpers---a-mentors-guide",
    "title": "Logistics overview",
    "section": "For Hackathon Helpers - A Mentor’s Guide",
    "text": "For Hackathon Helpers - A Mentor’s Guide\nMentor = trainer = helper = any DAAC or Openscapes staff\n\nSlack\nChannels in Openscapes workspace\nUpdate your Slack name temporarily to include “helper”, e.g. Catalina Oaida (Helper)\n\nMentors’/Helper channel - #nasa-daac-mentors1\n\nOur back channel to coordinate amongst ourselves\n\nGeneral channel - #2021-nasacloudhack-general\n\nAnnouncements, general questions and communications\n\nPitchfest channel - #2021-nasacloudhack-projects\n\nParticipants discuss project ideas\nUse slack direct message (DM) for team/project work\n\nHelp channel - #2021-nasacloudhack-help\n\nTroubleshooting, share screenshots, etc\nWhen you see a question you will reply to, please add the “eyes” emoji below so other helpers know that you are looking into it. Then please reply in-thread in Slack to help. Can tag additional helpers if you need further support.\n\n\n\n\nGithub\nHackathon repo: https://github.com/NASA-Openscapes/2021-Cloud-Hackathon; any updates to the main branch will be updated in this book (via GitHub Action).\n\ntutorials folder\ntutorials-templates folder for live coding - same as tutorials but the code is removed and only markdown remains\nParticipants will create their own repos, we can link to their projects in the cloud hackathon repo to capture the hackathon projects artifacts\nWorking with the CH repo\n\nParticipants are instructed to fork the CH repo to their own github, then clone the CH repo. This way they can push any updates or changes to the CH repo within their own version of the repo without impacting the original CH repo\nThis should be covered in Tutorial 0.\n\nWhat happens to my work on 2i2c after the 3 months?\n\nCode: push to your own Github repo\nData and analysis outputs - working on S3 solution\n\nFrom Clinic chapter: “This section is a step-by-step guide to set up git on your 2i2c instance and configure git to use your github.com account for managing your repositories hosted on github.com.” We are also looking into using Git Extension.\n\n\n\nZoom teleconference\n\nHost: Erin and Julie\nBreak-out rooms\n\nWe will set up Breakout rooms ahead of time, and have helpers ready to meet with participants. If there are many people in breakout rooms/lots of same issues, we’ll pause the tutorial and address things together\nIf a participant is stuck and needs 1:1 help, ping one of the Zoom hosts in the mentor/general channel to place you in a breakout room with participant X\n\nChat - Encourage participants to use the Slack general or help channel for questions, not the Zoom chat\n\nErin will mention this in the Welcome (logistics) session beginning of Day 1\n\n\n\n\nTutorials workflow\n\nWelcome mentor share link to CH Book during Welcome each day\nWelcome mentor to instruct participant to spin up 2i2c JupyterHub if they haven’t done so that morning already\nTutorial presenters can also role-model having the CH Book open as a tab when they teach if they want to refer to it.\nTutorial presenter will be using template notebooks based on each tutorial\nTutorial presenter will live code, and participants will follow along in their instance\nTutorial presenter screen zoom level should be 130%\n\n\n\nHelpers support during “live coding” Tutorials Demos\n\nTutorial mentor live codes, participants follow along\nAll other helpers monitor Slack help (and general) channel for questions, people being stuck\n\nWhen you see a question you will reply to, please add the “eyes” emoji below so other helpers know that you are looking into it. Then please reply in-thread in Slack to help. Can tag additional helpers if you need further support.\nIf needed, ask host to place you and participant in breakout room temporarily for easier help/support\n\nReminder: Update your Slack name temporarily to include “helper”, e.g. “Catalina Oaida (Helper)”\n\n\n\nHelper support during Team Hack Time\n\nZoom host - Place participants in breakout rooms based on their team\n\nCan collaborate in zoom breakout rooms and in their slack team DM (direct messaging)\n\nParticipants should post any team project questions in the help slack channel; screenshot are helpful\n\nParticipants can create a DM thread with their teammates for team work discussions\n\nMentors monitor the help slack channel for questions\n\nWhen you see a question you will reply to, please add the “eyes” emoji below so other helpers know that you are looking into it. Then please reply in-thread in Slack to help. Can tag additional helpers if you need further support.\nand/or ask host to be added to the team zoom breakout room\n\nMentors use the mentor slack channel if you need additional help from a colleague\n\nThey can then also be added to zoom breakout room, as needed\n\nBeyond the scheduled time for Team Hack Time, the zoom meeting will close out, but participants are welcome to continue to discuss via their DM in slack\n\nNo on-call mentor support beyond the scheduled team hack time session\nAdditional help from mentors available 8-9am Tue-Fri during optional office hours\n\n\n\n\n2i2c JupyterHub\nLog-in before we start each day to our JupyterHub instance.",
    "crumbs": [
      "Logistics",
      "Logistics overview"
    ]
  },
  {
    "objectID": "logistics/prerequisites.html",
    "href": "logistics/prerequisites.html",
    "title": "Prerequisites & help",
    "section": "",
    "text": "Before the Hackathon, please do the following (20 minutes). All software is free. If you are attending the Clinic, please do this in advance of the Clinic.\n\nGitHub username\n\nCreate a GitHub account (if you don’t already have one) at https://github.com. Follow optional advice on choosing your username\nPlease provide your GitHub username here; this will allow us to add you to the cloud hackathon workspace.\nRemember your username and password; you will need to be logged in during the workshop!\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov\nRemember your username and password; you will need to be logged in during the workshop!\n\nSlack\n\nJoin our Slack workspace (invite sent via email). In the 2021-nasacloudhack-projects channel, suggest a hackathon project idea/use case, and mention if you are looking for teammates to join. Read through other entries and comment on those of interest to you. On Day one of the hackathon we will hold a project pitchfest and finalize teams for the week.\nLearn more about how we’ll use Slack during the workshop\n\nZoom\n\nBe prepared to call into Zoom using the link provided in the Slack 2021-nasacloudhack-general Channel.\n\nGet comfortable\n\nConsider your computer set-up in advance, including an external monitor if possible. You will be following along in Jupyter Hub on your own computer while also watching an instructor live-code over Zoom (or equivalent), and will also want quick-access to Slack to ask for help and follow links.",
    "crumbs": [
      "Logistics",
      "Prerequisites & help"
    ]
  },
  {
    "objectID": "logistics/prerequisites.html#prerequisites",
    "href": "logistics/prerequisites.html#prerequisites",
    "title": "Prerequisites & help",
    "section": "",
    "text": "Before the Hackathon, please do the following (20 minutes). All software is free. If you are attending the Clinic, please do this in advance of the Clinic.\n\nGitHub username\n\nCreate a GitHub account (if you don’t already have one) at https://github.com. Follow optional advice on choosing your username\nPlease provide your GitHub username here; this will allow us to add you to the cloud hackathon workspace.\nRemember your username and password; you will need to be logged in during the workshop!\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov\nRemember your username and password; you will need to be logged in during the workshop!\n\nSlack\n\nJoin our Slack workspace (invite sent via email). In the 2021-nasacloudhack-projects channel, suggest a hackathon project idea/use case, and mention if you are looking for teammates to join. Read through other entries and comment on those of interest to you. On Day one of the hackathon we will hold a project pitchfest and finalize teams for the week.\nLearn more about how we’ll use Slack during the workshop\n\nZoom\n\nBe prepared to call into Zoom using the link provided in the Slack 2021-nasacloudhack-general Channel.\n\nGet comfortable\n\nConsider your computer set-up in advance, including an external monitor if possible. You will be following along in Jupyter Hub on your own computer while also watching an instructor live-code over Zoom (or equivalent), and will also want quick-access to Slack to ask for help and follow links.",
    "crumbs": [
      "Logistics",
      "Prerequisites & help"
    ]
  },
  {
    "objectID": "logistics/prerequisites.html#getting-help",
    "href": "logistics/prerequisites.html#getting-help",
    "title": "Prerequisites & help",
    "section": "Getting help",
    "text": "Getting help\nWe will use Slack rather than Zoom Chat as our main channel for help, since the conversations are preserved beyond a single call and since it’s easier to reply and have threaded conversations and post screenshots.\n\nSlack\nYou will be invited to the Openscapes Slack organization, where there is a growing community of Openscapes Champions. (New to Slack? See this Quick Start Guide). We have three private Slack channels for the Hackathon:\n2021-nasacloudhack-general: General channel is for announcements and general questions and communications.\n2021-nasacloudhack-projects: Projects channel is for for participants to pitch project ideas and discuss projects with the whole hackathon group. As teams form you can have direct messages with your team (and we can create new channels as needed).\n2021-nasacloudhack-help: Help channel is the place to get troubleshooting help: please paste error messages and post screenshots and we will help you by replying to your post.\nTo create a screenshot:\n\nOn your Mac - Screenshot\nOn your PC - Snipping Tool\n\n\n\nZoom Breakout Rooms\n\nDuring Tutorials Session\nIf you’d like to talk to someone and live-screenshare about your issue, please write in Zoom Chat that you need help and we will move you into a breakout room with a helper.\n\n\nDuring Team Hacktime\nDuring the team project time, you will be placed in a Zoom breakout room with your respective teammates to collaborate more easily. If you have questions as you work, post your question(s) in the Slack 2021-nasacloudhack-help Channel and a helper will respond in that thread. If needed, a helper can also join your team’s Zoom breakour room for easy screensharing, troubleshooting or to further discuss a question.",
    "crumbs": [
      "Logistics",
      "Prerequisites & help"
    ]
  },
  {
    "objectID": "further-resources.html",
    "href": "further-resources.html",
    "title": "Additional resources",
    "section": "",
    "text": "NASA Earthdata: How to Cloud\nUSGS Eyes on Earth Podcast: Satellites and Cloud Computing - with Aaron Friesz (LP DAAC!)\nPO.DAAC Cloud Data Page\nPO.DAAC Earthdata Webinar (Aug 2021): Surfing Ocean Data in the Cloud - The Beginner’s Guide to PO.DAAC in the NASA Earthdata Cloud\nPO.DAAC Github Repository\nNASA Earthdata Cloud Primer -AWS cloud primer: helpful tutorials for how to set up your own EC2 cloud instance in AWS, attach storeage, move files back and forth, and more.\nSetting up Jupyter Notebooks in a user EC2 instance in AWS - helpful blog post for setting up jupyter notebooks in an EC2 instance in AWS. (Builds on the Cloud Primer tutorials, which are missing that next step)\nRunning the NASA Cloud Workshop notebooks with mybinder.org - by Eli Holmes, 2021 Cloud Hackathon Participant who then set up working in Binder",
    "crumbs": [
      "Further Resources",
      "Additional resources"
    ]
  },
  {
    "objectID": "further-resources.html#a-growing-list-of-resources",
    "href": "further-resources.html#a-growing-list-of-resources",
    "title": "Additional resources",
    "section": "",
    "text": "NASA Earthdata: How to Cloud\nUSGS Eyes on Earth Podcast: Satellites and Cloud Computing - with Aaron Friesz (LP DAAC!)\nPO.DAAC Cloud Data Page\nPO.DAAC Earthdata Webinar (Aug 2021): Surfing Ocean Data in the Cloud - The Beginner’s Guide to PO.DAAC in the NASA Earthdata Cloud\nPO.DAAC Github Repository\nNASA Earthdata Cloud Primer -AWS cloud primer: helpful tutorials for how to set up your own EC2 cloud instance in AWS, attach storeage, move files back and forth, and more.\nSetting up Jupyter Notebooks in a user EC2 instance in AWS - helpful blog post for setting up jupyter notebooks in an EC2 instance in AWS. (Builds on the Cloud Primer tutorials, which are missing that next step)\nRunning the NASA Cloud Workshop notebooks with mybinder.org - by Eli Holmes, 2021 Cloud Hackathon Participant who then set up working in Binder",
    "crumbs": [
      "Further Resources",
      "Additional resources"
    ]
  },
  {
    "objectID": "further-resources.html#additional-tutorials",
    "href": "further-resources.html#additional-tutorials",
    "title": "Additional resources",
    "section": "Additional tutorials",
    "text": "Additional tutorials\n\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links\nDirect access to ECCO data in S3 (from us-west-2) - Direct S3 access example with netCDF data\nDirect_S3_Access__gdalvrt\nDirect_S3_Access__rioxarray_clipping\nGetting Started with Cloud-Native Harmonized Landsat Sentinel-2 (HLS) Data in R\nCalculate black-sky, white-sky, and actual albedo (MCD43A) from MCD43A1 BRDF Parameters using R\nXarray Zonal Statistics",
    "crumbs": [
      "Further Resources",
      "Additional resources"
    ]
  },
  {
    "objectID": "tutorials/05_Data_Access_Direct_S3.html",
    "href": "tutorials/05_Data_Access_Direct_S3.html",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "",
    "text": "Exercise: 20 minutes",
    "crumbs": [
      "Tutorials",
      "05. Direct S3 Data Access with rioxarray"
    ]
  },
  {
    "objectID": "tutorials/05_Data_Access_Direct_S3.html#timing",
    "href": "tutorials/05_Data_Access_Direct_S3.html#timing",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "",
    "text": "Exercise: 20 minutes",
    "crumbs": [
      "Tutorials",
      "05. Direct S3 Data Access with rioxarray"
    ]
  },
  {
    "objectID": "tutorials/05_Data_Access_Direct_S3.html#summary",
    "href": "tutorials/05_Data_Access_Direct_S3.html#summary",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "Summary",
    "text": "Summary\nIn the previous exercises we searched for and discovered cloud data assets that met certain search criteria (i.e., intersects with our region of interest and for a specified date range). The end goal was to find and save web links to the data assets we want to use in our workflow. The links we found allow us to download data via HTTPS (Hypertext Transfer Protocol Secure). However, NASA allows for direct in-region S3 bucket access for the same assets. In addition to saving the HTTPS links, we also created and saved the S3 links for those same cloud assets and we will use them here. In this exercise we will demonstrate how to perform direct in-region S3 bucket access for Harmonized Landsat Sentinel-2 (HLS) cloud data assets.\n\nDirect S3 Access\nNASA Earthdata Cloud provides two pathways for accessing data from the cloud. The first is via HTTPS. The other is through direct S3 bucket access. Below are some benefits and considerations when choosing to use direct S3 bucket access for NASA cloud assets.\n\nBenefits\n\nRetrieving data can be much quicker\nNo need to download data! Work with data in a more efficient manner, “next to it, in the cloud”\n\nIncreased capacity to do parallel processing, due to working in the cloud\n\nYou are working completely within the AWS cloud ecosystem and thus have access to the might of all AWS offerings (e.g., infrastructure, S3 API, services, etc.)\n\n\n\nConsiderations\n\nIf your workflow is in the cloud, choose S3 over HTTPS\n\nAccess only works within AWS us-west-2 region\n\nNeed an AWS S3 “token” to access S3 Bucket\n\nToken expires after 1 hour (currently)\n\nToken only works at the DAAC that generates it, e.g.,\n\nPO.DAAC token generator: https://archive.podaac.earthdata.nasa.gov/s3credentials\n\nLP DAAC token generator: https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials\n\n\nDirect S3 access on its own does not solve ‘cloud’ problems, but it is one key technology in solving big data problems\n\nStill have to load things in to memory, parallelize the computation, if working with really large data volumes. There are a lot of tools that allow you to do that, but are not discussed in this tutorial",
    "crumbs": [
      "Tutorials",
      "05. Direct S3 Data Access with rioxarray"
    ]
  },
  {
    "objectID": "tutorials/05_Data_Access_Direct_S3.html#what-you-will-learn-from-this-tutorial",
    "href": "tutorials/05_Data_Access_Direct_S3.html#what-you-will-learn-from-this-tutorial",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "What you will learn from this tutorial",
    "text": "What you will learn from this tutorial\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\n\nhow to configure our notebook environment for in-region direct S3 bucket access\n\nhow to access a single HLS file via in-region direct S3 bucket access\n\nhow to create an HLS time series data array from cloud assets via in-region direct S3 bucket access\n\nhow to plot results\n\nThis exercise can be found in the 2021 Cloud Hackathon Book",
    "crumbs": [
      "Tutorials",
      "05. Direct S3 Data Access with rioxarray"
    ]
  },
  {
    "objectID": "tutorials/05_Data_Access_Direct_S3.html#import-required-packages",
    "href": "tutorials/05_Data_Access_Direct_S3.html#import-required-packages",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "Import Required Packages",
    "text": "Import Required Packages\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport os\nimport requests\nimport boto3\nimport numpy as np\nimport xarray as xr\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nfrom rasterio.plot import show\nimport rioxarray\nimport geoviews as gv\nimport hvplot.xarray\nimport holoviews as hv\ngv.extension('bokeh', 'matplotlib')",
    "crumbs": [
      "Tutorials",
      "05. Direct S3 Data Access with rioxarray"
    ]
  },
  {
    "objectID": "tutorials/05_Data_Access_Direct_S3.html#configure-local-environment-and-get-temporary-credentials",
    "href": "tutorials/05_Data_Access_Direct_S3.html#configure-local-environment-and-get-temporary-credentials",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "Configure Local Environment and Get Temporary Credentials",
    "text": "Configure Local Environment and Get Temporary Credentials\nTo perform direct S3 data access one needs to acquire temporary S3 credentials. The credentials give users direct access to S3 buckets in NASA Earthdata Cloud. AWS credentials should not be shared, so take precautions when using them in notebooks our scripts. Note, these temporary credentials are valid for only 1 hour. For more information regarding the temporary credentials visit https://data.lpdaac.earthdatacloud.nasa.gov/s3credentialsREADME. A netrc file is required to aquire these credentials. Use the NASA Earthdata Authentication to create a netrc file in your home directory.\n\ns3_cred_endpoint = 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\n\n\ndef get_temp_creds():\n    temp_creds_url = s3_cred_endpoint\n    return requests.get(temp_creds_url).json()\n\n\ntemp_creds_req = get_temp_creds()\n#temp_creds_req                      # !!! BEWARE, removing the # on this line will print your temporary S3 credentials.\n\n\nInsert the credentials into our boto3 session and configure our rasterio environment for data access\nCreate a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nGDAL environment variables must be configured to access Earthdata Cloud data assets. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\n&lt;rasterio.env.Env at 0x7f1619fbb0a0&gt;",
    "crumbs": [
      "Tutorials",
      "05. Direct S3 Data Access with rioxarray"
    ]
  },
  {
    "objectID": "tutorials/05_Data_Access_Direct_S3.html#read-in-s3-links",
    "href": "tutorials/05_Data_Access_Direct_S3.html#read-in-s3-links",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "Read in S3 Links",
    "text": "Read in S3 Links\nIn the CMR-STAC API tutorial we saved off multiple text file containing links, both HTTPS and S3 links, to Harmonized Landsat Sentinel-2 (HLS) cloud data assets. We will now read in one of those file and show how to access those data assets.\n\nList the available files in the data directory\nNOTE: If you are running the notebook from the tutorials-templates directory, please use the following path to connect to the data directoy: “../tutorials/data”\n\n[f for f in os.listdir('./data') if '.txt' in f]\n\n['HTTPS_T13TGF_B02_Links.txt',\n 'S3_T13TGF_B05_Links.txt',\n 'HTTPS_T13TGF_Fmask_Links.txt',\n 'S3_T13TGF_B8A_Links.txt',\n 'HTTPS_T13TGF_B04_Links.txt',\n 'S3_T13TGF_B04_Links.txt',\n 'S3_T13TGF_Fmask_Links.txt',\n 'HTTPS_T13TGF_B8A_Links.txt',\n 'HTTPS_T13TGF_B05_Links.txt',\n 'S3_T13TGF_B02_Links.txt']\n\n\nWe will save our list of links and a single link as Python objects for use later.\nNOTE: If you are running the notebook from the tutorials-templates directory, please use the following path to connect to the data directoy: “../tutorials/data”\n\ns3_links = open('./data/S3_T13TGF_B04_Links.txt').read().splitlines()\ns3_links\n\n['s3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021133T173859.v2.0/HLS.S30.T13TGF.2021133T173859.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021140T173021.v2.0/HLS.L30.T13TGF.2021140T173021.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021140T172859.v2.0/HLS.S30.T13TGF.2021140T172859.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021145T172901.v2.0/HLS.S30.T13TGF.2021145T172901.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021155T172901.v2.0/HLS.S30.T13TGF.2021155T172901.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021156T173029.v2.0/HLS.L30.T13TGF.2021156T173029.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021158T173901.v2.0/HLS.S30.T13TGF.2021158T173901.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021163T173909.v2.0/HLS.S30.T13TGF.2021163T173909.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021165T172422.v2.0/HLS.L30.T13TGF.2021165T172422.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021165T172901.v2.0/HLS.S30.T13TGF.2021165T172901.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021173T173909.v2.0/HLS.S30.T13TGF.2021173T173909.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021185T172901.v2.0/HLS.S30.T13TGF.2021185T172901.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021188T173037.v2.0/HLS.L30.T13TGF.2021188T173037.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021190T172859.v2.0/HLS.S30.T13TGF.2021190T172859.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021193T173909.v2.0/HLS.S30.T13TGF.2021193T173909.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021198T173911.v2.0/HLS.S30.T13TGF.2021198T173911.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021200T172859.v2.0/HLS.S30.T13TGF.2021200T172859.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021203T173909.v2.0/HLS.S30.T13TGF.2021203T173909.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021204T173042.v2.0/HLS.L30.T13TGF.2021204T173042.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021208T173911.v2.0/HLS.S30.T13TGF.2021208T173911.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021210T172859.v2.0/HLS.S30.T13TGF.2021210T172859.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021215T172901.v2.0/HLS.S30.T13TGF.2021215T172901.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021218T173911.v2.0/HLS.S30.T13TGF.2021218T173911.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021220T173049.v2.0/HLS.L30.T13TGF.2021220T173049.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021220T172859.v2.0/HLS.S30.T13TGF.2021220T172859.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021223T173909.v2.0/HLS.S30.T13TGF.2021223T173909.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021228T173911.v2.0/HLS.S30.T13TGF.2021228T173911.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021229T172441.v2.0/HLS.L30.T13TGF.2021229T172441.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021230T172859.v2.0/HLS.S30.T13TGF.2021230T172859.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021235T172901.v2.0/HLS.S30.T13TGF.2021235T172901.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021243T173859.v2.0/HLS.S30.T13TGF.2021243T173859.v2.0.B04.tif']\n\n\n\ns3_link = s3_links[0]\ns3_link\n\n's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B04.tif'",
    "crumbs": [
      "Tutorials",
      "05. Direct S3 Data Access with rioxarray"
    ]
  },
  {
    "objectID": "tutorials/05_Data_Access_Direct_S3.html#read-in-a-single-hls-file",
    "href": "tutorials/05_Data_Access_Direct_S3.html#read-in-a-single-hls-file",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "Read in a single HLS file",
    "text": "Read in a single HLS file\nWe’ll access the HLS S3 object using the rioxarray Python package. The package is an extension of xarray and rasterio, allowing users to read in and interact with geospatial data using xarray data structures. We will also be leveraging the tight integration between xarray and dask to lazily read in data via the chunks parameter. This allows us to connect to the HLS S3 object, reading only metadata, an not load the data into memory until we request it via the loads() function.\n\nhls_da = rioxarray.open_rasterio(s3_link, chuncks=True)\nhls_da\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 3660, x: 3660)&gt;\n[13395600 values with dtype=int16]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayband: 1y: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (4)band(band)int641array([1])x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\nWhen GeoTIFFS/Cloud Optimized GeoTIFFS are read in, a band coordinate variable is automatically created (see the print out above). In this exercise we will not use that coordinate variable, so we will remove it using the squeeze() function to avoid confusion.\n\nhls_da = hls_da.squeeze('band', drop=True)\nhls_da\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (y: 3660, x: 3660)&gt;\n[13395600 values with dtype=int16]\nCoordinates:\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayy: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (3)x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\n\nPlot the HLS S3 object\n\nhls_da.hvplot.image(x='x', y='y', cmap='fire', rasterize=True, width=800, height=600, colorbar=True)    # colormaps -&gt; https://holoviews.org/user_guide/Colormaps.html\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe can print out the data value as a numpy array by typing .values\n\nhls_da.values\n\narray([[-9999, -9999, -9999, ...,  1527,  1440,  1412],\n       [-9999, -9999, -9999, ...,  1493,  1476,  1407],\n       [-9999, -9999, -9999, ...,  1466,  1438,  1359],\n       ...,\n       [-9999, -9999, -9999, ...,  1213,  1295,  1159],\n       [-9999, -9999, -9999, ...,  1042,  1232,  1185],\n       [-9999, -9999, -9999, ...,   954,  1127,  1133]], dtype=int16)\n\n\nUp to this point, we have not saved anything but metadata into memory. To save or load the data into memory we can call the .load() function.\n\nhls_da_data = hls_da.load()\nhls_da_data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (y: 3660, x: 3660)&gt;\narray([[-9999, -9999, -9999, ...,  1527,  1440,  1412],\n       [-9999, -9999, -9999, ...,  1493,  1476,  1407],\n       [-9999, -9999, -9999, ...,  1466,  1438,  1359],\n       ...,\n       [-9999, -9999, -9999, ...,  1213,  1295,  1159],\n       [-9999, -9999, -9999, ...,  1042,  1232,  1185],\n       [-9999, -9999, -9999, ...,   954,  1127,  1133]], dtype=int16)\nCoordinates:\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayy: 3660x: 3660-9999 -9999 -9999 -9999 -9999 -9999 ... 1676 1486 1112 954 1127 1133array([[-9999, -9999, -9999, ...,  1527,  1440,  1412],\n       [-9999, -9999, -9999, ...,  1493,  1476,  1407],\n       [-9999, -9999, -9999, ...,  1466,  1438,  1359],\n       ...,\n       [-9999, -9999, -9999, ...,  1213,  1295,  1159],\n       [-9999, -9999, -9999, ...,  1042,  1232,  1185],\n       [-9999, -9999, -9999, ...,   954,  1127,  1133]], dtype=int16)Coordinates: (3)x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\n\ndel(hls_da_data)",
    "crumbs": [
      "Tutorials",
      "05. Direct S3 Data Access with rioxarray"
    ]
  },
  {
    "objectID": "tutorials/05_Data_Access_Direct_S3.html#read-in-hls-as-a-time-series",
    "href": "tutorials/05_Data_Access_Direct_S3.html#read-in-hls-as-a-time-series",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "Read in HLS as a time series",
    "text": "Read in HLS as a time series\nNow we’ll read in multiple HLS S3 objects as a time series xarray. Let’s print the links list again to see what we’re working with.\n\ns3_links\n\n['s3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021133T173859.v2.0/HLS.S30.T13TGF.2021133T173859.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021140T173021.v2.0/HLS.L30.T13TGF.2021140T173021.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021140T172859.v2.0/HLS.S30.T13TGF.2021140T172859.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021145T172901.v2.0/HLS.S30.T13TGF.2021145T172901.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021155T172901.v2.0/HLS.S30.T13TGF.2021155T172901.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021156T173029.v2.0/HLS.L30.T13TGF.2021156T173029.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021158T173901.v2.0/HLS.S30.T13TGF.2021158T173901.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021163T173909.v2.0/HLS.S30.T13TGF.2021163T173909.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021165T172422.v2.0/HLS.L30.T13TGF.2021165T172422.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021165T172901.v2.0/HLS.S30.T13TGF.2021165T172901.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021173T173909.v2.0/HLS.S30.T13TGF.2021173T173909.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021185T172901.v2.0/HLS.S30.T13TGF.2021185T172901.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021188T173037.v2.0/HLS.L30.T13TGF.2021188T173037.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021190T172859.v2.0/HLS.S30.T13TGF.2021190T172859.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021193T173909.v2.0/HLS.S30.T13TGF.2021193T173909.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021198T173911.v2.0/HLS.S30.T13TGF.2021198T173911.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021200T172859.v2.0/HLS.S30.T13TGF.2021200T172859.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021203T173909.v2.0/HLS.S30.T13TGF.2021203T173909.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021204T173042.v2.0/HLS.L30.T13TGF.2021204T173042.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021208T173911.v2.0/HLS.S30.T13TGF.2021208T173911.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021210T172859.v2.0/HLS.S30.T13TGF.2021210T172859.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021215T172901.v2.0/HLS.S30.T13TGF.2021215T172901.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021218T173911.v2.0/HLS.S30.T13TGF.2021218T173911.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021220T173049.v2.0/HLS.L30.T13TGF.2021220T173049.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021220T172859.v2.0/HLS.S30.T13TGF.2021220T172859.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021223T173909.v2.0/HLS.S30.T13TGF.2021223T173909.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021228T173911.v2.0/HLS.S30.T13TGF.2021228T173911.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021229T172441.v2.0/HLS.L30.T13TGF.2021229T172441.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021230T172859.v2.0/HLS.S30.T13TGF.2021230T172859.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021235T172901.v2.0/HLS.S30.T13TGF.2021235T172901.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T13TGF.2021243T173859.v2.0/HLS.S30.T13TGF.2021243T173859.v2.0.B04.tif']\n\n\nCurrently, the utilities and packages used in Python to read in GeoTIFF/COG file do not recognize associated dates stored in the internal metadata. To account for the dates for each file we must create a time variable and add it as a dimension in our final time series xarray. We’ll create a function that extracts the date from the file link and create an xarray variable with a time array of datetime objects.\n\ndef time_index_from_filenames(file_links):\n    '''\n    Helper function to create a pandas DatetimeIndex\n    '''\n    return [datetime.strptime(f.split('.')[-5], '%Y%jT%H%M%S') for f in file_links]\n\n\ntime = xr.Variable('time', time_index_from_filenames(s3_links))\n\nWe’ll now specify a chunk size to use that matches the internal tiling of HLS files. This will help improve performance.\n\nchunks=dict(band=1, x=512, y=512)\n\nNow, we will create our time series.\n\nhls_ts_da = xr.concat([rioxarray.open_rasterio(f, chunks=chunks).squeeze('band', drop=True) for f in s3_links], dim=time)\nhls_ts_da\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (time: 32, y: 3660, x: 3660)&gt;\ndask.array&lt;concatenate, shape=(32, 3660, 3660), dtype=int16, chunksize=(1, 512, 512), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.6e+06 4.6e+06 4.6e+06 ... 4.49e+06 4.49e+06\n    spatial_ref  int64 0\n  * time         (time) datetime64[ns] 2021-05-13T17:24:06 ... 2021-08-31T17:...\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArraytime: 32y: 3660x: 3660dask.array&lt;chunksize=(1, 512, 512), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n817.60 MiB\n512.00 kiB\n\n\nShape\n(32, 3660, 3660)\n(1, 512, 512)\n\n\nCount\n8224 Tasks\n2048 Chunks\n\n\nType\nint16\nnumpy.ndarray\n\n\n\n\n                                                                                         3660 3660 32\n\n\n\n\nCoordinates: (4)x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.6e+06 4.6e+06 ... 4.49e+06array([4600005., 4599975., 4599945., ..., 4490295., 4490265., 4490235.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4600020.0 0.0 -30.0array(0)time(time)datetime64[ns]2021-05-13T17:24:06 ... 2021-08-...array(['2021-05-13T17:24:06.000000000', '2021-05-13T17:38:59.000000000',\n       '2021-05-20T17:30:21.000000000', '2021-05-20T17:28:59.000000000',\n       '2021-05-25T17:29:01.000000000', '2021-06-04T17:29:01.000000000',\n       '2021-06-05T17:30:29.000000000', '2021-06-07T17:39:01.000000000',\n       '2021-06-12T17:39:09.000000000', '2021-06-14T17:24:22.000000000',\n       '2021-06-14T17:29:01.000000000', '2021-06-22T17:39:09.000000000',\n       '2021-07-04T17:29:01.000000000', '2021-07-07T17:30:37.000000000',\n       '2021-07-09T17:28:59.000000000', '2021-07-12T17:39:09.000000000',\n       '2021-07-17T17:39:11.000000000', '2021-07-19T17:28:59.000000000',\n       '2021-07-22T17:39:09.000000000', '2021-07-23T17:30:42.000000000',\n       '2021-07-27T17:39:11.000000000', '2021-07-29T17:28:59.000000000',\n       '2021-08-03T17:29:01.000000000', '2021-08-06T17:39:11.000000000',\n       '2021-08-08T17:30:49.000000000', '2021-08-08T17:28:59.000000000',\n       '2021-08-11T17:39:09.000000000', '2021-08-16T17:39:11.000000000',\n       '2021-08-17T17:24:41.000000000', '2021-08-18T17:28:59.000000000',\n       '2021-08-23T17:29:01.000000000', '2021-08-31T17:38:59.000000000'],\n      dtype='datetime64[ns]')Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\nSince we used the chunks parameter while reading the data, the hls_ts_da object is not read into memory yet. To do that we’ll use the load() function.\nNow, we’ll see what we have. Use hvplot to plot our time series\n\nhls_ts_da_data = hls_ts_da.load()\n\n\nhls_ts_da_data.hvplot.image(x='x', y='y', rasterize=True, width=800, height=600, colorbar=True, cmap='fire').opts(clim=(hls_ts_da_data.values.min(), hls_ts_da_data.values.max()))\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n# Exit our context\nrio_env.__exit__()",
    "crumbs": [
      "Tutorials",
      "05. Direct S3 Data Access with rioxarray"
    ]
  },
  {
    "objectID": "tutorials/05_Data_Access_Direct_S3.html#concluding-remarks",
    "href": "tutorials/05_Data_Access_Direct_S3.html#concluding-remarks",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nThe above exercise demonstrated how to perform in-region direct S3 bucket access for HLS cloud data assets. HLS cloud data assets are stored as Cloud Optimized GeoTIFFs, a format that has been the benifactor of data discovery and access advancements within the Python ecosystem. Knowing what the data storage format is (e.g., COG, netcdf4, or zarr store) and/or what data access protocol you’re using is critical in determining what Python data access method you will use. For COG data, rioxarray package is often prefered due to is ability to bring the geospatial data format into an xarray object. For netcdf4 files, the standard xarray package incombination with s3fs allow users to perform in-region direct access reads into an xarray object. Finally, if you are using OPeNDAP to connect to data, specialized packages like pydap have been integrated into xarray for streamline access directly to an xarray object.",
    "crumbs": [
      "Tutorials",
      "05. Direct S3 Data Access with rioxarray"
    ]
  },
  {
    "objectID": "tutorials/05_Data_Access_Direct_S3.html#resources",
    "href": "tutorials/05_Data_Access_Direct_S3.html#resources",
    "title": "05. Direct S3 Data Access with rioxarray",
    "section": "Resources",
    "text": "Resources\n\nBuild time series from multiple GeoTIFF files\nHvplot/Holoview Colormap\nhttps://git.earthdata.nasa.gov/projects/LPDUR/repos/lpdaac_cloud_data_access/browse\nhttps://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-tutorial/browse\nDirect S3 Data Access - Rough PODAAC ECCO SSH Example\nDirect access to ECCO data in S3 (from us-west-2)\nDirect S3 Data Access with GDAL Virtual Raster Format (VRT)\nDirect S3 Data Access with rioxarray - Clipping Example",
    "crumbs": [
      "Tutorials",
      "05. Direct S3 Data Access with rioxarray"
    ]
  },
  {
    "objectID": "tutorials/Additional_Resources__Data_Access__Direct_S3_Access__PODAAC_ECCO_SSH.html#single-file-in-region-direct-s3-access-of-netcdf-file",
    "href": "tutorials/Additional_Resources__Data_Access__Direct_S3_Access__PODAAC_ECCO_SSH.html#single-file-in-region-direct-s3-access-of-netcdf-file",
    "title": "Direct S3 Data Access - Rough PODAAC ECCO SSH Example",
    "section": "Single file in-region direct S3 access of netcdf file",
    "text": "Single file in-region direct S3 access of netcdf file\n\nfs_s3 = s3fs.S3FileSystem(anon=False, key=temp_creds_req['accessKeyId'], secret=temp_creds_req['secretAccessKey'], token=temp_creds_req['sessionToken'])\n\n\ns3_file_obj = fs_s3.open(ssh_s3, mode='rb')\n\n\nssh_xr = xr.open_dataset(s3_file_obj, engine='h5netcdf')\nssh_xr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:         (time: 1, latitude: 360, longitude: 720, nv: 2)\nCoordinates:\n  * time            (time) datetime64[ns] 2015-01-16T12:00:00\n  * latitude        (latitude) float32 -89.75 -89.25 -88.75 ... 89.25 89.75\n  * longitude       (longitude) float32 -179.8 -179.2 -178.8 ... 179.2 179.8\n    time_bnds       (time, nv) datetime64[ns] 2015-01-01 2015-02-01\n    latitude_bnds   (latitude, nv) float32 -90.0 -89.5 -89.5 ... 89.5 89.5 90.0\n    longitude_bnds  (longitude, nv) float32 -180.0 -179.5 -179.5 ... 179.5 180.0\nDimensions without coordinates: nv\nData variables:\n    SSH             (time, latitude, longitude) float32 ...\n    SSHIBC          (time, latitude, longitude) float32 ...\n    SSHNOIBC        (time, latitude, longitude) float32 ...\nAttributes: (12/57)\n    acknowledgement:              This research was carried out by the Jet Pr...\n    author:                       Ian Fenty and Ou Wang\n    cdm_data_type:                Grid\n    comment:                      Fields provided on a regular lat-lon grid. ...\n    Conventions:                  CF-1.8, ACDD-1.3\n    coordinates_comment:          Note: the global 'coordinates' attribute de...\n    ...                           ...\n    time_coverage_duration:       P1M\n    time_coverage_end:            2015-02-01T00:00:00\n    time_coverage_resolution:     P1M\n    time_coverage_start:          2015-01-01T00:00:00\n    title:                        ECCO Sea Surface Height - Monthly Mean 0.5 ...\n    uuid:                         088d03b8-4158-11eb-876b-0cc47a3f47f1xarray.DatasetDimensions:time: 1latitude: 360longitude: 720nv: 2Coordinates: (6)time(time)datetime64[ns]2015-01-16T12:00:00axis :Tbounds :time_bndscoverage_content_type :coordinatelong_name :center time of averaging periodstandard_name :timearray(['2015-01-16T12:00:00.000000000'], dtype='datetime64[ns]')latitude(latitude)float32-89.75 -89.25 ... 89.25 89.75axis :Ybounds :latitude_bndscomment :uniform grid spacing from -89.75 to 89.75 by 0.5coverage_content_type :coordinatelong_name :latitude at grid cell centerstandard_name :latitudeunits :degrees_northarray([-89.75, -89.25, -88.75, ...,  88.75,  89.25,  89.75], dtype=float32)longitude(longitude)float32-179.8 -179.2 ... 179.2 179.8axis :Xbounds :longitude_bndscomment :uniform grid spacing from -179.75 to 179.75 by 0.5coverage_content_type :coordinatelong_name :longitude at grid cell centerstandard_name :longitudeunits :degrees_eastarray([-179.75, -179.25, -178.75, ...,  178.75,  179.25,  179.75],\n      dtype=float32)time_bnds(time, nv)datetime64[ns]...comment :Start and end times of averaging period.coverage_content_type :coordinatelong_name :time bounds of averaging periodarray([['2015-01-01T00:00:00.000000000', '2015-02-01T00:00:00.000000000']],\n      dtype='datetime64[ns]')latitude_bnds(latitude, nv)float32...coverage_content_type :coordinatelong_name :latitude bounds grid cellsarray([[-90. , -89.5],\n       [-89.5, -89. ],\n       [-89. , -88.5],\n       ...,\n       [ 88.5,  89. ],\n       [ 89. ,  89.5],\n       [ 89.5,  90. ]], dtype=float32)longitude_bnds(longitude, nv)float32...coverage_content_type :coordinatelong_name :longitude bounds grid cellsarray([[-180. , -179.5],\n       [-179.5, -179. ],\n       [-179. , -178.5],\n       ...,\n       [ 178.5,  179. ],\n       [ 179. ,  179.5],\n       [ 179.5,  180. ]], dtype=float32)Data variables: (3)SSH(time, latitude, longitude)float32...coverage_content_type :modelResultlong_name :Dynamic sea surface height anomalystandard_name :sea_surface_height_above_geoidunits :mcomment :Dynamic sea surface height anomaly above the geoid, suitable for comparisons with altimetry sea surface height data products that apply the inverse barometer (IB) correction. Note: SSH is calculated by correcting model sea level anomaly ETAN for three effects: a) global mean steric sea level changes related to density changes in the Boussinesq volume-conserving model (Greatbatch correction, see sterGloH), b) the inverted barometer (IB) effect (see SSHIBC) and c) sea level displacement due to sea-ice and snow pressure loading (see sIceLoad). SSH can be compared with the similarly-named SSH variable in previous ECCO products that did not include atmospheric pressure loading (e.g., Version 4 Release 3). Use SSHNOIBC for comparisons with altimetry data products that do NOT apply the IB correction.valid_min :[-1.88057721]valid_max :[1.42077196][259200 values with dtype=float32]SSHIBC(time, latitude, longitude)float32...coverage_content_type :modelResultlong_name :The inverted barometer (IB) correction to sea surface height due to atmospheric pressure loadingunits :mcomment :Not an SSH itself, but a correction to model sea level anomaly (ETAN) required to account for the static part of sea surface displacement by atmosphere pressure loading: SSH = SSHNOIBC - SSHIBC. Note: Use SSH for model-data comparisons with altimetry data products that DO apply the IB correction and SSHNOIBC for comparisons with altimetry data products that do NOT apply the IB correction.valid_min :[-0.3014482]valid_max :[0.52456337][259200 values with dtype=float32]SSHNOIBC(time, latitude, longitude)float32...coverage_content_type :modelResultlong_name :Sea surface height anomaly without the inverted barometer (IB) correctionunits :mcomment :Sea surface height anomaly above the geoid without the inverse barometer (IB) correction, suitable for comparisons with altimetry sea surface height data products that do NOT apply the inverse barometer (IB) correction. Note: SSHNOIBC is calculated by correcting model sea level anomaly ETAN for two effects: a) global mean steric sea level changes related to density changes in the Boussinesq volume-conserving model (Greatbatch correction, see sterGloH), b) sea level displacement due to sea-ice and snow pressure loading (see sIceLoad). In ECCO Version 4 Release 4 the model is forced with atmospheric pressure loading. SSHNOIBC does not correct for the static part of the effect of atmosphere pressure loading on sea surface height (the so-called inverse barometer (IB) correction). Use SSH for comparisons with altimetry data products that DO apply the IB correction.valid_min :[-1.66542721]valid_max :[1.4550364][259200 values with dtype=float32]Attributes: (57)acknowledgement :This research was carried out by the Jet Propulsion Laboratory, managed by the California Institute of Technology under a contract with the National Aeronautics and Space Administration.author :Ian Fenty and Ou Wangcdm_data_type :Gridcomment :Fields provided on a regular lat-lon grid. They have been mapped to the regular lat-lon grid from the original ECCO lat-lon-cap 90 (llc90) native model grid. SSH (dynamic sea surface height) = SSHNOIBC (dynamic sea surface without the inverse barometer correction) - SSHIBC (inverse barometer correction). The inverted barometer correction accounts for variations in sea surface height due to atmospheric pressure variations.Conventions :CF-1.8, ACDD-1.3coordinates_comment :Note: the global 'coordinates' attribute describes auxillary coordinates.creator_email :ecco-group@mit.educreator_institution :NASA Jet Propulsion Laboratory (JPL)creator_name :ECCO Consortiumcreator_type :groupcreator_url :https://ecco-group.orgdate_created :2020-12-18T09:39:51date_issued :2020-12-18T09:39:51date_metadata_modified :2021-03-15T22:07:49date_modified :2021-03-15T22:07:49geospatial_bounds_crs :EPSG:4326geospatial_lat_max :[90.]geospatial_lat_min :[-90.]geospatial_lat_resolution :[0.5]geospatial_lat_units :degrees_northgeospatial_lon_max :[180.]geospatial_lon_min :[-180.]geospatial_lon_resolution :[0.5]geospatial_lon_units :degrees_easthistory :Inaugural release of an ECCO Central Estimate solution to PO.DAACid :10.5067/ECG5M-SSH44institution :NASA Jet Propulsion Laboratory (JPL)instrument_vocabulary :GCMD instrument keywordskeywords :EARTH SCIENCE &gt; OCEANS &gt; SEA SURFACE TOPOGRAPHY &gt; SEA SURFACE HEIGHT, EARTH SCIENCE SERVICES &gt; MODELS &gt; EARTH SCIENCE REANALYSES/ASSIMILATION MODELSkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :Public Domainmetadata_link :https://cmr.earthdata.nasa.gov/search/collections.umm_json?ShortName=ECCO_L4_SSH_05DEG_MONTHLY_V4R4naming_authority :gov.nasa.jplplatform :ERS-1/2, TOPEX/Poseidon, Geosat Follow-On (GFO), ENVISAT, Jason-1, Jason-2, CryoSat-2, SARAL/AltiKa, Jason-3, AVHRR, Aquarius, SSM/I, SSMIS, GRACE, DTU17MDT, Argo, WOCE, GO-SHIP, MEOP, Ice Tethered Profilers (ITP)platform_vocabulary :GCMD platform keywordsprocessing_level :L4product_name :SEA_SURFACE_HEIGHT_mon_mean_2015-01_ECCO_V4r4_latlon_0p50deg.ncproduct_time_coverage_end :2018-01-01T00:00:00product_time_coverage_start :1992-01-01T12:00:00product_version :Version 4, Release 4program :NASA Physical Oceanography, Cryosphere, Modeling, Analysis, and Prediction (MAP)project :Estimating the Circulation and Climate of the Ocean (ECCO)publisher_email :podaac@podaac.jpl.nasa.govpublisher_institution :PO.DAACpublisher_name :Physical Oceanography Distributed Active Archive Center (PO.DAAC)publisher_type :institutionpublisher_url :https://podaac.jpl.nasa.govreferences :ECCO Consortium, Fukumori, I., Wang, O., Fenty, I., Forget, G., Heimbach, P., & Ponte, R. M. 2020. Synopsis of the ECCO Central Production Global Ocean and Sea-Ice State Estimate (Version 4 Release 4). doi:10.5281/zenodo.3765928source :The ECCO V4r4 state estimate was produced by fitting a free-running solution of the MITgcm (checkpoint 66g) to satellite and in situ observational data in a least squares sense using the adjoint methodstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsummary :This dataset provides monthly-averaged dynamic sea surface height interpolated to a regular 0.5-degree grid from the ECCO Version 4 Release 4 (V4r4) ocean and sea-ice state estimate. Estimating the Circulation and Climate of the Ocean (ECCO) state estimates are dynamically and kinematically-consistent reconstructions of the three-dimensional, time-evolving ocean, sea-ice, and surface atmospheric states. ECCO V4r4 is a free-running solution of a global, nominally 1-degree configuration of the MIT general circulation model (MITgcm) that has been fit to observations in a least-squares sense. Observational data constraints used in V4r4 include sea surface height (SSH) from satellite altimeters [ERS-1/2, TOPEX/Poseidon, GFO, ENVISAT, Jason-1,2,3, CryoSat-2, and SARAL/AltiKa]; sea surface temperature (SST) from satellite radiometers [AVHRR], sea surface salinity (SSS) from the Aquarius satellite radiometer/scatterometer, ocean bottom pressure (OBP) from the GRACE satellite gravimeter; sea-ice concentration from satellite radiometers [SSM/I and SSMIS], and in-situ ocean temperature and salinity measured with conductivity-temperature-depth (CTD) sensors and expendable bathythermographs (XBTs) from several programs [e.g., WOCE, GO-SHIP, Argo, and others] and platforms [e.g., research vessels, gliders, moorings, ice-tethered profilers, and instrumented pinnipeds]. V4r4 covers the period 1992-01-01T12:00:00 to 2018-01-01T00:00:00.time_coverage_duration :P1Mtime_coverage_end :2015-02-01T00:00:00time_coverage_resolution :P1Mtime_coverage_start :2015-01-01T00:00:00title :ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4)uuid :088d03b8-4158-11eb-876b-0cc47a3f47f1"
  },
  {
    "objectID": "tutorials/Additional_Resources__Data_Access__Direct_S3_Access__PODAAC_ECCO_SSH.html#multi-file-in-region-direct-s3-access-of-netcdf-files",
    "href": "tutorials/Additional_Resources__Data_Access__Direct_S3_Access__PODAAC_ECCO_SSH.html#multi-file-in-region-direct-s3-access-of-netcdf-files",
    "title": "Direct S3 Data Access - Rough PODAAC ECCO SSH Example",
    "section": "Multi-file in-region direct S3 access of netcdf files",
    "text": "Multi-file in-region direct S3 access of netcdf files\n\nssh_https_urls = [x.get_assets()['data'].href for x in items]\nssh_https_urls\n\n['https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2014-12_ECCO_V4r4_latlon_0p50deg.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-01_ECCO_V4r4_latlon_0p50deg.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-02_ECCO_V4r4_latlon_0p50deg.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-03_ECCO_V4r4_latlon_0p50deg.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-04_ECCO_V4r4_latlon_0p50deg.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-05_ECCO_V4r4_latlon_0p50deg.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-06_ECCO_V4r4_latlon_0p50deg.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-07_ECCO_V4r4_latlon_0p50deg.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-08_ECCO_V4r4_latlon_0p50deg.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-09_ECCO_V4r4_latlon_0p50deg.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-10_ECCO_V4r4_latlon_0p50deg.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-11_ECCO_V4r4_latlon_0p50deg.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-12_ECCO_V4r4_latlon_0p50deg.nc']\n\n\n\nssh_s3_urls = [x.replace('https://archive.podaac.earthdata.nasa.gov/', 's3://') for x in ssh_https_urls]\nssh_s3_urls\n\n['s3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2014-12_ECCO_V4r4_latlon_0p50deg.nc',\n 's3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-01_ECCO_V4r4_latlon_0p50deg.nc',\n 's3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-02_ECCO_V4r4_latlon_0p50deg.nc',\n 's3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-03_ECCO_V4r4_latlon_0p50deg.nc',\n 's3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-04_ECCO_V4r4_latlon_0p50deg.nc',\n 's3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-05_ECCO_V4r4_latlon_0p50deg.nc',\n 's3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-06_ECCO_V4r4_latlon_0p50deg.nc',\n 's3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-07_ECCO_V4r4_latlon_0p50deg.nc',\n 's3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-08_ECCO_V4r4_latlon_0p50deg.nc',\n 's3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-09_ECCO_V4r4_latlon_0p50deg.nc',\n 's3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-10_ECCO_V4r4_latlon_0p50deg.nc',\n 's3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-11_ECCO_V4r4_latlon_0p50deg.nc',\n 's3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-12_ECCO_V4r4_latlon_0p50deg.nc']\n\n\n\n# Iterate through remote_files to create a fileset\nfileset = [fs_s3.open(file) for file in ssh_s3_urls]\n\n\n# This works\nssh_xr_ts = xr.open_mfdataset(fileset, engine='h5netcdf')\n\n\nssh_xr_ts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:         (time: 13, latitude: 360, longitude: 720, nv: 2)\nCoordinates:\n  * time            (time) datetime64[ns] 2014-12-16T12:00:00 ... 2015-12-16T...\n  * latitude        (latitude) float32 -89.75 -89.25 -88.75 ... 89.25 89.75\n  * longitude       (longitude) float32 -179.8 -179.2 -178.8 ... 179.2 179.8\n    time_bnds       (time, nv) datetime64[ns] dask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;\n    latitude_bnds   (latitude, nv) float32 dask.array&lt;chunksize=(360, 2), meta=np.ndarray&gt;\n    longitude_bnds  (longitude, nv) float32 dask.array&lt;chunksize=(720, 2), meta=np.ndarray&gt;\nDimensions without coordinates: nv\nData variables:\n    SSH             (time, latitude, longitude) float32 dask.array&lt;chunksize=(1, 360, 720), meta=np.ndarray&gt;\n    SSHIBC          (time, latitude, longitude) float32 dask.array&lt;chunksize=(1, 360, 720), meta=np.ndarray&gt;\n    SSHNOIBC        (time, latitude, longitude) float32 dask.array&lt;chunksize=(1, 360, 720), meta=np.ndarray&gt;\nAttributes: (12/57)\n    acknowledgement:              This research was carried out by the Jet Pr...\n    author:                       Ian Fenty and Ou Wang\n    cdm_data_type:                Grid\n    comment:                      Fields provided on a regular lat-lon grid. ...\n    Conventions:                  CF-1.8, ACDD-1.3\n    coordinates_comment:          Note: the global 'coordinates' attribute de...\n    ...                           ...\n    time_coverage_duration:       P1M\n    time_coverage_end:            2015-01-01T00:00:00\n    time_coverage_resolution:     P1M\n    time_coverage_start:          2014-12-01T00:00:00\n    title:                        ECCO Sea Surface Height - Monthly Mean 0.5 ...\n    uuid:                         08a2fc68-4158-11eb-b498-0cc47a3f6943xarray.DatasetDimensions:time: 13latitude: 360longitude: 720nv: 2Coordinates: (6)time(time)datetime64[ns]2014-12-16T12:00:00 ... 2015-12-...axis :Tbounds :time_bndscoverage_content_type :coordinatelong_name :center time of averaging periodstandard_name :timearray(['2014-12-16T12:00:00.000000000', '2015-01-16T12:00:00.000000000',\n       '2015-02-15T00:00:00.000000000', '2015-03-16T12:00:00.000000000',\n       '2015-04-16T00:00:00.000000000', '2015-05-16T12:00:00.000000000',\n       '2015-06-16T00:00:00.000000000', '2015-07-16T12:00:00.000000000',\n       '2015-08-16T12:00:00.000000000', '2015-09-16T00:00:00.000000000',\n       '2015-10-16T12:00:00.000000000', '2015-11-16T00:00:00.000000000',\n       '2015-12-16T12:00:00.000000000'], dtype='datetime64[ns]')latitude(latitude)float32-89.75 -89.25 ... 89.25 89.75axis :Ybounds :latitude_bndscomment :uniform grid spacing from -89.75 to 89.75 by 0.5coverage_content_type :coordinatelong_name :latitude at grid cell centerstandard_name :latitudeunits :degrees_northarray([-89.75, -89.25, -88.75, ...,  88.75,  89.25,  89.75], dtype=float32)longitude(longitude)float32-179.8 -179.2 ... 179.2 179.8axis :Xbounds :longitude_bndscomment :uniform grid spacing from -179.75 to 179.75 by 0.5coverage_content_type :coordinatelong_name :longitude at grid cell centerstandard_name :longitudeunits :degrees_eastarray([-179.75, -179.25, -178.75, ...,  178.75,  179.25,  179.75],\n      dtype=float32)time_bnds(time, nv)datetime64[ns]dask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;comment :Start and end times of averaging period.coverage_content_type :coordinatelong_name :time bounds of averaging period\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n208 B\n16 B\n\n\nShape\n(13, 2)\n(1, 2)\n\n\nCount\n39 Tasks\n13 Chunks\n\n\nType\ndatetime64[ns]\nnumpy.ndarray\n\n\n\n\n                     2 13\n\n\n\n\nlatitude_bnds(latitude, nv)float32dask.array&lt;chunksize=(360, 2), meta=np.ndarray&gt;coverage_content_type :coordinatelong_name :latitude bounds grid cells\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.81 kiB\n2.81 kiB\n\n\nShape\n(360, 2)\n(360, 2)\n\n\nCount\n60 Tasks\n1 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n         2 360\n\n\n\n\nlongitude_bnds(longitude, nv)float32dask.array&lt;chunksize=(720, 2), meta=np.ndarray&gt;coverage_content_type :coordinatelong_name :longitude bounds grid cells\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n5.62 kiB\n5.62 kiB\n\n\nShape\n(720, 2)\n(720, 2)\n\n\nCount\n60 Tasks\n1 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n         2 720\n\n\n\n\nData variables: (3)SSH(time, latitude, longitude)float32dask.array&lt;chunksize=(1, 360, 720), meta=np.ndarray&gt;coverage_content_type :modelResultlong_name :Dynamic sea surface height anomalystandard_name :sea_surface_height_above_geoidunits :mcomment :Dynamic sea surface height anomaly above the geoid, suitable for comparisons with altimetry sea surface height data products that apply the inverse barometer (IB) correction. Note: SSH is calculated by correcting model sea level anomaly ETAN for three effects: a) global mean steric sea level changes related to density changes in the Boussinesq volume-conserving model (Greatbatch correction, see sterGloH), b) the inverted barometer (IB) effect (see SSHIBC) and c) sea level displacement due to sea-ice and snow pressure loading (see sIceLoad). SSH can be compared with the similarly-named SSH variable in previous ECCO products that did not include atmospheric pressure loading (e.g., Version 4 Release 3). Use SSHNOIBC for comparisons with altimetry data products that do NOT apply the IB correction.valid_min :[-1.88057721]valid_max :[1.42077196]\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n12.85 MiB\n0.99 MiB\n\n\nShape\n(13, 360, 720)\n(1, 360, 720)\n\n\nCount\n39 Tasks\n13 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n                                                 720 360 13\n\n\n\n\nSSHIBC(time, latitude, longitude)float32dask.array&lt;chunksize=(1, 360, 720), meta=np.ndarray&gt;coverage_content_type :modelResultlong_name :The inverted barometer (IB) correction to sea surface height due to atmospheric pressure loadingunits :mcomment :Not an SSH itself, but a correction to model sea level anomaly (ETAN) required to account for the static part of sea surface displacement by atmosphere pressure loading: SSH = SSHNOIBC - SSHIBC. Note: Use SSH for model-data comparisons with altimetry data products that DO apply the IB correction and SSHNOIBC for comparisons with altimetry data products that do NOT apply the IB correction.valid_min :[-0.3014482]valid_max :[0.52456337]\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n12.85 MiB\n0.99 MiB\n\n\nShape\n(13, 360, 720)\n(1, 360, 720)\n\n\nCount\n39 Tasks\n13 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n                                                 720 360 13\n\n\n\n\nSSHNOIBC(time, latitude, longitude)float32dask.array&lt;chunksize=(1, 360, 720), meta=np.ndarray&gt;coverage_content_type :modelResultlong_name :Sea surface height anomaly without the inverted barometer (IB) correctionunits :mcomment :Sea surface height anomaly above the geoid without the inverse barometer (IB) correction, suitable for comparisons with altimetry sea surface height data products that do NOT apply the inverse barometer (IB) correction. Note: SSHNOIBC is calculated by correcting model sea level anomaly ETAN for two effects: a) global mean steric sea level changes related to density changes in the Boussinesq volume-conserving model (Greatbatch correction, see sterGloH), b) sea level displacement due to sea-ice and snow pressure loading (see sIceLoad). In ECCO Version 4 Release 4 the model is forced with atmospheric pressure loading. SSHNOIBC does not correct for the static part of the effect of atmosphere pressure loading on sea surface height (the so-called inverse barometer (IB) correction). Use SSH for comparisons with altimetry data products that DO apply the IB correction.valid_min :[-1.66542721]valid_max :[1.4550364]\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n12.85 MiB\n0.99 MiB\n\n\nShape\n(13, 360, 720)\n(1, 360, 720)\n\n\nCount\n39 Tasks\n13 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n                                                 720 360 13\n\n\n\n\nAttributes: (57)acknowledgement :This research was carried out by the Jet Propulsion Laboratory, managed by the California Institute of Technology under a contract with the National Aeronautics and Space Administration.author :Ian Fenty and Ou Wangcdm_data_type :Gridcomment :Fields provided on a regular lat-lon grid. They have been mapped to the regular lat-lon grid from the original ECCO lat-lon-cap 90 (llc90) native model grid. SSH (dynamic sea surface height) = SSHNOIBC (dynamic sea surface without the inverse barometer correction) - SSHIBC (inverse barometer correction). The inverted barometer correction accounts for variations in sea surface height due to atmospheric pressure variations.Conventions :CF-1.8, ACDD-1.3coordinates_comment :Note: the global 'coordinates' attribute describes auxillary coordinates.creator_email :ecco-group@mit.educreator_institution :NASA Jet Propulsion Laboratory (JPL)creator_name :ECCO Consortiumcreator_type :groupcreator_url :https://ecco-group.orgdate_created :2020-12-18T09:39:51date_issued :2020-12-18T09:39:51date_metadata_modified :2021-03-15T22:07:49date_modified :2021-03-15T22:07:49geospatial_bounds_crs :EPSG:4326geospatial_lat_max :[90.]geospatial_lat_min :[-90.]geospatial_lat_resolution :[0.5]geospatial_lat_units :degrees_northgeospatial_lon_max :[180.]geospatial_lon_min :[-180.]geospatial_lon_resolution :[0.5]geospatial_lon_units :degrees_easthistory :Inaugural release of an ECCO Central Estimate solution to PO.DAACid :10.5067/ECG5M-SSH44institution :NASA Jet Propulsion Laboratory (JPL)instrument_vocabulary :GCMD instrument keywordskeywords :EARTH SCIENCE &gt; OCEANS &gt; SEA SURFACE TOPOGRAPHY &gt; SEA SURFACE HEIGHT, EARTH SCIENCE SERVICES &gt; MODELS &gt; EARTH SCIENCE REANALYSES/ASSIMILATION MODELSkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :Public Domainmetadata_link :https://cmr.earthdata.nasa.gov/search/collections.umm_json?ShortName=ECCO_L4_SSH_05DEG_MONTHLY_V4R4naming_authority :gov.nasa.jplplatform :ERS-1/2, TOPEX/Poseidon, Geosat Follow-On (GFO), ENVISAT, Jason-1, Jason-2, CryoSat-2, SARAL/AltiKa, Jason-3, AVHRR, Aquarius, SSM/I, SSMIS, GRACE, DTU17MDT, Argo, WOCE, GO-SHIP, MEOP, Ice Tethered Profilers (ITP)platform_vocabulary :GCMD platform keywordsprocessing_level :L4product_name :SEA_SURFACE_HEIGHT_mon_mean_2014-12_ECCO_V4r4_latlon_0p50deg.ncproduct_time_coverage_end :2018-01-01T00:00:00product_time_coverage_start :1992-01-01T12:00:00product_version :Version 4, Release 4program :NASA Physical Oceanography, Cryosphere, Modeling, Analysis, and Prediction (MAP)project :Estimating the Circulation and Climate of the Ocean (ECCO)publisher_email :podaac@podaac.jpl.nasa.govpublisher_institution :PO.DAACpublisher_name :Physical Oceanography Distributed Active Archive Center (PO.DAAC)publisher_type :institutionpublisher_url :https://podaac.jpl.nasa.govreferences :ECCO Consortium, Fukumori, I., Wang, O., Fenty, I., Forget, G., Heimbach, P., & Ponte, R. M. 2020. Synopsis of the ECCO Central Production Global Ocean and Sea-Ice State Estimate (Version 4 Release 4). doi:10.5281/zenodo.3765928source :The ECCO V4r4 state estimate was produced by fitting a free-running solution of the MITgcm (checkpoint 66g) to satellite and in situ observational data in a least squares sense using the adjoint methodstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsummary :This dataset provides monthly-averaged dynamic sea surface height interpolated to a regular 0.5-degree grid from the ECCO Version 4 Release 4 (V4r4) ocean and sea-ice state estimate. Estimating the Circulation and Climate of the Ocean (ECCO) state estimates are dynamically and kinematically-consistent reconstructions of the three-dimensional, time-evolving ocean, sea-ice, and surface atmospheric states. ECCO V4r4 is a free-running solution of a global, nominally 1-degree configuration of the MIT general circulation model (MITgcm) that has been fit to observations in a least-squares sense. Observational data constraints used in V4r4 include sea surface height (SSH) from satellite altimeters [ERS-1/2, TOPEX/Poseidon, GFO, ENVISAT, Jason-1,2,3, CryoSat-2, and SARAL/AltiKa]; sea surface temperature (SST) from satellite radiometers [AVHRR], sea surface salinity (SSS) from the Aquarius satellite radiometer/scatterometer, ocean bottom pressure (OBP) from the GRACE satellite gravimeter; sea-ice concentration from satellite radiometers [SSM/I and SSMIS], and in-situ ocean temperature and salinity measured with conductivity-temperature-depth (CTD) sensors and expendable bathythermographs (XBTs) from several programs [e.g., WOCE, GO-SHIP, Argo, and others] and platforms [e.g., research vessels, gliders, moorings, ice-tethered profilers, and instrumented pinnipeds]. V4r4 covers the period 1992-01-01T12:00:00 to 2018-01-01T00:00:00.time_coverage_duration :P1Mtime_coverage_end :2015-01-01T00:00:00time_coverage_resolution :P1Mtime_coverage_start :2014-12-01T00:00:00title :ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4)uuid :08a2fc68-4158-11eb-b498-0cc47a3f6943\n\n\n\nssh_xr_ts.SSH.hvplot.image()"
  },
  {
    "objectID": "tutorials/Additional_Resources__Data_Access__Direct_S3_Access__PODAAC_ECCO_SSH.html#single-file-access-via-rioxarray---not-pretty",
    "href": "tutorials/Additional_Resources__Data_Access__Direct_S3_Access__PODAAC_ECCO_SSH.html#single-file-access-via-rioxarray---not-pretty",
    "title": "Direct S3 Data Access - Rough PODAAC ECCO SSH Example",
    "section": "Single file access via rioxarray - not pretty",
    "text": "Single file access via rioxarray - not pretty\n\nssh = rioxarray.open_rasterio(ssh_s3)\n\nJust a warning, but a very obnoxious one.\nReturns a list of xarray data array???\n\nssh[0]\n\n\nssh[0].SSH\n\n\nssh[0].SSH.where(ssh[0].SSH &lt; 9).hvplot.image(x='x', y='y')"
  },
  {
    "objectID": "tutorials/06_S6_OPeNDAP_Access_Gridding.html",
    "href": "tutorials/06_S6_OPeNDAP_Access_Gridding.html",
    "title": "06. Sentinel-6 MF L2 Altimetry Data Access (OPeNDAP) & Gridding",
    "section": "",
    "text": "In this tutorial you will learn…\n\nabout level 2 radar altimetry data from the Sentinel-6 Michael Freilich mission;\nhow to efficiently download variable subsets using OPeNDAP;\nhow to grid the along-track altimetry observations produced by S6 at level 2.;\n\n\nAbout Ocean Surface Topography (OST)\n\nThe primary contribution of satellite altimetry to satellite oceanography has been to:\n\nImprove the knowledge of ocean tides and develop global tide models.\nMonitor the variation of global mean sea level and its relationship to changes in ocean mass and heat content.\nMap the general circulation variability of the ocean, including the ocean mesoscale, over decades and in near real-time using multi-satellite altimetric sampling.\n\n\n\nAbout Sentinel-6 MF\nhttps://search.earthdata.nasa.gov/search?fpj=Sentinel-6\n\nhttps://podaac.jpl.nasa.gov/Sentinel-6\n\nMission Characteristics\nSemi-major axis: 7,714.43 km\nEccentricity: 0.000095\nInclination: 66.04°\nArgument of periapsis: 90.0°\nMean anomaly: 253.13°\nReference altitude: 1,336 km\nNodal period: 6,745.72 sec\nRepeat period: 9.9156 days\nNumber of revolutions within a cycle: 127\nNumber of passes within a cycle: 254\nEquatorial cross track separation: 315 km\nGround track control band: +1 km\nAcute angle at Equator crossings: 39.5°\nGround track speed: 5.8 km/s\n\n\n\nRequirements\nThis workflow was developed using Python 3.9 (and tested against versions 3.7, 3.8).\n\nimport os\nimport tqdm\nimport numpy as np\nimport xarray as xr\nimport matplotlib.pyplot as plt\nfrom concurrent.futures import ThreadPoolExecutor\nfrom pyresample.kd_tree import resample_gauss\nimport pyresample as pr\n\n\n\nDataset\nhttps://podaac.jpl.nasa.gov/dataset/JASON_CS_S6A_L2_ALT_LR_RED_OST_NRT_F\nThis example operates on Level 2 Low Resolution Altimetry from Sentinel-6 Michael Freilich (the Near Real Time Reduced distribution). It is most easily identified by its collection ShortName, given below with the more cryptic concept-id, it’s unique identifier in the CMR.\n\nShortName = 'JASON_CS_S6A_L2_ALT_LR_RED_OST_NRT_F'\nconcept_id = 'C1968980576-POCLOUD'\n\n\n\ncycle = 25\n\nurl = f\"https://cmr.earthdata.nasa.gov/search/granules.csv?ShortName={ShortName}&cycle={cycle}&page_size=200\"\n\nprint(url)\n\nhttps://cmr.earthdata.nasa.gov/search/granules.csv?ShortName=JASON_CS_S6A_L2_ALT_LR_RED_OST_NRT_F&cycle=25&page_size=200\n\n\ncurl --silent --output\ncat\ntail --lines\ncut  --delimiter --fields\n\n!curl --silent --output \"results.csv\" \"$url\"\n\nfiles = !cat results.csv | tail --lines=+2 | cut --delimiter=',' --fields=5 | cut --delimiter='/' --fields=6\n\nprint(files.s.replace(\" \", \"\\n\"))\n\nS6A_P4_2__LR_RED__NR_025_001_20210713T162644_20210713T182234_F02.nc\nS6A_P4_2__LR_RED__NR_025_003_20210713T182234_20210713T201839_F02.nc\nS6A_P4_2__LR_RED__NR_025_006_20210713T201839_20210713T215450_F02.nc\nS6A_P4_2__LR_RED__NR_025_007_20210713T215450_20210713T234732_F02.nc\nS6A_P4_2__LR_RED__NR_025_009_20210713T234732_20210714T014224_F02.nc\nS6A_P4_2__LR_RED__NR_025_011_20210714T014224_20210714T033812_F02.nc\nS6A_P4_2__LR_RED__NR_025_013_20210714T033812_20210714T053356_F02.nc\nS6A_P4_2__LR_RED__NR_025_015_20210714T053357_20210714T072934_F02.nc\nS6A_P4_2__LR_RED__NR_025_017_20210714T072934_20210714T090919_F02.nc\nS6A_P4_2__LR_RED__NR_025_019_20210714T090919_20210714T110146_F02.nc\nS6A_P4_2__LR_RED__NR_025_021_20210714T110146_20210714T125702_F02.nc\nS6A_P4_2__LR_RED__NR_025_023_20210714T125702_20210714T145316_F02.nc\nS6A_P4_2__LR_RED__NR_025_025_20210714T145317_20210714T164922_F02.nc\nS6A_P4_2__LR_RED__NR_025_027_20210714T164922_20210714T184510_F02.nc\nS6A_P4_2__LR_RED__NR_025_029_20210714T184510_20210714T204143_F02.nc\nS6A_P4_2__LR_RED__NR_025_032_20210714T204143_20210714T221611_F02.nc\nS6A_P4_2__LR_RED__NR_025_033_20210714T221611_20210715T000941_F02.nc\nS6A_P4_2__LR_RED__NR_025_035_20210715T000941_20210715T020456_F02.nc\nS6A_P4_2__LR_RED__NR_025_037_20210715T020456_20210715T040047_F02.nc\nS6A_P4_2__LR_RED__NR_025_039_20210715T040047_20210715T055630_F02.nc\nS6A_P4_2__LR_RED__NR_025_041_20210715T055630_20210715T075208_F02.nc\nS6A_P4_2__LR_RED__NR_025_043_20210715T075208_20210715T093037_F02.nc\nS6A_P4_2__LR_RED__NR_025_045_20210715T093037_20210715T112356_F02.nc\nS6A_P4_2__LR_RED__NR_025_047_20210715T112356_20210715T131944_F02.nc\nS6A_P4_2__LR_RED__NR_025_049_20210715T131944_20210715T151600_F02.nc\nS6A_P4_2__LR_RED__NR_025_051_20210715T151602_20210715T165851_F02.nc\nS6A_P4_2__LR_RED__NR_025_053_20210715T171228_20210715T190748_F02.nc\nS6A_P4_2__LR_RED__NR_025_056_20210715T190748_20210715T204627_F02.nc\nS6A_P4_2__LR_RED__NR_025_057_20210715T204627_20210715T223758_F02.nc\nS6A_P4_2__LR_RED__NR_025_059_20210715T223758_20210716T003159_F02.nc\nS6A_P4_2__LR_RED__NR_025_061_20210716T003159_20210716T022732_F02.nc\nS6A_P4_2__LR_RED__NR_025_063_20210716T022732_20210716T042333_F02.nc\nS6A_P4_2__LR_RED__NR_025_065_20210716T042333_20210716T061901_F02.nc\nS6A_P4_2__LR_RED__NR_025_067_20210716T061901_20210716T081446_F02.nc\nS6A_P4_2__LR_RED__NR_025_070_20210716T081446_20210716T095203_F02.nc\nS6A_P4_2__LR_RED__NR_025_071_20210716T095203_20210716T114624_F02.nc\nS6A_P4_2__LR_RED__NR_025_073_20210716T114624_20210716T134228_F02.nc\nS6A_P4_2__LR_RED__NR_025_075_20210716T134228_20210716T153841_F02.nc\nS6A_P4_2__LR_RED__NR_025_077_20210716T153841_20210716T173433_F02.nc\nS6A_P4_2__LR_RED__NR_025_079_20210716T173433_20210716T193033_F02.nc\nS6A_P4_2__LR_RED__NR_025_082_20210716T193033_20210716T210718_F02.nc\nS6A_P4_2__LR_RED__NR_025_083_20210716T210718_20210716T225942_F02.nc\nS6A_P4_2__LR_RED__NR_025_085_20210716T225942_20210717T005425_F02.nc\nS6A_P4_2__LR_RED__NR_025_087_20210717T005425_20210717T025012_F02.nc\nS6A_P4_2__LR_RED__NR_025_089_20210717T025012_20210717T044557_F02.nc\nS6A_P4_2__LR_RED__NR_025_091_20210717T044557_20210717T064133_F02.nc\nS6A_P4_2__LR_RED__NR_025_093_20210717T064133_20210717T082134_F02.nc\nS6A_P4_2__LR_RED__NR_025_095_20210717T082134_20210717T101352_F02.nc\nS6A_P4_2__LR_RED__NR_025_097_20210717T101352_20210717T120859_F02.nc\nS6A_P4_2__LR_RED__NR_025_099_20210717T120859_20210717T140513_F02.nc\nS6A_P4_2__LR_RED__NR_025_101_20210717T140513_20210717T160120_F02.nc\nS6A_P4_2__LR_RED__NR_025_103_20210717T160120_20210717T175708_F02.nc\nS6A_P4_2__LR_RED__NR_025_105_20210717T175708_20210717T195329_F02.nc\nS6A_P4_2__LR_RED__NR_025_108_20210717T195329_20210717T212832_F02.nc\nS6A_P4_2__LR_RED__NR_025_109_20210717T212832_20210717T232147_F02.nc\nS6A_P4_2__LR_RED__NR_025_111_20210717T232147_20210718T011655_F02.nc\nS6A_P4_2__LR_RED__NR_025_113_20210718T011655_20210718T031245_F02.nc\nS6A_P4_2__LR_RED__NR_025_115_20210718T031245_20210718T050829_F02.nc\nS6A_P4_2__LR_RED__NR_025_117_20210718T050829_20210718T070406_F02.nc\nS6A_P4_2__LR_RED__NR_025_119_20210718T070406_20210718T084306_F02.nc\nS6A_P4_2__LR_RED__NR_025_121_20210718T084306_20210718T103559_F02.nc\nS6A_P4_2__LR_RED__NR_025_123_20210718T103559_20210718T123140_F02.nc\nS6A_P4_2__LR_RED__NR_025_125_20210718T123140_20210718T142756_F02.nc\nS6A_P4_2__LR_RED__NR_025_127_20210718T142756_20210718T162356_F02.nc\nS6A_P4_2__LR_RED__NR_025_129_20210718T162356_20210718T181945_F02.nc\nS6A_P4_2__LR_RED__NR_025_132_20210718T181945_20210718T195907_F02.nc\nS6A_P4_2__LR_RED__NR_025_133_20210718T195907_20210718T215014_F02.nc\nS6A_P4_2__LR_RED__NR_025_135_20210718T215014_20210718T234402_F02.nc\nS6A_P4_2__LR_RED__NR_025_137_20210718T234402_20210719T013937_F02.nc\nS6A_P4_2__LR_RED__NR_025_139_20210719T013937_20210719T033531_F02.nc\nS6A_P4_2__LR_RED__NR_025_141_20210719T033531_20210719T053101_F02.nc\nS6A_P4_2__LR_RED__NR_025_143_20210719T053101_20210719T072643_F02.nc\nS6A_P4_2__LR_RED__NR_025_146_20210719T072643_20210719T090425_F02.nc\nS6A_P4_2__LR_RED__NR_025_147_20210719T090425_20210719T105824_F02.nc\nS6A_P4_2__LR_RED__NR_025_149_20210719T105824_20210719T125424_F02.nc\nS6A_P4_2__LR_RED__NR_025_151_20210719T125424_20210719T145038_F02.nc\nS6A_P4_2__LR_RED__NR_025_153_20210719T145541_20210719T164632_F02.nc\nS6A_P4_2__LR_RED__NR_025_155_20210719T164632_20210719T184227_F02.nc\nS6A_P4_2__LR_RED__NR_025_158_20210719T184227_20210719T201949_F02.nc\nS6A_P4_2__LR_RED__NR_025_159_20210719T201949_20210719T221154_F02.nc\nS6A_P4_2__LR_RED__NR_025_161_20210719T221154_20210720T000626_F02.nc\nS6A_P4_2__LR_RED__NR_025_163_20210720T000626_20210720T020212_F02.nc\nS6A_P4_2__LR_RED__NR_025_165_20210720T020212_20210720T035756_F02.nc\nS6A_P4_2__LR_RED__NR_025_167_20210720T035756_20210720T055333_F02.nc\nS6A_P4_2__LR_RED__NR_025_169_20210720T055333_20210720T073350_F02.nc\nS6A_P4_2__LR_RED__NR_025_171_20210720T073350_20210720T092602_F02.nc\nS6A_P4_2__LR_RED__NR_025_173_20210720T092602_20210720T112057_F02.nc\nS6A_P4_2__LR_RED__NR_025_175_20210720T112057_20210720T131708_F02.nc\nS6A_P4_2__LR_RED__NR_025_177_20210720T131708_20210720T151317_F02.nc\nS6A_P4_2__LR_RED__NR_025_184_20210720T190549_20210720T204056_F02.nc\nS6A_P4_2__LR_RED__NR_025_185_20210720T204056_20210720T223355_F02.nc\nS6A_P4_2__LR_RED__NR_025_187_20210720T223355_20210721T002855_F02.nc\nS6A_P4_2__LR_RED__NR_025_189_20210721T002855_20210721T022443_F02.nc\nS6A_P4_2__LR_RED__NR_025_191_20210721T022443_20210721T042029_F02.nc\nS6A_P4_2__LR_RED__NR_025_193_20210721T042029_20210721T061605_F02.nc\nS6A_P4_2__LR_RED__NR_025_195_20210721T061605_20210721T075531_F02.nc\nS6A_P4_2__LR_RED__NR_025_197_20210721T075531_20210721T094805_F02.nc\nS6A_P4_2__LR_RED__NR_025_199_20210721T094805_20210721T114336_F02.nc\nS6A_P4_2__LR_RED__NR_025_201_20210721T114336_20210721T133952_F02.nc\nS6A_P4_2__LR_RED__NR_025_203_20210721T133952_20210721T153555_F02.nc\nS6A_P4_2__LR_RED__NR_025_205_20210721T153555_20210721T173143_F02.nc\nS6A_P4_2__LR_RED__NR_025_207_20210721T173143_20210721T191151_F02.nc\nS6A_P4_2__LR_RED__NR_025_209_20210721T191151_20210721T210223_F02.nc\nS6A_P4_2__LR_RED__NR_025_211_20210721T210223_20210721T225607_F02.nc\nS6A_P4_2__LR_RED__NR_025_213_20210721T225607_20210722T005131_F02.nc\nS6A_P4_2__LR_RED__NR_025_215_20210722T005131_20210722T024724_F02.nc\nS6A_P4_2__LR_RED__NR_025_217_20210722T024724_20210722T044301_F02.nc\nS6A_P4_2__LR_RED__NR_025_219_20210722T044301_20210722T063841_F02.nc\nS6A_P4_2__LR_RED__NR_025_221_20210722T063841_20210722T081646_F02.nc\nS6A_P4_2__LR_RED__NR_025_223_20210722T081646_20210722T101025_F02.nc\nS6A_P4_2__LR_RED__NR_025_225_20210722T101025_20210722T120619_F02.nc\nS6A_P4_2__LR_RED__NR_025_227_20210722T120619_20210722T140235_F02.nc\nS6A_P4_2__LR_RED__NR_025_229_20210722T140235_20210722T155831_F02.nc\nS6A_P4_2__LR_RED__NR_025_231_20210722T155831_20210722T175423_F02.nc\nS6A_P4_2__LR_RED__NR_025_234_20210722T175423_20210722T193222_F02.nc\nS6A_P4_2__LR_RED__NR_025_235_20210722T193222_20210722T212406_F02.nc\nS6A_P4_2__LR_RED__NR_025_237_20210722T212406_20210722T231828_F02.nc\nS6A_P4_2__LR_RED__NR_025_239_20210722T231828_20210723T011405_F02.nc\nS6A_P4_2__LR_RED__NR_025_241_20210723T011405_20210723T030955_F02.nc\nS6A_P4_2__LR_RED__NR_025_243_20210723T030955_20210723T050533_F02.nc\nS6A_P4_2__LR_RED__NR_025_245_20210723T050533_20210723T064603_F02.nc\nS6A_P4_2__LR_RED__NR_025_247_20210723T064603_20210723T083817_F02.nc\nS6A_P4_2__LR_RED__NR_025_249_20210723T083817_20210723T103256_F02.nc\nS6A_P4_2__LR_RED__NR_025_251_20210723T103256_20210723T122904_F02.nc\nS6A_P4_2__LR_RED__NR_025_253_20210723T122904_20210723T142514_F02.nc\n\n\n\nOPeNDAP\nhttps://opendap.github.io/documentation/UserGuideComprehensive.html#Constraint_Expressions (Hyrax/OPeNDAP docs)\n\ntmp = files.l[0].split('.')[0]\n\nprint(f\"https://opendap.earthdata.nasa.gov/collections/{concept_id}/granules/{tmp}.html\")\n\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_001_20210713T162644_20210713T182234_F02.html\n\n\n\nvariables = ['data_01_time',\n             'data_01_longitude',\n             'data_01_latitude',\n             'data_01_ku_ssha']\n\n\nv = \",\".join(variables)\n\nurls = []\nfor f in files:\n    urls.append(f\"https://opendap.earthdata.nasa.gov/collections/{concept_id}/granules/{f}4?{v}\")\n\nprint(urls[0])\n\nhttps://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_001_20210713T162644_20210713T182234_F02.nc4?data_01_time,data_01_longitude,data_01_latitude,data_01_ku_ssha\n\n\n\n\n\nDownload Subsets\nThese functions download one granule from the remote source to a local target, and will reliably manage simultaneous streaming downloads divided between multiple threads.\n\nwith python3:\nimport requests\n\ndef download(source: str, target: str):\n    with requests.get(source, stream=True) as remote, open(target, 'wb') as local:\n        if remote.status_code // 100 == 2: \n            for chunk in remote.iter_content(chunk_size=1024):\n                if chunk:\n                    local.write(chunk)\n\n\nwith wget:\n\ndef download(source: str):\n    \n    target = os.path.basename(source.split(\"?\")[0])\n    \n    if not os.path.isfile(target):\n        !wget --quiet --continue --output-document $target $source\n    \n    return target\n\n\nn_workers = 12\n\nwith ThreadPoolExecutor(max_workers=n_workers) as pool:\n\n    workers = pool.map(download, urls)\n    \n    files = list(tqdm.tqdm(workers, total=len(urls)))\n\n100%|██████████| 125/125 [01:01&lt;00:00,  2.05it/s]\n\n\nhttps://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor\nThe source files range from 2.5MB to 3.0MB. These OPeNDAP subsets are ~100KB apiece. (anecdote: it took less than 10 minutes to download subsets for &gt;1700 granules/files when I ran this routine for all cycles going back to 2021-06-22.)\n\n!du -sh .\n\n17M .\n\n\nhttps://www.gnu.org/software/coreutils/manual/html_node/du-invocation.html\n\n\nAggregate cycle\nSort the list of local subsets to ensure they concatenate in proper order. Call open_mfdataset on the list to open all the subsets in memory as one dataset in xarray.\n\nds = xr.open_mfdataset(sorted(files))\n\nprint(ds)\n\n&lt;xarray.Dataset&gt;\nDimensions:            (data_01_time: 827001)\nCoordinates:\n  * data_01_time       (data_01_time) datetime64[ns] 2021-07-13T16:26:45 ... ...\nData variables:\n    data_01_longitude  (data_01_time) float64 dask.array&lt;chunksize=(6950,), meta=np.ndarray&gt;\n    data_01_latitude   (data_01_time) float64 dask.array&lt;chunksize=(6950,), meta=np.ndarray&gt;\n    data_01_ku_ssha    (data_01_time) float64 dask.array&lt;chunksize=(6950,), meta=np.ndarray&gt;\nAttributes: (12/63)\n    Convention:                             CF-1.7\n    institution:                            EUMETSAT\n    references:                             Sentinel-6_Jason-CS ALT Generic P...\n    contact:                                ops@eumetsat.int\n    radiometer_sensor_name:                 AMR-C\n    doris_sensor_name:                      DORIS\n    ...                                     ...\n    xref_solid_earth_tide:                  S6__P4_2__SETD_AX_20151008T000000...\n    xref_surface_classification:            S6__P4____SURF_AX_20151008T000000...\n    xref_wind_speed_alt:                    S6A_P4_2__WNDL_AX_20151008T000000...\n    product_name:                           S6A_P4_2__LR______20210713T162644...\n    history:                                2021-07-13 18:38:07 : Creation\\n2...\n    history_json:                           [{\"$schema\":\"https:\\/\\/harmony.ea...\n\n\nhttps://xarray.pydata.org/en/stable/generated/xarray.open_mfdataset.html\nMake a dictionary to rename variables so that the data_01_ prefix is removed from each one.\n\nnew_variable_names = list(map(lambda x: x.split(\"_\")[-1], variables))\n\nmap_variable_names = dict(zip(variables, new_variable_names))\n\nmap_variable_names\n\n{'data_01_time': 'time',\n 'data_01_longitude': 'longitude',\n 'data_01_latitude': 'latitude',\n 'data_01_ku_ssha': 'ssha'}\n\n\nhttps://docs.python.org/3/library/functions.html#map\nhttps://docs.python.org/3/library/functions.html#zip\n\nds = ds.rename(map_variable_names)\n\nprint(list(ds.variables))\n\n['longitude', 'latitude', 'ssha', 'time']\n\n\n\n\nPlot ssha variable\nhttps://xarray.pydata.org/en/stable/generated/xarray.Dataset.rename.html\n\nds.plot.scatter( y=\"latitude\",\n                 x=\"longitude\", \n                 hue=\"ssha\",\n                 s=1,\n                 vmin=-0.4,\n                 vmax=0.4,\n                 levels=9, \n                 cmap=\"jet\",\n                 aspect=2.5,\n                 size=9, )\n\nplt.title(f\"ssha from s6 cycle {cycle}\")\nplt.xlim(  0., 360.)\nplt.ylim(-67.,  67.)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nBorrow 0.5-Degree Grid and Mask from ECCO V4r4\n\nAcknowledgement: This approach using pyresample was shared to me by Ian Fenty, ECCO Lead.\n\nhttps://search.earthdata.nasa.gov/search/granules?p=C2013583732-POCLOUD\nECCO V4r4 products are distributed in two spatial formats. One set of collections provides the ocean state estimates on the native model grid (LLC0090) and the other provides them after interpolating to a regular grid defined in geographic coordinates with horizontal cell size of 0.5-degrees.\nIt’s distributed as its own dataset/collection containing just one file. We can simply download it from the HTTPS download endpoint – the file size is inconsequential. The next cell downloads the file into the data folder from the granule’s https endpoint.\n\necco_url = \"https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ECCO_L4_GEOMETRY_05DEG_V4R4/GRID_GEOMETRY_ECCO_V4r4_latlon_0p50deg.nc\"\n\necco_file = download(ecco_url)\n\necco_grid = xr.open_dataset(ecco_file)\n\nprint(ecco_grid)\n\n&lt;xarray.Dataset&gt;\nDimensions:         (Z: 50, latitude: 360, longitude: 720, nv: 2)\nCoordinates:\n  * Z               (Z) float32 -5.0 -15.0 -25.0 ... -5.461e+03 -5.906e+03\n  * latitude        (latitude) float32 -89.75 -89.25 -88.75 ... 89.25 89.75\n  * longitude       (longitude) float32 -179.8 -179.2 -178.8 ... 179.2 179.8\n    latitude_bnds   (latitude, nv) float32 ...\n    longitude_bnds  (longitude, nv) float32 ...\n    Z_bnds          (Z, nv) float32 ...\nDimensions without coordinates: nv\nData variables:\n    hFacC           (Z, latitude, longitude) float64 ...\n    Depth           (latitude, longitude) float64 ...\n    area            (latitude, longitude) float64 ...\n    drF             (Z) float32 ...\n    maskC           (Z, latitude, longitude) bool ...\nAttributes: (12/57)\n    acknowledgement:                 This research was carried out by the Jet...\n    author:                          Ian Fenty and Ou Wang\n    cdm_data_type:                   Grid\n    comment:                         Fields provided on a regular lat-lon gri...\n    Conventions:                     CF-1.8, ACDD-1.3\n    coordinates_comment:             Note: the global 'coordinates' attribute...\n    ...                              ...\n    references:                      ECCO Consortium, Fukumori, I., Wang, O.,...\n    source:                          The ECCO V4r4 state estimate was produce...\n    standard_name_vocabulary:        NetCDF Climate and Forecast (CF) Metadat...\n    summary:                         This dataset provides geometric paramete...\n    title:                           ECCO Geometry Parameters for the 0.5 deg...\n    uuid:                            b4795c62-86e5-11eb-9c5f-f8f21e2ee3e0\n\n\nhttps://xarray.pydata.org/en/stable/generated/xarray.open_dataset.html\nSelect index 0 on the Z axis/dimension to get the depth layer at ocean surface.\n\necco_grid = ecco_grid.isel(Z=0).copy()\n\nhttps://xarray.pydata.org/en/stable/generated/xarray.DataArray.isel.html\nThe maskC variable contains a boolean mask representing the wet/dry state of the area contained in each cell of the 3d grid defined by Z and latitude and longitude. Here are the variable’s attributes:\n\nprint(ecco_grid.maskC)\n\n&lt;xarray.DataArray 'maskC' (latitude: 360, longitude: 720)&gt;\n[259200 values with dtype=bool]\nCoordinates:\n    Z          float32 -5.0\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              wet/dry boolean mask for grid cell\n    comment:                True for grid cells with nonzero open vertical fr...\n\n\nPlot the land/water mask maskC:\n\necco_grid.maskC.plot()\n\n\n\n\n\n\n\n\nhttps://xarray.pydata.org/en/stable/generated/xarray.DataArray.plot.html\nDefine target grid based on the longitudes and latitudes from the ECCO grid geometry dataset. This time define the grid using two 2-dimensional arrays that give positions of all SSHA values in geographic/longitude-latitude coordinates.\n\necco_lons = ecco_grid.maskC.longitude.values\necco_lats = ecco_grid.maskC.latitude.values\n\necco_lons_2d, ecco_lats_2d = np.meshgrid(ecco_lons, ecco_lats)\n\nprint(ecco_lons_2d.shape, ecco_lats_2d.shape)\n\n(360, 720) (360, 720)\n\n\nCreate the target swath definition from the 2d arrays of lons and lats from ECCO V4r4 0.5-degree grid.\n\ntgt = pr.SwathDefinition(ecco_lons_2d, ecco_lats_2d)\n\npyresample.geometry.SwathDefinition\n\n\nGrid ssha or other variable\nGet one timestamp to represent the midpoint of the 10-day cycle.\n\ntime = np.datetime64(ds['time'].mean().data)\n\nprint(time)\n\n2021-07-18T15:11:35.073955170\n\n\nAccess the target variable, ssha in this case. Make a nan mask from the ssha variable.\n\nnans = ~np.isnan(ds.ssha.values)\n\nssha = ds.ssha.values[nans]\n\nssha.shape\n\n(518164,)\n\n\nCreate the source swath definition from the 1d arrays of lons and lats from the S6 level-2 along-track altimetry time series.\n\nlons = ds.longitude.values[nans]\nlats = ds.latitude.values[nans]\n        \nprint(lons.shape, lats.shape)\n\n(518164,) (518164,)\n\n\n\nlons = (lons + 180) % 360 - 180\n\nsrc = pr.SwathDefinition(lons, lats)\n\npyresample.geometry.SwathDefinition\nResample ssha data using kd-tree gaussian weighting neighbour approach.\n\nresult, stddev, counts = resample_gauss(\n    src,\n    ssha,\n    tgt,\n    radius_of_influence=175000,  \n    sigmas=25000,\n    neighbours=100,\n    fill_value=np.nan,\n    with_uncert=True,\n)\n\nresult.shape\n\n/srv/conda/envs/notebook/lib/python3.9/site-packages/pyresample/kd_tree.py:384: UserWarning: Possible more than 100 neighbours within 175000 m for some data points\n  warnings.warn(('Possible more than %s neighbours '\n\n\n(360, 720)\n\n\npyresample.kd_tree.resample_gauss\n\ndef to_xrda(data):\n    return xr.DataArray(data,\n                        dims=['latitude', 'longitude'], \n                        coords={'time': time, \n                                'latitude': ecco_lats, \n                                'longitude': ecco_lons})\n\nApply the land/water mask in the numpy array created from the ECCO layer in the steps above. Then, convert the masked numpy array to an xarray data array object named gridded. Print its header.\n\ngrid = to_xrda(result)\n\ngrid.sel(latitude=slice(-67.0, 67.0)).plot(vmin=-0.4, vmax=0.4, cmap=\"jet\", figsize=(18, 7))\n\n\n\n\n\n\n\n\n\nstddev = to_xrda(stddev)\n\nstddev.sel(latitude=slice(-67.0, 67.0)).plot(robust=True, cmap=\"jet\", figsize=(18, 7))\n\n\n\n\n\n\n\n\n\ncounts = to_xrda(counts)\n\ncounts.sel(latitude=slice(-67.0, 67.0)).plot(robust=True, cmap=\"jet\", figsize=(18, 7))\n\n\n\n\n\n\n\n\n\n\nExercise\nCalculate area-weighted mean sea level.\n\n\nReferences\nnumpy (https://numpy.org/doc/stable/reference)\n\nnumpy.ndarray.data\n\nnumpy.where\n\nnumpy.isnan\n\ndatetimes\n\nnumpy.sum\n\nnumpy.nansum\n\nxarray (https://xarray.pydata.org/en/stable)\n\nxarray.DataArray\n\nxarray.DataArray.values\n\nxarray.DataArray.mean",
    "crumbs": [
      "Tutorials",
      "06. Sentinel-6 MF L2 Altimetry Data Access (OPeNDAP) & Gridding"
    ]
  },
  {
    "objectID": "tutorials/03_Xarray_hvplot.html",
    "href": "tutorials/03_Xarray_hvplot.html",
    "title": "03. Introduction to xarray",
    "section": "",
    "text": "As Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g. zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors."
  },
  {
    "objectID": "tutorials/03_Xarray_hvplot.html#why-do-we-need-xarray",
    "href": "tutorials/03_Xarray_hvplot.html#why-do-we-need-xarray",
    "title": "03. Introduction to xarray",
    "section": "",
    "text": "As Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g. zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors."
  },
  {
    "objectID": "tutorials/03_Xarray_hvplot.html#what-is-xarray",
    "href": "tutorials/03_Xarray_hvplot.html#what-is-xarray",
    "title": "03. Introduction to xarray",
    "section": "What is xarray",
    "text": "What is xarray\nxarray is an open-source project and python package to work with labelled multi-dimensional arrays. It is leverages numpy, pandas, matplotlib and dask to build Dataset and DataArray objects with built-in methods to subset, analyze, interpolate, and plot multi-dimensional data. It makes working with multi-dimensional data cubes efficient and fun. It will change your life for the better. You’ll be more attractive, more interesting, and better equiped to take on lifes challenges."
  },
  {
    "objectID": "tutorials/03_Xarray_hvplot.html#what-you-will-learn-from-this-tutorial",
    "href": "tutorials/03_Xarray_hvplot.html#what-you-will-learn-from-this-tutorial",
    "title": "03. Introduction to xarray",
    "section": "What you will learn from this tutorial",
    "text": "What you will learn from this tutorial\nIn this tutorial you will learn how to:\n\nload a netcdf file into xarray\ninterrogate the Dataset and understand the difference between DataArray and Dataset\nsubset a Dataset\ncalculate annual and monthly mean fields\ncalculate a time series of zonal means\nplot these results\n\nAs always, we’ll start by importing xarray. We’ll follow convention by giving the module the shortname xr\n\nimport xarray as xr\nxr.set_options(keep_attrs=True)\nimport hvplot.xarray\n\n\n\n\n\n\n\n\n\n\nI’m going to use one of xarray’s tutorial datasets. In this case, air temperature from the NCEP reanalysis. I’ll assign the result of the open_dataset to ds. I may change this to access a dataset directly\n\nds = xr.tutorial.open_dataset(\"air_temperature\")\n\nAs we are in an interactive environment, we can just type ds to see what we have.\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air      (time, lat, lon) float32 ...\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 25time: 2920lon: 53Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (1)air(time, lat, lon)float32...long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ][3869000 values with dtype=float32]Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\nFirst thing to notice is that ds is an xarray.Dataset object. It has dimensions, lat, lon, and time. It also has coordinate variables with the same names as these dimensions. These coordinate variables are 1-dimensional. This is a NetCDF convention. The Dataset contains one data variable, air. This has dimensions (time, lat, lon).\nClicking on the document icon reveals attributes for each variable. Clicking on the disk icon reveals a representation of the data.\nEach of the data and coordinate variables can be accessed and examined using the variable name as a key.\n\nds.air\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 2920, lat: 25, lon: 53)&gt;\n[3869000 values with dtype=float32]\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920lat: 25lon: 53...[3869000 values with dtype=float32]Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\n\nds['air']\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 2920, lat: 25, lon: 53)&gt;\n[3869000 values with dtype=float32]\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920lat: 25lon: 53...[3869000 values with dtype=float32]Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nThese are xarray.DataArray objects. This is the basic building block for xarray.\nVariables can also be accessed as attributes of ds.\n\nds.time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'time' (time: 2920)&gt;\narray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')\nCoordinates:\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    standard_name:  time\n    long_name:      Timexarray.DataArray'time'time: 29202013-01-01 2013-01-01T06:00:00 ... 2014-12-31T18:00:00array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Coordinates: (1)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (2)standard_name :timelong_name :Time\n\n\nA major difference between accessing a variable as an attribute versus using a key is that the attribute is read-only but the key method can be used to update the variable. For example, if I want to convert the units of air from Kelvin to degrees Celsius.\n\nds['air'] = ds.air - 273.15\n\nThis approach can also be used to add new variables\n\nds['air_kelvin'] = ds.air + 273.15\n\nIt is helpful to update attributes such as units, this saves time, confusion and mistakes, especially when you save the dataset.\n\nds['air'].attrs['units'] = 'degC'\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:     (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * lon         (lon) float32 200.0 202.5 205.0 207.5 ... 325.0 327.5 330.0\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air         (time, lat, lon) float32 -31.95 -30.65 -29.65 ... 23.04 22.54\n    air_kelvin  (time, lat, lon) float32 241.2 242.5 243.5 ... 296.5 296.2 295.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 25time: 2920lon: 53Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)air(time, lat, lon)float32-31.95 -30.65 ... 23.04 22.54long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 20.540009,  20.73999 ,  22.23999 , ...,  21.940002,\n          21.540009,  21.140015],\n        [ 23.140015,  24.040009,  24.440002, ...,  22.140015,\n          21.940002,  21.23999 ],\n        [ 24.640015,  25.23999 ,  25.339996, ...,  22.540009,\n          22.339996,  22.040009]],\n\n       [[-28.059998, -28.86    , -29.86    , ..., -31.460007,\n         -31.660004, -31.36    ],\n        [-23.259995, -23.86    , -24.759995, ..., -33.559998,\n         -32.86    , -31.460007],\n        [-10.160004, -10.959991, -11.76001 , ..., -33.259995,\n         -30.559998, -26.86    ],\n        ...,\n        [ 20.640015,  20.540009,  21.940002, ...,  22.140015,\n          21.940002,  21.540009],\n        [ 22.940002,  23.73999 ,  24.040009, ...,  22.540009,\n          22.540009,  22.040009],\n        [ 24.540009,  24.940002,  24.940002, ...,  23.339996,\n          23.040009,  22.540009]]], dtype=float32)air_kelvin(time, lat, lon)float32241.2 242.5 243.5 ... 296.2 295.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html"
  },
  {
    "objectID": "tutorials/03_Xarray_hvplot.html#subsetting-and-indexing",
    "href": "tutorials/03_Xarray_hvplot.html#subsetting-and-indexing",
    "title": "03. Introduction to xarray",
    "section": "Subsetting and Indexing",
    "text": "Subsetting and Indexing\nSubsetting and indexing methods depend on whether you are working with a Dataset or DataArray. A DataArray can be accessed using positional indexing just like a numpy array. To access the temperature field for the first time step, you do the following.\n\nds['air'][0,:,:]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (lat: 25, lon: 53)&gt;\narray([[-31.949997, -30.649994, -29.649994, ..., -40.350006, -37.649994,\n        -34.550003],\n       [-29.350006, -28.649994, -28.449997, ..., -40.350006, -37.850006,\n        -33.850006],\n       [-23.149994, -23.350006, -24.259995, ..., -39.949997, -36.759995,\n        -31.449997],\n       ...,\n       [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,  21.950012,\n         21.549988],\n       [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,  22.75    ,\n         22.049988],\n       [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,  23.640015,\n         23.450012]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n    time     datetime64[ns] 2013-01-01\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'lat: 25lon: 53-31.95 -30.65 -29.65 -29.15 -29.05 ... 24.64 24.45 23.75 23.64 23.45array([[-31.949997, -30.649994, -29.649994, ..., -40.350006, -37.649994,\n        -34.550003],\n       [-29.350006, -28.649994, -28.449997, ..., -40.350006, -37.850006,\n        -33.850006],\n       [-23.149994, -23.350006, -24.259995, ..., -39.949997, -36.759995,\n        -31.449997],\n       ...,\n       [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,  21.950012,\n         21.549988],\n       [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,  22.75    ,\n         22.049988],\n       [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,  23.640015,\n         23.450012]], dtype=float32)Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time()datetime64[ns]2013-01-01standard_name :timelong_name :Timearray('2013-01-01T00:00:00.000000000', dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nNote this returns a DataArray with coordinates but not attributes.\nHowever, the real power is being able to access variables using coordinate variables. I can get the same subset using the following. (It’s also more explicit about what is being selected and robust in case I modify the DataArray and expect the same output.)\n\nds['air'].sel(time='2013-01-01').time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'time' (time: 4)&gt;\narray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')\nCoordinates:\n  * time     (time) datetime64[ns] 2013-01-01 ... 2013-01-01T18:00:00\nAttributes:\n    standard_name:  time\n    long_name:      Timexarray.DataArray'time'time: 42013-01-01 2013-01-01T06:00:00 2013-01-01T12:00:00 2013-01-01T18:00:00array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')Coordinates: (1)time(time)datetime64[ns]2013-01-01 ... 2013-01-01T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (2)standard_name :timelong_name :Time\n\n\n\nds.air.sel(time='2013-01-01')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 4, lat: 25, lon: 53)&gt;\narray([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 22.450012,  22.25    ,  22.25    , ...,  23.140015,\n          22.140015,  21.850006],\n        [ 23.049988,  23.350006,  23.140015, ...,  23.25    ,\n          22.850006,  22.450012],\n        [ 23.25    ,  23.140015,  23.25    , ...,  23.850006,\n          23.850006,  23.640015]],\n\n       [[-31.259995, -31.350006, -31.350006, ..., -38.759995,\n         -37.649994, -35.550003],\n        [-26.850006, -27.850006, -28.949997, ..., -42.259995,\n         -41.649994, -38.649994],\n        [-16.549988, -18.449997, -21.050003, ..., -42.449997,\n         -41.350006, -37.050003],\n        ...,\n        [ 23.450012,  23.25    ,  22.850006, ...,  23.350006,\n          22.640015,  22.140015],\n        [ 23.850006,  24.350006,  23.950012, ...,  23.640015,\n          23.450012,  23.140015],\n        [ 24.350006,  24.549988,  24.350006, ...,  24.640015,\n          24.850006,  24.75    ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2013-01-01T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 4lat: 25lon: 53-31.95 -30.65 -29.65 -29.15 -29.05 ... 25.45 25.05 24.64 24.85 24.75array([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 22.450012,  22.25    ,  22.25    , ...,  23.140015,\n          22.140015,  21.850006],\n        [ 23.049988,  23.350006,  23.140015, ...,  23.25    ,\n          22.850006,  22.450012],\n        [ 23.25    ,  23.140015,  23.25    , ...,  23.850006,\n          23.850006,  23.640015]],\n\n       [[-31.259995, -31.350006, -31.350006, ..., -38.759995,\n         -37.649994, -35.550003],\n        [-26.850006, -27.850006, -28.949997, ..., -42.259995,\n         -41.649994, -38.649994],\n        [-16.549988, -18.449997, -21.050003, ..., -42.449997,\n         -41.350006, -37.050003],\n        ...,\n        [ 23.450012,  23.25    ,  22.850006, ...,  23.350006,\n          22.640015,  22.140015],\n        [ 23.850006,  24.350006,  23.950012, ...,  23.640015,\n          23.450012,  23.140015],\n        [ 24.350006,  24.549988,  24.350006, ...,  24.640015,\n          24.850006,  24.75    ]]], dtype=float32)Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2013-01-01T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nI can also do slices. I’ll extract temperatures for the state of Colorado. The bounding box for the state is [-109 E, -102 E, 37 N, 41 N].\nIn the code below, pay attention to both the order of the coordinates and the range of values. The first value of the lat coordinate variable is 41 N, the second value is 37 N. Unfortunately, xarray expects slices of coordinates to be in the same order as the coordinates. Note lon is 0 to 360 not -180 to 180, and I let python calculate it for me within the slice.\n\nds.air.sel(lat=slice(41.,37.), lon=slice(360-109,360-102))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 2920, lat: 2, lon: 3)&gt;\narray([[[-10.049988 ,  -9.25     ,  -8.75     ],\n        [ -6.25     ,  -6.549988 ,  -6.3599854]],\n\n       [[-18.149994 , -14.950012 ,  -9.950012 ],\n        [-13.649994 , -11.049988 ,  -7.25     ]],\n\n       [[-20.449997 , -18.649994 , -13.359985 ],\n        [-19.350006 , -16.950012 , -11.25     ]],\n\n       ...,\n\n       [[-24.460007 , -28.259995 , -25.759995 ],\n        [-16.959991 , -24.059998 , -24.059998 ]],\n\n       [[-24.36     , -26.160004 , -23.460007 ],\n        [-15.959991 , -22.86     , -22.960007 ]],\n\n       [[-17.559998 , -15.359985 , -13.660004 ],\n        [-13.76001  , -15.959991 , -14.459991 ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 40.0 37.5\n  * lon      (lon) float32 252.5 255.0 257.5\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920lat: 2lon: 3-10.05 -9.25 -8.75 -6.25 -6.55 ... -15.36 -13.66 -13.76 -15.96 -14.46array([[[-10.049988 ,  -9.25     ,  -8.75     ],\n        [ -6.25     ,  -6.549988 ,  -6.3599854]],\n\n       [[-18.149994 , -14.950012 ,  -9.950012 ],\n        [-13.649994 , -11.049988 ,  -7.25     ]],\n\n       [[-20.449997 , -18.649994 , -13.359985 ],\n        [-19.350006 , -16.950012 , -11.25     ]],\n\n       ...,\n\n       [[-24.460007 , -28.259995 , -25.759995 ],\n        [-16.959991 , -24.059998 , -24.059998 ]],\n\n       [[-24.36     , -26.160004 , -23.460007 ],\n        [-15.959991 , -22.86     , -22.960007 ]],\n\n       [[-17.559998 , -15.359985 , -13.660004 ],\n        [-13.76001  , -15.959991 , -14.459991 ]]], dtype=float32)Coordinates: (3)lat(lat)float3240.0 37.5standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([40. , 37.5], dtype=float32)lon(lon)float32252.5 255.0 257.5standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([252.5, 255. , 257.5], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nWhat if we want temperature for a point, for example Denver, CO (39.72510678889283 N, -104.98785545855408 E). xarray can handle this! If we just want data from the nearest grid point, we can use sel and specify the method as “nearest”.\n\ndenver_lat, denver_lon = 39.72510678889283, -104.98785545855408\n\n\nds.air.sel(lat=denver_lat, lon=360+denver_lon, method='nearest').hvplot()\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nIf we want to interpolate, we can use interp(). In this case I use linear or bilinear interpolation.\ninterp() can also be used to resample data to a new grid and even reproject data\n\nds.air.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 2920)&gt;\narray([ -8.95085077, -14.49752791, -18.43715163, ..., -27.78736503,\n       -25.78552388, -15.41780902])\nCoordinates:\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\n    lat      float64 39.73\n    lon      float64 255.0\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920-8.951 -14.5 -18.44 -11.33 -8.942 ... -22.4 -27.79 -25.79 -15.42array([ -8.95085077, -14.49752791, -18.43715163, ..., -27.78736503,\n       -25.78552388, -15.41780902])Coordinates: (3)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat()float6439.73standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray(39.72510679)lon()float64255.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray(255.01214454)Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nsel() and interp() can also be used on Dataset objects.\n\nds.sel(lat=slice(41,37), lon=slice(360-109,360-102))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:     (lat: 2, time: 2920, lon: 3)\nCoordinates:\n  * lat         (lat) float32 40.0 37.5\n  * lon         (lon) float32 252.5 255.0 257.5\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air         (time, lat, lon) float32 -10.05 -9.25 -8.75 ... -15.96 -14.46\n    air_kelvin  (time, lat, lon) float32 263.1 263.9 264.4 ... 259.4 257.2 258.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 2time: 2920lon: 3Coordinates: (3)lat(lat)float3240.0 37.5standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([40. , 37.5], dtype=float32)lon(lon)float32252.5 255.0 257.5standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([252.5, 255. , 257.5], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)air(time, lat, lon)float32-10.05 -9.25 ... -15.96 -14.46long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-10.049988 ,  -9.25     ,  -8.75     ],\n        [ -6.25     ,  -6.549988 ,  -6.3599854]],\n\n       [[-18.149994 , -14.950012 ,  -9.950012 ],\n        [-13.649994 , -11.049988 ,  -7.25     ]],\n\n       [[-20.449997 , -18.649994 , -13.359985 ],\n        [-19.350006 , -16.950012 , -11.25     ]],\n\n       ...,\n\n       [[-24.460007 , -28.259995 , -25.759995 ],\n        [-16.959991 , -24.059998 , -24.059998 ]],\n\n       [[-24.36     , -26.160004 , -23.460007 ],\n        [-15.959991 , -22.86     , -22.960007 ]],\n\n       [[-17.559998 , -15.359985 , -13.660004 ],\n        [-13.76001  , -15.959991 , -14.459991 ]]], dtype=float32)air_kelvin(time, lat, lon)float32263.1 263.9 264.4 ... 257.2 258.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[263.1    , 263.9    , 264.4    ],\n        [266.9    , 266.6    , 266.79   ]],\n\n       [[255.     , 258.19998, 263.19998],\n        [259.5    , 262.1    , 265.9    ]],\n\n       [[252.7    , 254.5    , 259.79   ],\n        [253.79999, 256.19998, 261.9    ]],\n\n       ...,\n\n       [[248.68999, 244.89   , 247.39   ],\n        [256.19   , 249.09   , 249.09   ]],\n\n       [[248.79   , 246.98999, 249.68999],\n        [257.19   , 250.29   , 250.18999]],\n\n       [[255.59   , 257.79   , 259.49   ],\n        [259.38998, 257.19   , 258.69   ]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\n\nds.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:     (time: 2920)\nCoordinates:\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\n    lat         float64 39.73\n    lon         float64 255.0\nData variables:\n    air         (time) float64 -8.951 -14.5 -18.44 ... -27.79 -25.79 -15.42\n    air_kelvin  (time) float64 264.2 258.7 254.7 261.8 ... 245.4 247.4 257.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:time: 2920Coordinates: (3)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat()float6439.73standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray(39.72510679)lon()float64255.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray(255.01214454)Data variables: (2)air(time)float64-8.951 -14.5 ... -25.79 -15.42long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([ -8.95085077, -14.49752791, -18.43715163, ..., -27.78736503,\n       -25.78552388, -15.41780902])air_kelvin(time)float64264.2 258.7 254.7 ... 247.4 257.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([264.19914312, 258.65246598, 254.71284227, ..., 245.36262886,\n       247.36447002, 257.73218487])Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html"
  },
  {
    "objectID": "tutorials/03_Xarray_hvplot.html#analysis",
    "href": "tutorials/03_Xarray_hvplot.html#analysis",
    "title": "03. Introduction to xarray",
    "section": "Analysis",
    "text": "Analysis\nAs a simple example, let’s try to calculate a mean field for the whole time range.\n\nds.mean(dim='time').hvplot()\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe can also calculate a zonal mean (averaging over longitude)\n\nds.mean(dim='lon').hvplot()\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nOther aggregation methods include min(), max(), std(), along with others.\n\nds.std(dim='time').hvplot()\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nThe data we have are in 6h timesteps. This can be resampled to daily or monthly. If you are familiar with pandas, xarray uses the same methods.\n\nds.resample(time='M').mean().hvplot()\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nds_mon = ds.resample(time='M').mean()\nds_mon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:     (time: 24, lat: 25, lon: 53)\nCoordinates:\n  * time        (time) datetime64[ns] 2013-01-31 2013-02-28 ... 2014-12-31\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * lon         (lon) float32 200.0 202.5 205.0 207.5 ... 325.0 327.5 330.0\nData variables:\n    air         (time, lat, lon) float32 -28.68 -28.49 -28.48 ... 24.57 24.56\n    air_kelvin  (time, lat, lon) float32 244.5 244.7 244.7 ... 297.7 297.7 297.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:time: 24lat: 25lon: 53Coordinates: (3)time(time)datetime64[ns]2013-01-31 ... 2014-12-31array(['2013-01-31T00:00:00.000000000', '2013-02-28T00:00:00.000000000',\n       '2013-03-31T00:00:00.000000000', '2013-04-30T00:00:00.000000000',\n       '2013-05-31T00:00:00.000000000', '2013-06-30T00:00:00.000000000',\n       '2013-07-31T00:00:00.000000000', '2013-08-31T00:00:00.000000000',\n       '2013-09-30T00:00:00.000000000', '2013-10-31T00:00:00.000000000',\n       '2013-11-30T00:00:00.000000000', '2013-12-31T00:00:00.000000000',\n       '2014-01-31T00:00:00.000000000', '2014-02-28T00:00:00.000000000',\n       '2014-03-31T00:00:00.000000000', '2014-04-30T00:00:00.000000000',\n       '2014-05-31T00:00:00.000000000', '2014-06-30T00:00:00.000000000',\n       '2014-07-31T00:00:00.000000000', '2014-08-31T00:00:00.000000000',\n       '2014-09-30T00:00:00.000000000', '2014-10-31T00:00:00.000000000',\n       '2014-11-30T00:00:00.000000000', '2014-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)Data variables: (2)air(time, lat, lon)float32-28.68 -28.49 ... 24.57 24.56long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-28.68323  , -28.486452 , -28.479755 , ..., -30.658554 ,\n         -29.743628 , -28.474194 ],\n        [-26.076784 , -26.127504 , -26.4225   , ..., -32.5679   ,\n         -31.105167 , -28.442825 ],\n        [-22.770565 , -23.31516  , -24.042498 , ..., -31.165657 ,\n         -28.38291  , -24.144924 ],\n        ...,\n        [ 22.688152 ,  22.00097  ,  21.773153 , ...,  22.218397 ,\n          21.734531 ,  21.118395 ],\n        [ 23.31952  ,  23.16702  ,  22.698233 , ...,  22.43775  ,\n          22.190727 ,  21.715578 ],\n        [ 23.903486 ,  23.89203  ,  23.585333 , ...,  23.154608 ,\n          22.947426 ,  22.889124 ]],\n\n       [[-32.41607  , -32.44866  , -32.738483 , ..., -31.54482  ,\n         -30.430185 , -29.205448 ],\n        [-31.216885 , -31.08063  , -31.236965 , ..., -32.135708 ,\n         -30.825186 , -28.42241  ],\n        [-27.826433 , -28.123934 , -28.78045  , ..., -29.734114 ,\n         -27.383936 , -23.491434 ],\n...\n        [ 24.899088 ,  24.200085 ,  24.072004 , ...,  24.861843 ,\n          24.510258 ,  23.995668 ],\n        [ 25.815008 ,  25.661922 ,  25.121607 , ...,  24.954088 ,\n          25.071083 ,  24.735588 ],\n        [ 26.023424 ,  26.06767  ,  25.74576  , ...,  25.566338 ,\n          25.591848 ,  25.630259 ]],\n\n       [[-26.348473 , -26.260897 , -26.380894 , ..., -33.07903  ,\n         -32.067986 , -30.868315 ],\n        [-25.419994 , -24.849277 , -24.405483 , ..., -34.531376 ,\n         -32.82783  , -30.179682 ],\n        [-23.181051 , -23.56476  , -23.574757 , ..., -35.446938 ,\n         -31.91259  , -26.923311 ],\n        ...,\n        [ 23.299198 ,  22.541454 ,  22.60839  , ...,  23.378307 ,\n          23.067505 ,  22.662996 ],\n        [ 24.295895 ,  24.286139 ,  24.031782 , ...,  23.80259  ,\n          23.908312 ,  23.579037 ],\n        [ 24.897346 ,  25.076134 ,  24.909689 , ...,  24.547583 ,\n          24.573233 ,  24.560413 ]]], dtype=float32)air_kelvin(time, lat, lon)float32244.5 244.7 244.7 ... 297.7 297.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[244.4667 , 244.66354, 244.67027, ..., 242.49142, 243.40633,\n         244.67577],\n        [247.07323, 247.02248, 246.7275 , ..., 240.58205, 242.04489,\n         244.70726],\n        [250.37941, 249.83484, 249.10748, ..., 241.98434, 244.76712,\n         249.00505],\n        ...,\n        [295.83795, 295.15085, 294.9229 , ..., 295.36826, 294.88437,\n         294.26828],\n        [296.46942, 296.31686, 295.84802, ..., 295.5876 , 295.34058,\n         294.86536],\n        [297.05316, 297.0418 , 296.73517, ..., 296.30438, 296.09732,\n         296.0389 ]],\n\n       [[240.73384, 240.7013 , 240.4115 , ..., 241.60518, 242.71988,\n         243.94455],\n        [241.93309, 242.06935, 241.913  , ..., 241.01428, 242.32481,\n         244.72758],\n        [245.32361, 245.0261 , 244.36955, ..., 243.41588, 245.7661 ,\n         249.65858],\n...\n        [298.04895, 297.35007, 297.22195, ..., 298.01172, 297.66013,\n         297.14554],\n        [298.96484, 298.81186, 298.27136, ..., 298.10403, 298.22104,\n         297.88547],\n        [299.17334, 299.2175 , 298.89566, ..., 298.71625, 298.74167,\n         298.7802 ]],\n\n       [[246.80156, 246.88907, 246.76907, ..., 240.07089, 241.08206,\n         242.2817 ],\n        [247.72998, 248.30064, 248.74443, ..., 238.61859, 240.3222 ,\n         242.97026],\n        [249.96893, 249.58516, 249.57521, ..., 237.70308, 241.23743,\n         246.22667],\n        ...,\n        [296.4491 , 295.6914 , 295.75824, ..., 296.52817, 296.21747,\n         295.8128 ],\n        [297.44586, 297.43613, 297.1817 , ..., 296.95242, 297.05823,\n         296.72897],\n        [298.0472 , 298.22598, 298.0595 , ..., 297.6975 , 297.72318,\n         297.71024]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\nThis is a really short time series but as an example, let’s calculate a monthly climatology (at least for 2 months). For this we can use groupby()\n\nds_clim = ds_mon.groupby(ds_mon.time.dt.month).mean()"
  },
  {
    "objectID": "tutorials/03_Xarray_hvplot.html#plot-results",
    "href": "tutorials/03_Xarray_hvplot.html#plot-results",
    "title": "03. Introduction to xarray",
    "section": "Plot results",
    "text": "Plot results\nFinally, let’s plot the results! This will plot the lat/lon axes of the original ds DataArray.\n\nds_clim.air.sel(month=10).plot()"
  },
  {
    "objectID": "tutorials/09_Zarr_Access.html",
    "href": "tutorials/09_Zarr_Access.html",
    "title": "09. Zarr Access for NetCDF4 files",
    "section": "",
    "text": "Exercise: 45 minutes",
    "crumbs": [
      "Tutorials",
      "09. Zarr Access for NetCDF4 files"
    ]
  },
  {
    "objectID": "tutorials/09_Zarr_Access.html#timing",
    "href": "tutorials/09_Zarr_Access.html#timing",
    "title": "09. Zarr Access for NetCDF4 files",
    "section": "",
    "text": "Exercise: 45 minutes",
    "crumbs": [
      "Tutorials",
      "09. Zarr Access for NetCDF4 files"
    ]
  },
  {
    "objectID": "tutorials/09_Zarr_Access.html#summary",
    "href": "tutorials/09_Zarr_Access.html#summary",
    "title": "09. Zarr Access for NetCDF4 files",
    "section": "Summary",
    "text": "Summary\nZarr is an open source library for storing N-dimensional array data. It supports multidimensional arrays with attributes and dimensions similar to NetCDF4, and it can be read by XArray. Zarr is often used for data held in cloud object storage (like Amazon S3), because it is better optimized for these situations than NetCDF4.\nThe zarr-eosdis-store library allows NASA EOSDIS NetCDF4 files to be read more efficiently by transferring only file metadata and data needed for computation in a small number of requests, rather than moving the whole file or making many small requests. It works by making the files directly readable by the Zarr Python library and XArray across a network. To use it, files must have a corresponding metadata file ending in .dmrpp, which increasingly true for cloud-accessible EOSDIS data. https://github.com/nasa/zarr-eosdis-store\nThe zarr-eosdis-store library provides several benefits over downloading EOSDIS data files and accessing them using XArray, NetCDF4, or HDF5 Python libraries:\n\nIt only downloads the chunks of data you actually read, so if you don’t read all variables or the full spatiotemporal extent of a file, you usually won’t spend time downloading those portions of the file\nIt parallelizes and optimizes downloads for the portions of files you do read, so download speeds can be faster in general\nIt automatically interoperates with Earthdata Login if you have a .netrc file set up\nIt is aware of some EOSDIS cloud implementation quirks and provides caching that can save time for repeated requests to individual files\n\nIt can also be faster than using XArray pointing NetCDF4 files with s3:// URLs, depending on the file’s internal structure, and is often more convenient.\nConsider using this library when: 1. The portion of the data file you need to use is much smaller than the full file, e.g. in cases of spatial subsets or reading a single variable from a file containing several 1. s3:// URLs are not readily available 1. Code need to run outside of the AWS cloud or us-west-2 region or in a hybrid cloud / non-cloud manner 1. s3:// access using XArray seems slower than you would expect (possibly due to unoptimized internal file structure) 1. No readily-available, public, cloud-optimized version of the data exists already. The example we show is also available as an AWS Public Dataset: https://registry.opendata.aws/mur/ 1. Adding “.dmrpp” to the end of a data URL returns a file\n\nObjectives\n\nBuild on prior knowledge from CMR and Earthdata Login tutorials\nWork through an example of using the EOSDIS Zarr Store to access data using XArray\nLearn about the Zarr format and library for accessing data in the cloud",
    "crumbs": [
      "Tutorials",
      "09. Zarr Access for NetCDF4 files"
    ]
  },
  {
    "objectID": "tutorials/09_Zarr_Access.html#exercise",
    "href": "tutorials/09_Zarr_Access.html#exercise",
    "title": "09. Zarr Access for NetCDF4 files",
    "section": "Exercise",
    "text": "Exercise\nIn this exercise, we will be using the eosdis-zarr-store library to aggregate and analyze a month of sea surface temperature for the Great Lakes region\n\nSet up\n\nImport Required Packages\n\n# Core libraries for this tutorial\n# Available via `pip install zarr zarr-eosdis-store`\nfrom eosdis_store import EosdisStore\nimport xarray as xr\n\n# Other Python libraries\nimport requests\nfrom pqdm.threads import pqdm\nfrom matplotlib import animation, pyplot as plt\nfrom IPython.core.display import display, HTML\n\n# Python standard library imports\nfrom pprint import pprint\n\nAlso set the width / height for plots we show\n\nplt.rcParams['figure.figsize'] = 12, 6\n\n\n\nSet Dataset, Time, and Region of Interest\nLook in PO.DAAC’s cloud archive for Group for High Resolution Sea Surface Temperature (GHRSST) Level 4 Multiscale Ultrahigh Resolution (MUR) data\n\ndata_provider = 'POCLOUD'\nmur_short_name = 'MUR-JPL-L4-GLOB-v4.1'\n\nLooking for data from the month of September over the Great Lakes\n\nstart_time = '2021-09-01T21:00:00Z'\nend_time = '2021-09-30T20:59:59Z'\n\n# Bounding box around the Great Lakes\nlats = slice(41, 49)\nlons = slice(-93, -76)\n\n# Some other possibly interesting bounding boxes:\n\n# Hawaiian Islands\n# lats = slice(18, 22.5)\n# lons = slice(-161, -154)\n\n# Mediterranean Sea\n# lats = slice(29, 45)\n# lons = slice(-7, 37)\n\n\n\n\nFind URLs for the dataset and AOI\nSet up a CMR granules search for our area of interest, as we saw in prior tutorials\n\ncmr_url = 'https://cmr.earthdata.nasa.gov/search/granules.json'\n\nSearch for granules in our area of interest, expecting one granule per day of September\n\nresponse = requests.get(cmr_url, \n                        params={\n                            'provider': data_provider,\n                            'short_name': mur_short_name, \n                            'temporal': f'{start_time},{end_time}',\n                            'bounding_box': f'{lons.start},{lats.start},{lons.stop},{lats.stop}',\n                            'page_size': 2000,\n                            }\n                       )\n\n\ngranules = response.json()['feed']['entry']\n\nfor granule in granules:\n    print(granule['title'])\n\n20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210902090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210903090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210904090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210905090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210906090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210907090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210908090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210909090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210910090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210911090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210912090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210913090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210914090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210915090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210916090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210917090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210918090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210919090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210920090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210921090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210922090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210923090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210924090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210925090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210926090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210927090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210928090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210929090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n20210930090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1\n\n\n\npprint(granules[0])\n\n{'boxes': ['-90 -180 90 180'],\n 'browse_flag': False,\n 'collection_concept_id': 'C1996881146-POCLOUD',\n 'coordinate_system': 'CARTESIAN',\n 'data_center': 'POCLOUD',\n 'dataset_id': 'GHRSST Level 4 MUR Global Foundation Sea Surface Temperature '\n               'Analysis (v4.1)',\n 'day_night_flag': 'UNSPECIFIED',\n 'granule_size': '9.059906005859375E-5',\n 'id': 'G2113241213-POCLOUD',\n 'links': [{'href': 's3://podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule.'},\n           {'href': 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     '20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc'},\n           {'href': 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-public/MUR-JPL-L4-GLOB-v4.1/20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc.md5',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'Download '\n                     '20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc.md5'},\n           {'href': 'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'api endpoint to retrieve temporary credentials valid for '\n                     'same-region direct s3 access'},\n           {'href': 'https://opendap.earthdata.nasa.gov/collections/C1996881146-POCLOUD/granules/20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/service#',\n            'title': 'OPeNDAP request URL'},\n           {'href': 'https://github.com/nasa/podaac_tools_and_services/tree/master/subset_opendap',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://ghrsst.jpl.nasa.gov',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://earthdata.nasa.gov/esds/competitive-programs/measures/mur-sst',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'http://journals.ametsoc.org/doi/abs/10.1175/1520-0426%281998%29015%3C0741:BSHWSS%3E2.0.CO;2',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://podaac-tools.jpl.nasa.gov/drive/files/OceanTemperature/ghrsst/docs/GDS20r5.pdf',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://github.com/podaac/data-readers',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://doi.org/10.1016/j.rse.2017.07.029',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://registry.opendata.aws/mur/#usageexa',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'http://www.ghrsst.org',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://podaac.jpl.nasa.gov/CitingPODAAC',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://cmr.earthdata.nasa.gov/virtual-directory/collections/C1996881146-POCLOUD ',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'length': '300.0MB',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n           {'href': ' '\n                    'https://search.earthdata.nasa.gov/search/granules?p=C1996881146-POCLOUD ',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'length': '700.0MB',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n           {'href': 'https://podaac.jpl.nasa.gov/MEaSUREs-MUR',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://github.com/podaac/tutorials/blob/master/notebooks/SWOT-EA-2021/Colocate_satellite_insitu_ocean.ipynb',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'}],\n 'online_access_flag': True,\n 'original_format': 'UMM_JSON',\n 'time_end': '2021-09-01T21:00:00.000Z',\n 'time_start': '2021-08-31T21:00:00.000Z',\n 'title': '20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1',\n 'updated': '2021-09-10T07:29:40.511Z'}\n\n\n\nurls = []\nfor granule in granules:\n    for link in granule['links']:\n        if link['rel'].endswith('/data#'):\n            urls.append(link['href'])\n            break\npprint(urls)\n\n['https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210901090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210902090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210903090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210904090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210905090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210906090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210907090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210908090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210909090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210910090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210911090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210912090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210913090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210914090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210915090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210916090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210917090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210918090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210919090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210920090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210921090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210922090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210923090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210924090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210925090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210926090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210927090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210928090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210929090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc',\n 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210930090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc']\n\n\n\n\nOpen and view our AOI without downloading a whole file\n\nCheck to see if we can use an efficient partial-access technique\n\nresponse = requests.head(f'{urls[0]}.dmrpp')\n\nprint('Can we use EosdisZarrStore and XArray to access these files more efficiently?')\nprint('Yes' if response.ok else 'No')\n\nCan we use EosdisZarrStore and XArray to access these files more efficiently?\nYes\n\n\nOpen our first URL using the Zarr library\n\nurl = urls[0]\n\nds = xr.open_zarr(EosdisStore(url), consolidated=False)\n\nThat’s it! No downloads, temporary credentials, or S3 filesystems. Hereafter, we interact with the ds variable as with any XArray dataset. We need not worry about the EosdisStore anymore.\nView the file’s variable structure\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:           (time: 1, lat: 17999, lon: 36000)\nCoordinates:\n  * lat               (lat) float32 -89.99 -89.98 -89.97 ... 89.97 89.98 89.99\n  * lon               (lon) float32 -180.0 -180.0 -180.0 ... 180.0 180.0 180.0\n  * time              (time) datetime64[ns] 2021-09-01T09:00:00\nData variables:\n    analysed_sst      (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n    analysis_error    (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n    dt_1km_data       (time, lat, lon) timedelta64[ns] dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n    mask              (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n    sea_ice_fraction  (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n    sst_anomaly       (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\nAttributes: (12/47)\n    Conventions:                CF-1.7\n    title:                      Daily MUR SST, Final product\n    summary:                    A merged, multi-sensor L4 Foundation SST anal...\n    references:                 http://podaac.jpl.nasa.gov/Multi-scale_Ultra-...\n    institution:                Jet Propulsion Laboratory\n    history:                    created at nominal 4-day latency; replaced nr...\n    ...                         ...\n    project:                    NASA Making Earth Science Data Records for Us...\n    publisher_name:             GHRSST Project Office\n    publisher_url:              http://www.ghrsst.org\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L4\n    cdm_data_type:              gridxarray.DatasetDimensions:time: 1lat: 17999lon: 36000Coordinates: (3)lat(lat)float32-89.99 -89.98 ... 89.98 89.99long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([-89.99, -89.98, -89.97, ...,  89.97,  89.98,  89.99], dtype=float32)lon(lon)float32-180.0 -180.0 ... 180.0 180.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([-179.99, -179.98, -179.97, ...,  179.98,  179.99,  180.  ],\n      dtype=float32)time(time)datetime64[ns]2021-09-01T09:00:00long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2021-09-01T09:00:00.000000000'], dtype='datetime64[ns]')Data variables: (6)analysed_sst(time, lat, lon)float32dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\\\"Final\\\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.41 GiB\n7.99 MiB\n\n\nShape\n(1, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n325 Tasks\n324 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n                                                                                             36000 17999 1\n\n\n\n\nanalysis_error(time, lat, lon)float32dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;long_name :estimated error standard deviation of analysed_sstunits :kelvinvalid_min :0valid_max :32767comment :uncertainty in \\\"analysed_sst\\\"\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.41 GiB\n7.99 MiB\n\n\nShape\n(1, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n325 Tasks\n324 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n                                                                                             36000 17999 1\n\n\n\n\ndt_1km_data(time, lat, lon)timedelta64[ns]dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;long_name :time to most recent 1km datavalid_min :-127valid_max :127source :MODIS and VIIRS pixels ingested by MURcomment :The grid value is hours between the analysis time and the most recent MODIS or VIIRS 1km L2P datum within 0.01 degrees from the grid point.  \\\"Fill value\\\" indicates absence of such 1km data at the grid point.\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.83 GiB\n31.96 MiB\n\n\nShape\n(1, 17999, 36000)\n(1, 1447, 2895)\n\n\nCount\n170 Tasks\n169 Chunks\n\n\nType\ntimedelta64[ns]\nnumpy.ndarray\n\n\n\n\n                                                                         36000 17999 1\n\n\n\n\nmask(time, lat, lon)float32dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;long_name :sea/land field composite maskvalid_min :1valid_max :31flag_masks :[1, 2, 4, 8, 16]flag_meanings :open_sea land open_lake open_sea_with_ice_in_the_grid open_lake_with_ice_in_the_gridcomment :mask can be used to further filter the data.source :GMT \\\"grdlandmask\\\", ice flag from sea_ice_fraction data\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.41 GiB\n15.98 MiB\n\n\nShape\n(1, 17999, 36000)\n(1, 1447, 2895)\n\n\nCount\n170 Tasks\n169 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n                                                                         36000 17999 1\n\n\n\n\nsea_ice_fraction(time, lat, lon)float32dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;long_name :sea ice area fractionstandard_name :sea_ice_area_fractionvalid_min :0valid_max :100source :EUMETSAT OSI-SAF, copyright EUMETSATcomment :ice fraction is a dimensionless quantity between 0 and 1; it has been interpolated by a nearest neighbor approach.\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.41 GiB\n15.98 MiB\n\n\nShape\n(1, 17999, 36000)\n(1, 1447, 2895)\n\n\nCount\n170 Tasks\n169 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n                                                                         36000 17999 1\n\n\n\n\nsst_anomaly(time, lat, lon)float32dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;long_name :SST anomaly from a seasonal SST climatology based on the MUR data over 2003-2014 periodunits :kelvinvalid_min :-32767valid_max :32767comment :anomaly reference to the day-of-year average between 2003 and 2014\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.41 GiB\n7.99 MiB\n\n\nShape\n(1, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n325 Tasks\n324 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n                                                                                             36000 17999 1\n\n\n\n\nAttributes: (47)Conventions :CF-1.7title :Daily MUR SST, Final productsummary :A merged, multi-sensor L4 Foundation SST analysis product from JPL.references :http://podaac.jpl.nasa.gov/Multi-scale_Ultra-high_Resolution_MUR-SSTinstitution :Jet Propulsion Laboratoryhistory :created at nominal 4-day latency; replaced nrt (1-day latency) version.comment :MUR = \\\"Multi-scale Ultra-high Resolution\\\"license :These data are available free of charge under data policy of JPL PO.DAAC.id :MUR-JPL-L4-GLOB-v04.1naming_authority :org.ghrsstproduct_version :04.1uuid :27665bc0-d5fc-11e1-9b23-0800200c9a66gds_version_id :2.0netcdf_version_id :4.1date_created :20210910T072132Zstart_time :20210901T090000Zstop_time :20210901T090000Ztime_coverage_start :20210831T210000Ztime_coverage_end :20210901T210000Zfile_quality_level :3source :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAFplatform :Terra, Aqua, GCOM-W, MetOp-A, MetOp-B, Buoys/Shipssensor :MODIS, AMSR2, AVHRR, in-situMetadata_Conventions :Unidata Observation Dataset v1.0metadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MUR-JPL-L4-GLOB-v04.1keywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsouthernmost_latitude :-90.0northernmost_latitude :90.0westernmost_longitude :-180.0easternmost_longitude :180.0spatial_resolution :0.01 degreesgeospatial_lat_units :degrees northgeospatial_lat_resolution :0.009999999776geospatial_lon_units :degrees eastgeospatial_lon_resolution :0.009999999776acknowledgment :Please acknowledge the use of these data with the following statement:  These data were provided by JPL under support by NASA MEaSUREs program.creator_name :JPL MUR SST projectcreator_email :ghrsst@podaac.jpl.nasa.govcreator_url :http://mur.jpl.nasa.govproject :NASA Making Earth Science Data Records for Use in Research Environments (MEaSUREs) Programpublisher_name :GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L4cdm_data_type :grid\n\n\n\nds.analysed_sst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'analysed_sst' (time: 1, lat: 17999, lon: 36000)&gt;\ndask.array&lt;open_dataset-4d5a9a1e1fda090e80524b67b2e413c6analysed_sst, shape=(1, 17999, 36000), dtype=float32, chunksize=(1, 1023, 2047), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float32 -89.99 -89.98 -89.97 -89.96 ... 89.97 89.98 89.99\n  * lon      (lon) float32 -180.0 -180.0 -180.0 -180.0 ... 180.0 180.0 180.0\n  * time     (time) datetime64[ns] 2021-09-01T09:00:00\nAttributes:\n    long_name:      analysed sea surface temperature\n    standard_name:  sea_surface_foundation_temperature\n    units:          kelvin\n    valid_min:      -32767\n    valid_max:      32767\n    comment:        \\\"Final\\\" version using Multi-Resolution Variational Anal...\n    source:         MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, A...xarray.DataArray'analysed_sst'time: 1lat: 17999lon: 36000dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.41 GiB\n7.99 MiB\n\n\nShape\n(1, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n325 Tasks\n324 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n                                                                                             36000 17999 1\n\n\n\n\nCoordinates: (3)lat(lat)float32-89.99 -89.98 ... 89.98 89.99long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([-89.99, -89.98, -89.97, ...,  89.97,  89.98,  89.99], dtype=float32)lon(lon)float32-180.0 -180.0 ... 180.0 180.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([-179.99, -179.98, -179.97, ...,  179.98,  179.99,  180.  ],\n      dtype=float32)time(time)datetime64[ns]2021-09-01T09:00:00long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2021-09-01T09:00:00.000000000'], dtype='datetime64[ns]')Attributes: (7)long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\\\"Final\\\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\nsst = ds.analysed_sst.sel(lat=lats, lon=lons)\nsst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'analysed_sst' (time: 1, lat: 801, lon: 1701)&gt;\ndask.array&lt;getitem, shape=(1, 801, 1701), dtype=float32, chunksize=(1, 601, 1536), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float32 41.0 41.01 41.02 41.03 ... 48.97 48.98 48.99 49.0\n  * lon      (lon) float32 -93.0 -92.99 -92.98 -92.97 ... -76.02 -76.01 -76.0\n  * time     (time) datetime64[ns] 2021-09-01T09:00:00\nAttributes:\n    long_name:      analysed sea surface temperature\n    standard_name:  sea_surface_foundation_temperature\n    units:          kelvin\n    valid_min:      -32767\n    valid_max:      32767\n    comment:        \\\"Final\\\" version using Multi-Resolution Variational Anal...\n    source:         MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, A...xarray.DataArray'analysed_sst'time: 1lat: 801lon: 1701dask.array&lt;chunksize=(1, 200, 1536), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n5.20 MiB\n3.52 MiB\n\n\nShape\n(1, 801, 1701)\n(1, 601, 1536)\n\n\nCount\n329 Tasks\n4 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n                             1701 801 1\n\n\n\n\nCoordinates: (3)lat(lat)float3241.0 41.01 41.02 ... 48.99 49.0long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([41.  , 41.01, 41.02, ..., 48.98, 48.99, 49.  ], dtype=float32)lon(lon)float32-93.0 -92.99 ... -76.01 -76.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([-93.  , -92.99, -92.98, ..., -76.02, -76.01, -76.  ], dtype=float32)time(time)datetime64[ns]2021-09-01T09:00:00long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2021-09-01T09:00:00.000000000'], dtype='datetime64[ns]')Attributes: (7)long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\\\"Final\\\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\nsst.plot()\n\n\n\n\n\n\n\n\n\n\n\nAggregate and analyze 30 files\nSet up a function to open all of our URLs as XArrays in parallel\n\ndef open_as_zarr_xarray(url):\n    return xr.open_zarr(EosdisStore(url), consolidated=False)\n\ndatasets = pqdm(urls, open_as_zarr_xarray, n_jobs=30)\n\n\n\n\n\n\n\n\n\n\nCombine the individual file-based datasets into a single xarray dataset with a time axis\n\nds = xr.concat(datasets, 'time')\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:           (time: 30, lat: 17999, lon: 36000)\nCoordinates:\n  * lat               (lat) float32 -89.99 -89.98 -89.97 ... 89.97 89.98 89.99\n  * lon               (lon) float32 -180.0 -180.0 -180.0 ... 180.0 180.0 180.0\n  * time              (time) datetime64[ns] 2021-09-01T09:00:00 ... 2021-09-3...\nData variables:\n    analysed_sst      (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n    analysis_error    (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n    dt_1km_data       (time, lat, lon) timedelta64[ns] dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n    mask              (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n    sea_ice_fraction  (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n    sst_anomaly       (time, lat, lon) float32 dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\nAttributes: (12/47)\n    Conventions:                CF-1.7\n    title:                      Daily MUR SST, Final product\n    summary:                    A merged, multi-sensor L4 Foundation SST anal...\n    references:                 http://podaac.jpl.nasa.gov/Multi-scale_Ultra-...\n    institution:                Jet Propulsion Laboratory\n    history:                    created at nominal 4-day latency; replaced nr...\n    ...                         ...\n    project:                    NASA Making Earth Science Data Records for Us...\n    publisher_name:             GHRSST Project Office\n    publisher_url:              http://www.ghrsst.org\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L4\n    cdm_data_type:              gridxarray.DatasetDimensions:time: 30lat: 17999lon: 36000Coordinates: (3)lat(lat)float32-89.99 -89.98 ... 89.98 89.99long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([-89.99, -89.98, -89.97, ...,  89.97,  89.98,  89.99], dtype=float32)lon(lon)float32-180.0 -180.0 ... 180.0 180.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([-179.99, -179.98, -179.97, ...,  179.98,  179.99,  180.  ],\n      dtype=float32)time(time)datetime64[ns]2021-09-01T09:00:00 ... 2021-09-...long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2021-09-01T09:00:00.000000000', '2021-09-02T09:00:00.000000000',\n       '2021-09-03T09:00:00.000000000', '2021-09-04T09:00:00.000000000',\n       '2021-09-05T09:00:00.000000000', '2021-09-06T09:00:00.000000000',\n       '2021-09-07T09:00:00.000000000', '2021-09-08T09:00:00.000000000',\n       '2021-09-09T09:00:00.000000000', '2021-09-10T09:00:00.000000000',\n       '2021-09-11T09:00:00.000000000', '2021-09-12T09:00:00.000000000',\n       '2021-09-13T09:00:00.000000000', '2021-09-14T09:00:00.000000000',\n       '2021-09-15T09:00:00.000000000', '2021-09-16T09:00:00.000000000',\n       '2021-09-17T09:00:00.000000000', '2021-09-18T09:00:00.000000000',\n       '2021-09-19T09:00:00.000000000', '2021-09-20T09:00:00.000000000',\n       '2021-09-21T09:00:00.000000000', '2021-09-22T09:00:00.000000000',\n       '2021-09-23T09:00:00.000000000', '2021-09-24T09:00:00.000000000',\n       '2021-09-25T09:00:00.000000000', '2021-09-26T09:00:00.000000000',\n       '2021-09-27T09:00:00.000000000', '2021-09-28T09:00:00.000000000',\n       '2021-09-29T09:00:00.000000000', '2021-09-30T09:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (6)analysed_sst(time, lat, lon)float32dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\\\"Final\\\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n72.42 GiB\n7.99 MiB\n\n\nShape\n(30, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n19470 Tasks\n9720 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n                                                                                                                                 36000 17999 30\n\n\n\n\nanalysis_error(time, lat, lon)float32dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;long_name :estimated error standard deviation of analysed_sstunits :kelvinvalid_min :0valid_max :32767comment :uncertainty in \\\"analysed_sst\\\"\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n72.42 GiB\n7.99 MiB\n\n\nShape\n(30, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n19470 Tasks\n9720 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n                                                                                                                                 36000 17999 30\n\n\n\n\ndt_1km_data(time, lat, lon)timedelta64[ns]dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;long_name :time to most recent 1km datavalid_min :-127valid_max :127source :MODIS and VIIRS pixels ingested by MURcomment :The grid value is hours between the analysis time and the most recent MODIS or VIIRS 1km L2P datum within 0.01 degrees from the grid point.  \\\"Fill value\\\" indicates absence of such 1km data at the grid point.\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n144.83 GiB\n31.96 MiB\n\n\nShape\n(30, 17999, 36000)\n(1, 1447, 2895)\n\n\nCount\n10170 Tasks\n5070 Chunks\n\n\nType\ntimedelta64[ns]\nnumpy.ndarray\n\n\n\n\n                                                                                                             36000 17999 30\n\n\n\n\nmask(time, lat, lon)float32dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;long_name :sea/land field composite maskvalid_min :1valid_max :31flag_masks :[1, 2, 4, 8, 16]flag_meanings :open_sea land open_lake open_sea_with_ice_in_the_grid open_lake_with_ice_in_the_gridcomment :mask can be used to further filter the data.source :GMT \\\"grdlandmask\\\", ice flag from sea_ice_fraction data\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n72.42 GiB\n15.98 MiB\n\n\nShape\n(30, 17999, 36000)\n(1, 1447, 2895)\n\n\nCount\n10170 Tasks\n5070 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n                                                                                                             36000 17999 30\n\n\n\n\nsea_ice_fraction(time, lat, lon)float32dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;long_name :sea ice area fractionstandard_name :sea_ice_area_fractionvalid_min :0valid_max :100source :EUMETSAT OSI-SAF, copyright EUMETSATcomment :ice fraction is a dimensionless quantity between 0 and 1; it has been interpolated by a nearest neighbor approach.\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n72.42 GiB\n15.98 MiB\n\n\nShape\n(30, 17999, 36000)\n(1, 1447, 2895)\n\n\nCount\n10170 Tasks\n5070 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n                                                                                                             36000 17999 30\n\n\n\n\nsst_anomaly(time, lat, lon)float32dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;long_name :SST anomaly from a seasonal SST climatology based on the MUR data over 2003-2014 periodunits :kelvinvalid_min :-32767valid_max :32767comment :anomaly reference to the day-of-year average between 2003 and 2014\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n72.42 GiB\n7.99 MiB\n\n\nShape\n(30, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n19470 Tasks\n9720 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n                                                                                                                                 36000 17999 30\n\n\n\n\nAttributes: (47)Conventions :CF-1.7title :Daily MUR SST, Final productsummary :A merged, multi-sensor L4 Foundation SST analysis product from JPL.references :http://podaac.jpl.nasa.gov/Multi-scale_Ultra-high_Resolution_MUR-SSTinstitution :Jet Propulsion Laboratoryhistory :created at nominal 4-day latency; replaced nrt (1-day latency) version.comment :MUR = \\\"Multi-scale Ultra-high Resolution\\\"license :These data are available free of charge under data policy of JPL PO.DAAC.id :MUR-JPL-L4-GLOB-v04.1naming_authority :org.ghrsstproduct_version :04.1uuid :27665bc0-d5fc-11e1-9b23-0800200c9a66gds_version_id :2.0netcdf_version_id :4.1date_created :20210910T072132Zstart_time :20210901T090000Zstop_time :20210901T090000Ztime_coverage_start :20210831T210000Ztime_coverage_end :20210901T210000Zfile_quality_level :3source :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAFplatform :Terra, Aqua, GCOM-W, MetOp-A, MetOp-B, Buoys/Shipssensor :MODIS, AMSR2, AVHRR, in-situMetadata_Conventions :Unidata Observation Dataset v1.0metadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MUR-JPL-L4-GLOB-v04.1keywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsouthernmost_latitude :-90.0northernmost_latitude :90.0westernmost_longitude :-180.0easternmost_longitude :180.0spatial_resolution :0.01 degreesgeospatial_lat_units :degrees northgeospatial_lat_resolution :0.009999999776geospatial_lon_units :degrees eastgeospatial_lon_resolution :0.009999999776acknowledgment :Please acknowledge the use of these data with the following statement:  These data were provided by JPL under support by NASA MEaSUREs program.creator_name :JPL MUR SST projectcreator_email :ghrsst@podaac.jpl.nasa.govcreator_url :http://mur.jpl.nasa.govproject :NASA Making Earth Science Data Records for Use in Research Environments (MEaSUREs) Programpublisher_name :GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L4cdm_data_type :grid\n\n\nLook at the Analysed SST variable metadata\n\nall_sst = ds.analysed_sst\nall_sst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'analysed_sst' (time: 30, lat: 17999, lon: 36000)&gt;\ndask.array&lt;concatenate, shape=(30, 17999, 36000), dtype=float32, chunksize=(1, 1023, 2047), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float32 -89.99 -89.98 -89.97 -89.96 ... 89.97 89.98 89.99\n  * lon      (lon) float32 -180.0 -180.0 -180.0 -180.0 ... 180.0 180.0 180.0\n  * time     (time) datetime64[ns] 2021-09-01T09:00:00 ... 2021-09-30T09:00:00\nAttributes:\n    long_name:      analysed sea surface temperature\n    standard_name:  sea_surface_foundation_temperature\n    units:          kelvin\n    valid_min:      -32767\n    valid_max:      32767\n    comment:        \\\"Final\\\" version using Multi-Resolution Variational Anal...\n    source:         MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, A...xarray.DataArray'analysed_sst'time: 30lat: 17999lon: 36000dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n72.42 GiB\n7.99 MiB\n\n\nShape\n(30, 17999, 36000)\n(1, 1023, 2047)\n\n\nCount\n19470 Tasks\n9720 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n                                                                                                                                 36000 17999 30\n\n\n\n\nCoordinates: (3)lat(lat)float32-89.99 -89.98 ... 89.98 89.99long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([-89.99, -89.98, -89.97, ...,  89.97,  89.98,  89.99], dtype=float32)lon(lon)float32-180.0 -180.0 ... 180.0 180.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([-179.99, -179.98, -179.97, ...,  179.98,  179.99,  180.  ],\n      dtype=float32)time(time)datetime64[ns]2021-09-01T09:00:00 ... 2021-09-...long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2021-09-01T09:00:00.000000000', '2021-09-02T09:00:00.000000000',\n       '2021-09-03T09:00:00.000000000', '2021-09-04T09:00:00.000000000',\n       '2021-09-05T09:00:00.000000000', '2021-09-06T09:00:00.000000000',\n       '2021-09-07T09:00:00.000000000', '2021-09-08T09:00:00.000000000',\n       '2021-09-09T09:00:00.000000000', '2021-09-10T09:00:00.000000000',\n       '2021-09-11T09:00:00.000000000', '2021-09-12T09:00:00.000000000',\n       '2021-09-13T09:00:00.000000000', '2021-09-14T09:00:00.000000000',\n       '2021-09-15T09:00:00.000000000', '2021-09-16T09:00:00.000000000',\n       '2021-09-17T09:00:00.000000000', '2021-09-18T09:00:00.000000000',\n       '2021-09-19T09:00:00.000000000', '2021-09-20T09:00:00.000000000',\n       '2021-09-21T09:00:00.000000000', '2021-09-22T09:00:00.000000000',\n       '2021-09-23T09:00:00.000000000', '2021-09-24T09:00:00.000000000',\n       '2021-09-25T09:00:00.000000000', '2021-09-26T09:00:00.000000000',\n       '2021-09-27T09:00:00.000000000', '2021-09-28T09:00:00.000000000',\n       '2021-09-29T09:00:00.000000000', '2021-09-30T09:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (7)long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\\\"Final\\\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\nCreate a dataset / variable that is only our area of interest and view its metadata\n\nsst = ds.analysed_sst.sel(lat=lats, lon=lons)\nsst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'analysed_sst' (time: 30, lat: 801, lon: 1701)&gt;\ndask.array&lt;getitem, shape=(30, 801, 1701), dtype=float32, chunksize=(1, 601, 1536), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float32 41.0 41.01 41.02 41.03 ... 48.97 48.98 48.99 49.0\n  * lon      (lon) float32 -93.0 -92.99 -92.98 -92.97 ... -76.02 -76.01 -76.0\n  * time     (time) datetime64[ns] 2021-09-01T09:00:00 ... 2021-09-30T09:00:00\nAttributes:\n    long_name:      analysed sea surface temperature\n    standard_name:  sea_surface_foundation_temperature\n    units:          kelvin\n    valid_min:      -32767\n    valid_max:      32767\n    comment:        \\\"Final\\\" version using Multi-Resolution Variational Anal...\n    source:         MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, A...xarray.DataArray'analysed_sst'time: 30lat: 801lon: 1701dask.array&lt;chunksize=(1, 200, 1536), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n155.93 MiB\n3.52 MiB\n\n\nShape\n(30, 801, 1701)\n(1, 601, 1536)\n\n\nCount\n19590 Tasks\n120 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n                                                                 1701 801 30\n\n\n\n\nCoordinates: (3)lat(lat)float3241.0 41.01 41.02 ... 48.99 49.0long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([41.  , 41.01, 41.02, ..., 48.98, 48.99, 49.  ], dtype=float32)lon(lon)float32-93.0 -92.99 ... -76.01 -76.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([-93.  , -92.99, -92.98, ..., -76.02, -76.01, -76.  ], dtype=float32)time(time)datetime64[ns]2021-09-01T09:00:00 ... 2021-09-...long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2021-09-01T09:00:00.000000000', '2021-09-02T09:00:00.000000000',\n       '2021-09-03T09:00:00.000000000', '2021-09-04T09:00:00.000000000',\n       '2021-09-05T09:00:00.000000000', '2021-09-06T09:00:00.000000000',\n       '2021-09-07T09:00:00.000000000', '2021-09-08T09:00:00.000000000',\n       '2021-09-09T09:00:00.000000000', '2021-09-10T09:00:00.000000000',\n       '2021-09-11T09:00:00.000000000', '2021-09-12T09:00:00.000000000',\n       '2021-09-13T09:00:00.000000000', '2021-09-14T09:00:00.000000000',\n       '2021-09-15T09:00:00.000000000', '2021-09-16T09:00:00.000000000',\n       '2021-09-17T09:00:00.000000000', '2021-09-18T09:00:00.000000000',\n       '2021-09-19T09:00:00.000000000', '2021-09-20T09:00:00.000000000',\n       '2021-09-21T09:00:00.000000000', '2021-09-22T09:00:00.000000000',\n       '2021-09-23T09:00:00.000000000', '2021-09-24T09:00:00.000000000',\n       '2021-09-25T09:00:00.000000000', '2021-09-26T09:00:00.000000000',\n       '2021-09-27T09:00:00.000000000', '2021-09-28T09:00:00.000000000',\n       '2021-09-29T09:00:00.000000000', '2021-09-30T09:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (7)long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\\\"Final\\\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\nXArray reads data lazily, i.e. only when our code actually needs it. Up to this point, we haven’t read any data values, only metadata. The next line will force XArray to read the portions of the source files containing our area of interest. Behind the scenes, the eosdis-zarr-store library is ensuring data is fetched as efficiently as possible.\nNote: This line isn’t strictly necessary, since XArray will automatically read the data we need the first time our code tries to use it, but calling this will make sure that we can read the data multiple times later on without re-fetching anything from the source files.\nThis line will take several seconds to complete, but since it is retrieving only about 50 MB of data from 22 GB of source files, several seconds constitutes a significant time, bandwidth, and disk space savings.\n\nsst.load();\n\nNow we can start looking at aggregations across the time dimension. In this case, plot the standard deviation of the temperature at each point to get a visual sense of how much temperatures fluctuate over the course of the month.\n\n# We expect a warning here, from finding the standard deviation of arrays that contain all N/A values.\n# numpy produces N/A for these points, though, which is exactly what we want.\nstdev_sst = sst.std('time')\nstdev_sst.name = 'stdev of analysed_sst [Kelvin]'\nstdev_sst.plot();\n\n/srv/conda/envs/notebook/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1670: RuntimeWarning: Degrees of freedom &lt;= 0 for slice.\n  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n\n\n\n\n\n\n\n\n\n\nInteractive animation of a month of data\nThis section isn’t as important to fully understand. It shows us a way to get an interactive animation to see what we have retrieved so far\nDefine an animation function to plot the ith time step. We need to make sure each plot is using the same color scale, set by vmin and vmax so the animation is consistent\n\nsst_min = sst.min()\nsst_max = sst.max()\n\ndef show_time_step(i):\n    plt.clf()\n    res = sst[i].plot.imshow(vmin=sst_min, vmax=sst_max)\n    return (res,)\n\nRender each time slice once and show it as an HTML animation with interactive controls\n\n#anim = animation.FuncAnimation(plt.gcf(), func=show_time_step, frames=len(sst))\n#display(HTML(anim.to_jshtml()))\n#plt.close()\n\n\n\n\nSupplemental: What’s happening here?\nFor EOSDIS data in the cloud, we have begun producing a metadata sidecar file in a format called DMR++ that extracts all of the information about arrays, variables, and dimensions from data files, as well as the byte offsets in the NetCDF4 file where data can be found. This information is sufficient to let the Zarr library read data from our NetCDF4 files, but it’s in the wrong format. zarr-eosdis-store knows how to fetch the sidecar file and transform it into something the Zarr library understands. Passing it when reading Zarr using XArray or the Zarr library lets these libraries interact with EOSDIS data exactly as if they were Zarr stores in a way that’s more optimal for reading data in the cloud. Beyond this, the zarr-eosdis-store library makes some optimizations in the way it reads data to help make up for situations where the NetCDF4 file is not internally arranged well for cloud-based access patterns.",
    "crumbs": [
      "Tutorials",
      "09. Zarr Access for NetCDF4 files"
    ]
  },
  {
    "objectID": "tutorials/00_Setup.html",
    "href": "tutorials/00_Setup.html",
    "title": "00. Setup for tutorials",
    "section": "",
    "text": "This tutorial will help you set up your JupyterHub (or Hub) with tutorials and other materials from our Cloud Hackathon github repository and connect your github account.",
    "crumbs": [
      "Tutorials",
      "00. Setup for tutorials"
    ]
  },
  {
    "objectID": "tutorials/00_Setup.html#step-1.-login-to-the-hub",
    "href": "tutorials/00_Setup.html#step-1.-login-to-the-hub",
    "title": "00. Setup for tutorials",
    "section": "Step 1. Login to the Hub",
    "text": "Step 1. Login to the Hub\nPlease go to the Openscapes Jupyter Hub. Log in with your GitHub Account, and select “Small”.\n\n\nNote: It takes a few minutes for the Hub to load. Please be patient!\n\nWhile the Hub loads, we’ll:\n\nDiscuss cloud environments\nSee how my Desktop is setup\nFork the Hackathon repository at github.com\nDiscuss python and conda environments\n\nThen, when the Hub is loaded, we’ll get oriented in the Hub and clone the forked repository into our cloud environment.",
    "crumbs": [
      "Tutorials",
      "00. Setup for tutorials"
    ]
  },
  {
    "objectID": "tutorials/00_Setup.html#discussion-cloud-environment",
    "href": "tutorials/00_Setup.html#discussion-cloud-environment",
    "title": "00. Setup for tutorials",
    "section": "Discussion: Cloud environment",
    "text": "Discussion: Cloud environment\nA brief overview about the NASA Openscapes Cloud Environment (following lessons from the Clinic).\n\nCloud infrastructure\n\nCloud: AWS us-west-2\n\nData: AWS S3 (cloud) and NASA DAAC data centers (on-prem).\nCloud compute environment: 2i2c Jupyterhub deployment\n\nIDE: JupyterLab",
    "crumbs": [
      "Tutorials",
      "00. Setup for tutorials"
    ]
  },
  {
    "objectID": "tutorials/00_Setup.html#discussion-my-desktop-setup",
    "href": "tutorials/00_Setup.html#discussion-my-desktop-setup",
    "title": "00. Setup for tutorials",
    "section": "Discussion: My desktop setup",
    "text": "Discussion: My desktop setup\nI’ll screenshare to show and/or talk through how I have oriented the following software we’re using:\n\n2i2c Jupyterhub (our main workspace)\nHackathon Repo &lt;&gt; Hackathon Book (my teaching notes, your reference material)\nZoom Chat\nSlack",
    "crumbs": [
      "Tutorials",
      "00. Setup for tutorials"
    ]
  },
  {
    "objectID": "tutorials/00_Setup.html#step-2.-fork-the-hackathon-github-repository",
    "href": "tutorials/00_Setup.html#step-2.-fork-the-hackathon-github-repository",
    "title": "00. Setup for tutorials",
    "section": "Step 2. Fork the Hackathon GitHub repository",
    "text": "Step 2. Fork the Hackathon GitHub repository\n“How do I get the tutorial repository into the Hub?”. There are 2 steps. The first is from GitHub.com to fork the tutorial repository so that there is a connected copy in your user account that you can edit and push changes that won’t affect the nasa-openscapes copy.\nGo to https://github.com/nasa-openscapes/2021-Cloud-Hackathon and fork the repository.\n\nNote: if you’ve already done this in the Pre-Hackathon Clinic, you’ll need to make sure you have the latest, following the daily setup instructions.",
    "crumbs": [
      "Tutorials",
      "00. Setup for tutorials"
    ]
  },
  {
    "objectID": "tutorials/00_Setup.html#discussion-python-and-conda-environments",
    "href": "tutorials/00_Setup.html#discussion-python-and-conda-environments",
    "title": "00. Setup for tutorials",
    "section": "Discussion: Python and Conda environments",
    "text": "Discussion: Python and Conda environments\nWhy Python?\n\n\n\nPython Data Stack. Source: Jake VanderPlas, “The State of the Stack,” SciPy Keynote (SciPy 2015).\n\n\nDefault Python Environment:\nWe’ve set up the Python environment with conda.\n\n\n\n\n\n\nConda environment\n\n\n\n\n\nname: openscapes\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.9\n  - pangeo-notebook\n  - awscli~=1.20\n  - boto3~=1.19\n  - gdal~=3.3\n  - rioxarray~=0.8\n  - xarray~=0.19\n  - h5netcdf~=0.11\n  - netcdf4~=1.5\n  - h5py~=2.10\n  - geoviews~=1.9\n  - matplotlib-base~=3.4\n  - hvplot~=0.7\n  - pyproj~=3.2\n  - bqplot~=0.12\n  - geopandas~=0.10\n  - zarr~=2.10\n  - cartopy~=0.20\n  - shapely==1.7.1\n  - pyresample~=1.22\n  - joblib~=1.1\n  - pystac-client~=0.3\n  - s3fs~=2021.7\n  - ipyleaflet~=0.14\n  - sidecar~=0.5\n  - jupyterlab-geojson~=3.1\n  - jupyterlab-git\n  - jupyter-resource-usage\n  - ipympl~=0.6\n  - conda-lock~=0.12\n  - pooch~=1.5\n  - pip\n  - pip:\n    - tqdm\n    - harmony-py\n    - earthdata\n    - zarr-eosdis-store\n\n\n\n\nBash terminal and installed software\nLibraries that are available from the terminal\n\ngdal 3.3 commands ( gdalinfo, gdaltransform…)\nhdf5 commands ( h5dump, h5ls..)\nnetcdf4 commands (ncdump, ncinfo …)\njq (parsing json files or streams from curl)\ncurl (fetch resources from the web)\nawscli (AWS API client, to interact with AWS cloud services)\nvim (editor)\ntree ( directory tree)\nmore …\n\n\n\nUpdating the environment\nScientific Python is a vast space and we only included libraries that are needed in our tutorials. Our default environment can be updated to include any Python library that’s available on pip or conda.\nThe project used to create our default environment is called corn (as it can include many Python kernels).\nIf we want to update a library or install a whole new environment we need to open an issue on this repository. We can help your teams do this during project hacktime.\n\n\ncorn 🌽",
    "crumbs": [
      "Tutorials",
      "00. Setup for tutorials"
    ]
  },
  {
    "objectID": "tutorials/00_Setup.html#step-3.-jupyterhub-orientation",
    "href": "tutorials/00_Setup.html#step-3.-jupyterhub-orientation",
    "title": "00. Setup for tutorials",
    "section": "Step 3. JupyterHub orientation",
    "text": "Step 3. JupyterHub orientation\nNow that the Hub is loaded, let’s get oriented.\n\n\nFirst impressions\n\nLauncher & the big blue button\n“home directory”",
    "crumbs": [
      "Tutorials",
      "00. Setup for tutorials"
    ]
  },
  {
    "objectID": "tutorials/00_Setup.html#step-4.-clone-the-hackathon-github-repository",
    "href": "tutorials/00_Setup.html#step-4.-clone-the-hackathon-github-repository",
    "title": "00. Setup for tutorials",
    "section": "Step 4. Clone the Hackathon GitHub repository",
    "text": "Step 4. Clone the Hackathon GitHub repository\nNow we’ll clone the GitHub repository, using a git extension for the JupyterHub. Go to your github account, and navigate to the repository that you just created by forking from the Openscapes repository.\nClick to copy the url for cloning the repository.\n\nNow, go to JupyterHub and click on the git extension in the left panel and then click the blue button “Clone a Repository”.\n\nThen, paste the repository link to the forked repository that you copied from your github account into the “Clone a repo” pop up window. Then click the blue “CLONE” button. It will take a few moments to clone the repository into your Hub.\nYour link should look like https://github.com/YOUR-USERNAME/2021-Cloud-Hackathon. For example, the link is https://github.com/virdi/2021-Cloud-Hackathon. Note that it include your github username in the repo link.\n\nAlternatively, you can use the terminal (command line) as per github workflows: first-time setup.\nOnce the repository is cloned, you will see a new directory in the “File Browser” panel on the left named “2021-Cloud-Hackathon”. In this directory, you have all hackathon material including the tutorials and this book to follow along during other Tutorials. You are all set.\n\n\nREMEMBER: This is your copy (or fork) of the hackathon materials and jupyter notebooks. So feel free to make any changes to the content of this repository.",
    "crumbs": [
      "Tutorials",
      "00. Setup for tutorials"
    ]
  },
  {
    "objectID": "tutorials/00_Setup.html#jupyter-notebooks",
    "href": "tutorials/00_Setup.html#jupyter-notebooks",
    "title": "00. Setup for tutorials",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nLet’s get oriented to Jupyter notebooks, which we’ll use in all the tutorials.",
    "crumbs": [
      "Tutorials",
      "00. Setup for tutorials"
    ]
  },
  {
    "objectID": "tutorials/00_Setup.html#how-do-i-end-my-session",
    "href": "tutorials/00_Setup.html#how-do-i-end-my-session",
    "title": "00. Setup for tutorials",
    "section": "How do I end my session?",
    "text": "How do I end my session?\n(Also see How do I end my Openscapes session? Will I lose all of my work?)\nWhen you are finished working for the day it is important to explicitly log out of your Openscapes session. The reason for this is it will save us a bit of money! When you keep a session active it uses up AWS resources and keeps a series of virtual machines deployed.\nStopping the server happens automatically when you log out, so navigate to “File -&gt; Log Out” and just click “Log Out”!\n!!! NOTE “logging out” - Logging out will NOT cause any of your work to be lost or deleted. It simply shuts down some resources. It would be equivalent to turning off your desktop computer at the end of the day.",
    "crumbs": [
      "Tutorials",
      "00. Setup for tutorials"
    ]
  },
  {
    "objectID": "tutorials/00_Setup.html#step-5.-tracking-changes-optional",
    "href": "tutorials/00_Setup.html#step-5.-tracking-changes-optional",
    "title": "00. Setup for tutorials",
    "section": "Step 5. Tracking changes (Optional)",
    "text": "Step 5. Tracking changes (Optional)\nNow that you have forked and cloned the repository in your Hub, you can make changes (edit, add, and/or delete content) and track these files using git. In this step, we will provide an overview of how to use git using the graphical interface (the JupyterLab git extension).\n\nStep 5.1. Configure Git (git config)\nConfigure git with your name and email address as shown here.\ngit config --global user.name \"Makhan Virdi\"\ngit config --global user.email \"Makhan.Virdi@gmail.com\"\nOpen a new terminal: File &gt;&gt; New &gt;&gt; Terminal\n\nConfigure git to store your github credentials to avoid having to enter your github username and token each time you push changes to your repository(in Step 5.5, we will describe how to use github token instead of a password)\ngit config --global credential.helper store\n\n\nStep 5.2. Create a new file\nLet’s create a new file: In the left panel on your Hub, click on the “directory” icon and then double click on “2021-Cloud-Hackathon” directory. Then, create a new file using the text editor in your 2i2c JupyterHub (File &gt;&gt; New &gt;&gt; Text File). Add some text to this file, for example: A test file. Save this file and rename it to test.txt.\n\n\n\nStep 5.3. Track the changes to the new file (git add)\nClick the git icon in the left panel. You can see that the newly added file is in the “Untracked” section. You can click the + icon next to the file name to let git track this file for changes.\n\n\n\nStep 5.4. Commit the changes to the new file (git commit)\nNow, you will see that the file is Staged, which means that git is ready to take a snapshot of this file (and the repository) with the changes that you made. This snapshot is called a commit. To commit the changes, add a note (called a commit message) by typing in the text box that say “Summary”.\nNow, click the blue “COMMIT” button to commit this change.\n\nNote: A short message indicating the type of change to this file is a good practice. Optionally, a longer description may be added to the “Description” field.\n\n\n\n\nStep 5.5. Transmit committed changes to your github (git push)\nAt this stage, you have committed the changes to your git repository on your Hub. However, these changes are still on your Hub and needs to be transmitted to your repository on github (so that both the local copy on the JupyterHub and the remote copy on github are in sync).\nAs seen in the picture below, the git extension indicates (with an orange dot on the cloud icon) that it is ready to push your changes to the remote (remote = your repository on github.com). To push to github, click the cloud button with an up arrow (circled in red in the picture).\n\nWhen you push for the first time, it will ask that you input your credentials. You will need to set this up with your Personal Access Token (PAT), explained next.\n\n\nStep 5.6. Setup your Personal Access Token (PAT)\nWhen you see the following screenshot, GitHub is asking for you to input your credentials. (Note: you see this screenshot when you have committed work to push to GitHub.com, as detailed above).\n\nThe git extension in the Hub is prompting you to enter your github.com credentials. Enter you github.com username and a Personal Access Token (PAT); DO NOT use your password.\nTo create a PAT, visit https://github.com/settings/tokens/new and create a new token with the permission as per the image below and specify its validity for 90 days.\n\nIMPORTANT: You will see this token only once, so be sure to copy this. If you do not copy your token at this stage, you will need to generate a new token.\n\nOnce you generate the token, copy it and paste in the Hub window that prompted you to enter the “Personal Access Token”.\n\nGit will show a message at the bottom right telling that the changes were “Successfully pushed”. Also, you will see that the “cloud icon with an up arrow” no longer has an orange dot, indicating that there are no more committed changes to push to the remote (github.com).\n\nNote: You have configured git extension to store your credentials. You will not be prompted for your login/token again!\n\n\nThat’s all. You can use the same workflow (add &gt; commit &gt; push) for any other new or modified files!\n\n\nNote: If you are comfortable with the command line, you can use the Terminal (In Hub, New &gt; Terminal) and follow the steps outlined in the Clinic section.",
    "crumbs": [
      "Tutorials",
      "00. Setup for tutorials"
    ]
  },
  {
    "objectID": "tutorials/03_Xarray.html",
    "href": "tutorials/03_Xarray.html",
    "title": "03. Introduction to xarray",
    "section": "",
    "text": "As Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g. zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors.",
    "crumbs": [
      "Tutorials",
      "03. Introduction to `xarray`"
    ]
  },
  {
    "objectID": "tutorials/03_Xarray.html#why-do-we-need-xarray",
    "href": "tutorials/03_Xarray.html#why-do-we-need-xarray",
    "title": "03. Introduction to xarray",
    "section": "",
    "text": "As Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g. zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors.",
    "crumbs": [
      "Tutorials",
      "03. Introduction to `xarray`"
    ]
  },
  {
    "objectID": "tutorials/03_Xarray.html#what-is-xarray",
    "href": "tutorials/03_Xarray.html#what-is-xarray",
    "title": "03. Introduction to xarray",
    "section": "What is xarray",
    "text": "What is xarray\nxarray is an open-source project and python package to work with labelled multi-dimensional arrays. It is leverages numpy, pandas, matplotlib and dask to build Dataset and DataArray objects with built-in methods to subset, analyze, interpolate, and plot multi-dimensional data. It makes working with multi-dimensional data cubes efficient and fun. It will change your life for the better. You’ll be more attractive, more interesting, and better equiped to take on lifes challenges.",
    "crumbs": [
      "Tutorials",
      "03. Introduction to `xarray`"
    ]
  },
  {
    "objectID": "tutorials/03_Xarray.html#what-you-will-learn-from-this-tutorial",
    "href": "tutorials/03_Xarray.html#what-you-will-learn-from-this-tutorial",
    "title": "03. Introduction to xarray",
    "section": "What you will learn from this tutorial",
    "text": "What you will learn from this tutorial\nIn this tutorial you will learn how to:\n\nload a netcdf file into xarray\ninterrogate the Dataset and understand the difference between DataArray and Dataset\nsubset a Dataset\ncalculate annual and monthly mean fields\ncalculate a time series of zonal means\nplot these results\n\nAs always, we’ll start by importing xarray. We’ll follow convention by giving the module the shortname xr\n\nimport xarray as xr\nxr.set_options(keep_attrs=True)\n\nI’m going to use one of xarray’s tutorial datasets. In this case, air temperature from the NCEP reanalysis. I’ll assign the result of the open_dataset to ds. I may change this to access a dataset directly\n\nds = xr.tutorial.open_dataset(\"air_temperature\")\n\nAs we are in an interactive environment, we can just type ds to see what we have.\n\nds\n\nFirst thing to notice is that ds is an xarray.Dataset object. It has dimensions, lat, lon, and time. It also has coordinate variables with the same names as these dimensions. These coordinate variables are 1-dimensional. This is a NetCDF convention. The Dataset contains one data variable, air. This has dimensions (time, lat, lon).\nClicking on the document icon reveals attributes for each variable. Clicking on the disk icon reveals a representation of the data.\nEach of the data and coordinate variables can be accessed and examined using the variable name as a key.\n\nds.air\n\n\nds['air']\n\nThese are xarray.DataArray objects. This is the basic building block for xarray.\nVariables can also be accessed as attributes of ds.\n\nds.time\n\nA major difference between accessing a variable as an attribute versus using a key is that the attribute is read-only but the key method can be used to update the variable. For example, if I want to convert the units of air from Kelvin to degrees Celsius.\n\nds['air'] = ds.air - 273.15\n\nThis approach can also be used to add new variables\n\nds['air_kelvin'] = ds.air + 273.15\n\nIt is helpful to update attributes such as units, this saves time, confusion and mistakes, especially when you save the dataset.\n\nds['air'].attrs['units'] = 'degC'\n\n\nds",
    "crumbs": [
      "Tutorials",
      "03. Introduction to `xarray`"
    ]
  },
  {
    "objectID": "tutorials/03_Xarray.html#subsetting-and-indexing",
    "href": "tutorials/03_Xarray.html#subsetting-and-indexing",
    "title": "03. Introduction to xarray",
    "section": "Subsetting and Indexing",
    "text": "Subsetting and Indexing\nSubsetting and indexing methods depend on whether you are working with a Dataset or DataArray. A DataArray can be accessed using positional indexing just like a numpy array. To access the temperature field for the first time step, you do the following.\n\nds['air'][0,:,:]\n\nNote this returns a DataArray with coordinates but not attributes.\nHowever, the real power is being able to access variables using coordinate variables. I can get the same subset using the following. (It’s also more explicit about what is being selected and robust in case I modify the DataArray and expect the same output.)\n\nds['air'].sel(time='2013-01-01').time\n\n\nds.air.sel(time='2013-01-01')\n\nI can also do slices. I’ll extract temperatures for the state of Colorado. The bounding box for the state is [-109 E, -102 E, 37 N, 41 N].\nIn the code below, pay attention to both the order of the coordinates and the range of values. The first value of the lat coordinate variable is 41 N, the second value is 37 N. Unfortunately, xarray expects slices of coordinates to be in the same order as the coordinates. Note lon is 0 to 360 not -180 to 180, and I let python calculate it for me within the slice.\n\nds.air.sel(lat=slice(41.,37.), lon=slice(360-109,360-102))\n\nWhat if we want temperature for a point, for example Denver, CO (39.72510678889283 N, -104.98785545855408 E). xarray can handle this! If we just want data from the nearest grid point, we can use sel and specify the method as “nearest”.\n\ndenver_lat, denver_lon = 39.72510678889283, -104.98785545855408\n\n\nds.air.sel(lat=denver_lat, lon=360+denver_lon, method='nearest')\n\nIf we want to interpolate, we can use interp(). In this case I use linear or bilinear interpolation.\ninterp() can also be used to resample data to a new grid and even reproject data\n\nds.air.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\nsel() and interp() can also be used on Dataset objects.\n\nds.sel(lat=slice(41.,37.), lon=slice(360-109,360-102))\n\n\nds.interp(lat=denver_lat, lon=360+denver_lon, method='linear')",
    "crumbs": [
      "Tutorials",
      "03. Introduction to `xarray`"
    ]
  },
  {
    "objectID": "tutorials/03_Xarray.html#analysis",
    "href": "tutorials/03_Xarray.html#analysis",
    "title": "03. Introduction to xarray",
    "section": "Analysis",
    "text": "Analysis\nAs a simple example, let’s try to calculate a mean field for the whole time range.\n\nds.mean(dim='time')\n\nWe can also calculate a zonal mean (averaging over longitude)\n\nds.mean(dim='lon')\n\nOther aggregation methods include min(), max(), std(), along with others.\n\nds.std(dim='time')\n\nThe data we have are in 6h timesteps. This can be resampled to daily or monthly. If you are familiar with pandas, xarray uses the same methods.\n\nds.resample(time='M').mean()\n\n\nds_mon = ds.resample(time='M').mean()\nds_mon\n\nThis is a really short time series but as an example, let’s calculate a monthly climatology (at least for 2 months). For this we can use groupby()\n\nds_clim = ds_mon.groupby(ds_mon.time.dt.month).mean()",
    "crumbs": [
      "Tutorials",
      "03. Introduction to `xarray`"
    ]
  },
  {
    "objectID": "tutorials/03_Xarray.html#plot-results",
    "href": "tutorials/03_Xarray.html#plot-results",
    "title": "03. Introduction to xarray",
    "section": "Plot results",
    "text": "Plot results\nFinally, let’s plot the results! This will plot the lat/lon axes of the original ds DataArray.\n\nds_clim.air.sel(month=10).plot()",
    "crumbs": [
      "Tutorials",
      "03. Introduction to `xarray`"
    ]
  },
  {
    "objectID": "cloud-paradigm.html",
    "href": "cloud-paradigm.html",
    "title": "Cloud Paradigm",
    "section": "",
    "text": "Slides that introduce NASA Earthdata Cloud & the Cloud Paradigm.",
    "crumbs": [
      "Welcome",
      "Cloud Paradigm"
    ]
  },
  {
    "objectID": "projects/hackathon-projects.html",
    "href": "projects/hackathon-projects.html",
    "title": "Hackathon Projects",
    "section": "",
    "text": "On the final day of the Hackathon, 9 awesome teams presented work-in-progress projects they had been hacking on, and we were so excited to see what they had already done and their ongoing momentum. Teams evolved throughout the Hackathon, some joined as a team and welcomed others, and other teams formed and merged throughout the week.\nHackathon participants will have 3 months of continued access to the 2i2c JupyterHub and Slack as they continue experimenting. This way we can all learn more about what the transition to the Cloud looks like (and costs) and support more researchers using NASA Earthdata on the Cloud.\nBelow is brief information about each project team, with links and screenshots from slides and notebooks in GitHub repos that they shared on the final day of the Hackathon.",
    "crumbs": [
      "Projects",
      "Hackathon Projects"
    ]
  },
  {
    "objectID": "projects/hackathon-projects.html#field-campaigns",
    "href": "projects/hackathon-projects.html#field-campaigns",
    "title": "Hackathon Projects",
    "section": "Field Campaigns",
    "text": "Field Campaigns\n\nTeam: Tom Farrar, Kyla Drushka, Bia Villas Boas., Matthew Archer, Kathleen Dohan, Severine Fournier, Eli Hunter, John Wilkin\nPrimary Helpers: Jinbo Wang, Jack McNelis, Ed Armstrong\nSlides; GitHub: https://github.com/NASA-Openscapes/nch2021-projects-contexdata\nProject Goals: Extract and visualize multiple data sets that can be used to give context to field campaigns or other regional events (e.g., the “Warm Blob” or the recent atmospheric river event on the West Coast). For example, choose a target region and time period, cycle through all available high-resolution sea surface temperature data, identify clear images, catalog them. Extract wind, wave, sea surface height, salinity data.",
    "crumbs": [
      "Projects",
      "Hackathon Projects"
    ]
  },
  {
    "objectID": "projects/hackathon-projects.html#global-upwelling-patterns",
    "href": "projects/hackathon-projects.html#global-upwelling-patterns",
    "title": "Hackathon Projects",
    "section": "Global Upwelling Patterns",
    "text": "Global Upwelling Patterns\n\nTeam: Eli Holmes, Jorge Vazquez\nPrimary Helpers: Aaron Friesz, Andy Barrett, Jack McNelis\nSlides, GitHub: https://github.com/NASA-Openscapes/nhw21-projects-upwelling, Notebook: http://nbviewer.org/github/NASA-Openscapes/nhw21-projects-upwelling/blob/main/notebooks/05-Multi-year-upi-plots.ipynb\nProject Goals: Climate change is predicted to change coastal upwelling. This study explores the use of simple SST metrics for studying upwelling signatures.\n\nAlso: Running the NASA Cloud Workshop notebooks with mybinder.org - by Eli Holmes, 2021 Cloud Hackathon Participant who then set up working in Binder",
    "crumbs": [
      "Projects",
      "Hackathon Projects"
    ]
  },
  {
    "objectID": "projects/hackathon-projects.html#surface-body-water",
    "href": "projects/hackathon-projects.html#surface-body-water",
    "title": "Hackathon Projects",
    "section": "Surface Body Water",
    "text": "Surface Body Water\n\nTeam: Matthew Bonnema, Maximilian Parzen\nSlides, GitHub: https://github.com/pz-max/NASA_projectX\nProject Goals: Estimate surface area of surface water body and track changes over time. Test Case: Oroville Reservoir, California",
    "crumbs": [
      "Projects",
      "Hackathon Projects"
    ]
  },
  {
    "objectID": "projects/hackathon-projects.html#pypsicle-cryo-computing-in-the-cloud",
    "href": "projects/hackathon-projects.html#pypsicle-cryo-computing-in-the-cloud",
    "title": "Hackathon Projects",
    "section": "Pypsicle: cryo computing in the cloud",
    "text": "Pypsicle: cryo computing in the cloud\n\nTeam: Chad Greene, Susan Howard, Luis Lopez, Cassandra Nickles, Christine Sadlik, Jessica Scheick\nPrimary Helpers: Luis Lopez\nSlides, GitHub: https://github.com/NASA-Openscapes/nch2021-projects-pypsicle, Notebook:  https://nbviewer.org/github/NASA-Openscapes/nch2021-projects-pypsicle/blob/main/S3-h5_example_rendered.ipynb\nProject Goals: Selectively read in part of an hdf5 file, explore a dataset on the cloud",
    "crumbs": [
      "Projects",
      "Hackathon Projects"
    ]
  },
  {
    "objectID": "projects/hackathon-projects.html#query-stact-for-hls-data-and-load-as-a-time-series",
    "href": "projects/hackathon-projects.html#query-stact-for-hls-data-and-load-as-a-time-series",
    "title": "Hackathon Projects",
    "section": "Query stact for HLS data and load as a time series",
    "text": "Query stact for HLS data and load as a time series\n\nTeam: Todd Hawbaker, Jodi Riegle, and Kehan Yang\nSlides; GitHub: https://github.com/tjhawbaker/nch21_hls_timeseries\nProject Goals: A cloud-deployable framework to identify burned areas in the harmonized Landsat Sentinel-2 data, and possibly other remotely sensed data sources. Basically, extending what we’ve done with Landsat to HLS data (Hawbaker et. al., 2020). The end product we are planning is code or a container that users could run in the cloud to generate a time series of burned area products for their area of interest.",
    "crumbs": [
      "Projects",
      "Hackathon Projects"
    ]
  },
  {
    "objectID": "projects/hackathon-projects.html#drought-monitor",
    "href": "projects/hackathon-projects.html#drought-monitor",
    "title": "Hackathon Projects",
    "section": "Drought Monitor",
    "text": "Drought Monitor\n\nTeam: Colin Brust, Arthur Endsley\nProject Goals: Build a data pipeline library that clips, reprojects and aggregates a given dataset to a desired spatial and temporal resolution. Then, use this library to feed GHRSST data into a neural network that forecasts changes in the U.S. Drought Monitor.",
    "crumbs": [
      "Projects",
      "Hackathon Projects"
    ]
  },
  {
    "objectID": "projects/hackathon-projects.html#county-temperatures",
    "href": "projects/hackathon-projects.html#county-temperatures",
    "title": "Hackathon Projects",
    "section": "County Temperatures",
    "text": "County Temperatures\n\nTeam: Binita KC, Xiaohua Pan\nPrimary Helpers: Alexis Hunzinger\nGitHub: https://github.com/NASA-Openscapes/nch21-projects-counties-temp, Notebook: https://github.com/NASA-Openscapes/nch21-projects-counties-temp/blob/main/Project9.ipynb\nProject Goals: Read the hourly MERRA-2 data from the GES DISC S3 bucket, subset by temperature and calculate the monthly average, ideally extract zonal mean using county shapefile",
    "crumbs": [
      "Projects",
      "Hackathon Projects"
    ]
  },
  {
    "objectID": "projects/hackathon-projects.html#retrieval-of-pre-swot-data",
    "href": "projects/hackathon-projects.html#retrieval-of-pre-swot-data",
    "title": "Hackathon Projects",
    "section": "Retrieval of Pre SWOT data",
    "text": "Retrieval of Pre SWOT data\n\nTeam: Prof. J. Indu, Kaushlendra V., Nitish K., Manu K.S., Girish P.\nSlides\nProject Goals: To access multi-mission altimetry data sets and merge them to generate high temporal resolution altimetry data",
    "crumbs": [
      "Projects",
      "Hackathon Projects"
    ]
  },
  {
    "objectID": "tutorials-templates/index.html",
    "href": "tutorials-templates/index.html",
    "title": "Notebooks for live-coding",
    "section": "",
    "text": "Templates of all tutorials — .ipynb notebooks with code removed — are available at:\nhttps://github.com/NASA-Openscapes/2021-Cloud-Hackathon/tree/main/tutorials-templates.\nPlease open these tutorial templates in our JupyterHub to follow along and live-code with the tutorial lead.",
    "crumbs": [
      "Tutorial Templates",
      "Notebooks for live-coding"
    ]
  },
  {
    "objectID": "tutorials-templates/08_On-Prem_Cloud.html#timing",
    "href": "tutorials-templates/08_On-Prem_Cloud.html#timing",
    "title": "08. Pairing Cloud and non-Cloud Data",
    "section": "Timing",
    "text": "Timing\n\nExercise: 45 min"
  },
  {
    "objectID": "tutorials-templates/08_On-Prem_Cloud.html#summary",
    "href": "tutorials-templates/08_On-Prem_Cloud.html#summary",
    "title": "08. Pairing Cloud and non-Cloud Data",
    "section": "Summary",
    "text": "Summary\nThis tutorial will combine several workflow steps and components from the previous days, demonstrating the process of using the geolocation of data available outside of the Earthdata Cloud to then access coincident variables of cloud-accessible data. This may be a common use case as NASA Earthdata continues to migrate to the cloud, producing a “hybrid” data archive across Amazon Web Services (AWS) and original on-premise data storage systems. Additionally, you may also want to combine field measurements with remote sensing data available on the Earthdata Cloud.\nThis specific example explores the pairing of the ICESat-2 ATL07 Sea Ice Height data product, currently (as of November 2021) available publicly via direct download at the NSIDC DAAC, along with Sea Surface Temperature (SST) from the GHRSST MODIS L2 dataset (MODIS_A-JPL-L2P-v2019.0) available from PO.DAAC on the Earthdata Cloud.\nThe use case we’re looking at today centers over an area north of Greenland for a single day in June, where a melt pond was observed using the NASA OpenAltimetry application. Melt ponds are an important feature of Arctic sea ice dynamics, leading to an decrease in sea ice albedo and other changes in heat balance. Many NASA Earthdata datasets produce variables including sea ice albedo, sea surface temperature, air temperature, and sea ice height, which can be used to better understand these dynamics.\n\nObjectives\n\nPractice skills searching for data in CMR, determining granule coverage across two datasets over an area of interest.\nDownload data from an on-premise storage system to our cloud environment.\nRead in 1-dimensional trajectory data (ICESat-2 ATL07) into xarray and perform attribute conversions.\nSelect and read in sea surface temperature (SST) data (MODIS_A-JPL-L2P-v2019.0) from the Earthdata Cloud into xarray.\nExtract, resample, and plot coincident SST data based on ICESat-2 geolocation."
  },
  {
    "objectID": "tutorials-templates/08_On-Prem_Cloud.html#import-packages",
    "href": "tutorials-templates/08_On-Prem_Cloud.html#import-packages",
    "title": "08. Pairing Cloud and non-Cloud Data",
    "section": "Import packages",
    "text": "Import packages\n\n#"
  },
  {
    "objectID": "tutorials-templates/08_On-Prem_Cloud.html#specify-data-time-range-and-area-of-interest",
    "href": "tutorials-templates/08_On-Prem_Cloud.html#specify-data-time-range-and-area-of-interest",
    "title": "08. Pairing Cloud and non-Cloud Data",
    "section": "Specify data, time range, and area of interest",
    "text": "Specify data, time range, and area of interest\nWe are going to focus on getting data for an area north of Greenland for a single day in June.\nThese bounding_box and temporal variables will be used for data search, subset, and access below:\n\n#\n\nSince we’ve already demonstrated how to locate a dataset’s collection_id and use the cloud_hosted parameter to determine whether a dataset resides in the Earthdata Cloud, we are going to skip forward and declare these variables:\n\n#"
  },
  {
    "objectID": "tutorials-templates/08_On-Prem_Cloud.html#search-and-download-icesat-2-atl07-files",
    "href": "tutorials-templates/08_On-Prem_Cloud.html#search-and-download-icesat-2-atl07-files",
    "title": "08. Pairing Cloud and non-Cloud Data",
    "section": "Search and download ICESat-2 ATL07 files",
    "text": "Search and download ICESat-2 ATL07 files\nPerform a granule search over our time and area of interest. How many granules are returned?\n\n#\n\n\n#\n\nPrint the file names, size, and links:\n\n#\n\n\nDownload ATL07 files\nAlthough several services are supported for ICESat-2 data, we are demonstrating direct access through the “on-prem” file system at NSIDC for simplicity.\nSome of these services include: - icepyx - From the icepyx documentation: “icepyx is both a software library and a community composed of ICESat-2 data users, developers, and the scientific community. We are working together to develop a shared library of resources - including existing resources, new code, tutorials, and use-cases/examples - that simplify the process of querying, obtaining, analyzing, and manipulating ICESat-2 datasets to enable scientific discovery.” - NSIDC DAAC Data Access and Service API - The API provided by the NSIDC DAAC allows you to access data programmatically using specific temporal and spatial filters. The same subsetting, reformatting, and reprojection services available on select data sets through NASA Earthdata Search can also be applied using this API. - IceFlow - The IceFlow python library simplifies accessing and combining data from several of NASA’s cryospheric altimetry missions, including ICESat/GLAS, Operation IceBridge, and ICESat-2. In particular, IceFlow harmonizes the various file formats and georeferencing parameters across several of the missions’ data sets, allowing you to analyze data across the multi-decadal time series.\nWe’ve found 2 granules. We’ll download the first one and write it to a file with the same name as the producer_granule_id.\nWe need the url for the granule as well. This is href links we printed out above.\n\n#\n\nTo retrieve the granule data, we use the requests.get() method, which will utilize the .netrc file on the backend to authenticate the request against Earthdata Login.\n\n#\n\nThe response returned by requests has the same structure as all the other responses: a header and contents. The header information has information about the response, including the size of the data we downloaded in bytes.\n\n#\n\nThe contents needs to be saved to a file. To keep the directory clean, we will create a downloads directory to store the file. We can use a shell command to do this or use the makedirs method from the os package.\n\n#\n\nYou should see a downloads directory in the file browser.\nTo write the data to a file, we use open to open a file. We need to specify that the file is open for writing by using the write-mode w. We also need to specify that we want to write bytes by setting the binary-mode b. This is important because the response contents are bytes. The default mode for open is text-mode. So make sure you use b.\nWe’ll use the with statement context-manager to open the file, write the contents of the response, and then close the file. Once the data in r.content is written sucessfully to the file, or if there is an error, the file is closed by the context-manager.\nWe also need to prepend the downloads path to the filename. We do this using Path from the pathlib package in the standard library.\n\n#\n\n\n#\n\nATL07-01_20190622055317_12980301_004_01.h5 is an HDF5 file. xarray can open this but you need to tell it which group to read the data from. In this case we read the sea ice segment height data for ground-track 1 left-beam. You can explore the variable hierarchy in Earthdata Search, by selecting the Customize option under Download Data.\nThis code block performs the following operations: - Extracts the height_segment_height variable from the heights group, along with the dimension variables contained in the higher level sea_ice_segments group, - Convert attributes from bytestrings to strings, - Drops the HDF attribute DIMENSION_LIST, - Sets _FillValue to NaN\n\n#\n\n\n#"
  },
  {
    "objectID": "tutorials-templates/08_On-Prem_Cloud.html#determine-the-ghrsst-modis-l2-granules-returned-from-our-time-and-area-of-interest",
    "href": "tutorials-templates/08_On-Prem_Cloud.html#determine-the-ghrsst-modis-l2-granules-returned-from-our-time-and-area-of-interest",
    "title": "08. Pairing Cloud and non-Cloud Data",
    "section": "Determine the GHRSST MODIS L2 granules returned from our time and area of interest",
    "text": "Determine the GHRSST MODIS L2 granules returned from our time and area of interest\n\n#\n\n\n#"
  },
  {
    "objectID": "tutorials-templates/08_On-Prem_Cloud.html#load-data-into-xarray-via-s3-direct-access",
    "href": "tutorials-templates/08_On-Prem_Cloud.html#load-data-into-xarray-via-s3-direct-access",
    "title": "08. Pairing Cloud and non-Cloud Data",
    "section": "Load data into xarray via S3 direct access",
    "text": "Load data into xarray via S3 direct access\nOur CMR granule search returned 14 files for our time and area of interest. However, not all granules will be suitable for analysis.\nI’ve identified the image with granule id G1956158784-POCLOUD as a good candidate, this is the 9th granule. In this image, our area of interest is close to nadir. This means that the instantaneous field of view over the area of interest cover a smaller area than at the edge of the image.\nWe are looking for the link for direct download access via s3. This is a url but with a prefix s3://. This happens to be the first href link in the metadata.\nFor a single granule we can cut and paste the s3 link. If we have several granules, the s3 links can be extracted with some simple code.\n\n#\n\n\nGet S3 credentials\nAs with the previous S3 download tutorials we need credentials to access data from s3: access keys and tokens.\n\n#\n\nEssentially, what we are doing in this step is to “mount” the s3 bucket as a file system. This allows us to treat the S3 bucket in a similar way to a local file system.\n\n#\n\n\n\nOpen a s3 file\nNow we have the S3FileSystem set up, we can access the granule. xarray cannot open a S3File directly, so we use the open method for the S3FileSystem to open the granule using the endpoint url we extracted from the metadata. We also have to set the mode='rb'. This opens the granule in read-only mode and in byte-mode. Byte-mode is important. By default, open opens a file as text - in this case it would just be a string of characters - and xarray doesn’t know what to do with that.\nWe then pass the S3File object f to xarray.open_dataset. For this dataset, we also have to set decode_cf=False. This switch tells xarray not to use information contained in variable attributes to generate human readable coordinate variables. Normally, this should work for netcdf files but for this particular cloud-hosted dataset, variable attribute data is not in the form expected by xarray. We’ll fix this.\n\n#\n\nIf you click on the Show/Hide Attributes icon (the first document-like icon to the right of coordinate variable metadata) you can see that attributes are one-element arrays containing bytestrings.\n\n#\n\nTo fix this, we need to extract array elements as scalars, and convert those scalars from bytestrings to strings. We use the decode method to do this. The bytestrings are encoded as utf-8, which is a unicode character format. This is the default encoding for decode but we’ve included it as an argument to be explicit.\nNot all attributes are bytestrings. Some are floats. Take a look at _FillValue, and valid_min and valid_max. To avoid an error, we use the isinstance function to check if the value of an attributes is type bytes - a bytestring. If it is, then we decode it. If not, we just extract the scalar and do nothing else.\nWe also fix the global attributes.\n\n#\n\nWith this done, we can use the xarray function decode_cf to convert the attributes.\n\n#\n\n\n#\n\nLet’s make a quick plot to take a look at the sea_surface_temperature variable.\n\n#\n\n\n\nPlot MODIS and ICESat-2 data on a map\n\n#\n\n\n\nExtract SST coincident with ICESat-2 track\nThe MODIS SST is swath data, not a regularly-spaced grid of sea surface temperatures. ICESat-2 sea surface heights are irregularly spaced segments along one ground-track traced by the ATLAS instrument on-board ICESat-2. Fortunately, pyresample allows us to resample swath data.\npyresample has many resampling methods. We’re going to use the nearest neighbour resampling method, which is implemented using a k-dimensional tree algorithm or K-d tree. K-d trees are data structures that improve search efficiency for large data sets.\nThe first step is to define the geometry of the ICESat-2 and MODIS data. To do this we use the latitudes and longitudes of the datasets.\n\n#\n\n\n#\n\nWe then implement the resampling method, passing the two geometries we have defined, the data array we want to resample - in this case sea surface temperature, and a search radius. The resampling method expects a numpy.Array rather than an xarray.DataArray, so we use values to get the data as a numpy.Array.\nWe set the search radius to 1000 m. The MODIS data is nominally 1km spacing.\n\n#\n\n\n#\n\n\n#\n\n\n\nPlot SST and Height along track\nThis is a quick plot of the extracted data. We’re using matplotlib so we can use latitude as the x-value:\n\n#"
  },
  {
    "objectID": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html",
    "href": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "",
    "text": "Exercise: 30 min"
  },
  {
    "objectID": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#timing",
    "href": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#timing",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "",
    "text": "Exercise: 30 min"
  },
  {
    "objectID": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#summary",
    "href": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#summary",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "Summary",
    "text": "Summary\nIn this example we will access the NASA’s Harmonized Landsat Sentinel-2 (HLS) version 2 assets, which are archived in cloud optimized geoTIFF (COG) format in the LP DAAC Cumulus cloud space. The COGs can be used like any other geoTIFF file, but have some added features that make them more efficient within the cloud data access paradigm. These features include: overviews and internal tiling. Below we will demonstrate how to leverage these features.\n\nBut first, what is STAC?\nSpatioTemporal Asset Catalog (STAC) is a specification that provides a common language for interpreting geospatial information in order to standardize indexing and discovering data.\nThe STAC specification is made up of a collection of related, yet independent specifications that when used together provide search and discovery capabilities for remote assets.\n\nFour STAC Specifications\nSTAC Catalog (aka DAAC Archive)\nSTAC Collection (aka Data Product)\nSTAC Item (aka Granule)\nSTAC API\nIn the following sections, we will explore each of STAC element using NASA’s Common Metadata Repository (CMR) STAC application programming interface (API), or CMR-STAC API for short.\n\n\n\nCMR-STAC API\nThe CMR-STAC API is NASA’s implementation of the STAC API specification for all NASA data holdings within EOSDIS. The current implementation does not allow for querries accross the entire NASA catalog. Users must execute searches within provider catalogs (e.g., LPCLOUD) to find the STAC Items they are searching for. All the providers can be found at the CMR-STAC endpoint here: https://cmr.earthdata.nasa.gov/stac/.\nIn this exercise, we will query the LPCLOUD provider to identify STAC Items from the Harmonized Landsat Sentinel-2 (HLS) collection that fall within our region of interest (ROI) and within our specified time range."
  },
  {
    "objectID": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#what-you-will-learn-from-this-tutorial",
    "href": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#what-you-will-learn-from-this-tutorial",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "What you will learn from this tutorial",
    "text": "What you will learn from this tutorial\n\nhow to connect to NASA CMR-STAC API using Python’s pystac-client\n\nhow to navigate CMR-STAC records\n\nhow to read in a geojson file using geopandas to specify your region of interest\nhow to use the CMR-STAC API to search for data\nhow to perform post-search filtering of CMR-STAC API search result in Python\n\nhow to extract and save data access URLs for geospatial assets\n\nThis exercise can be found in the 2021 Cloud Hackathon Book"
  },
  {
    "objectID": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#import-required-packages",
    "href": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#import-required-packages",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "Import Required Packages",
    "text": "Import Required Packages\n\n#"
  },
  {
    "objectID": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#explored-available-nasa-providers",
    "href": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#explored-available-nasa-providers",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "Explored available NASA Providers",
    "text": "Explored available NASA Providers\n\n#\n\n\nConnect to the CMR-STAC API\n\n#\n\nWe’ll create a providers variable so we can take a deeper look into available data providers - subcategories are referred to as “children”. We can then print them as a for loop.\n\n#"
  },
  {
    "objectID": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#connect-to-the-lpcloud-providerstac-catalog",
    "href": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#connect-to-the-lpcloud-providerstac-catalog",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "Connect to the LPCLOUD Provider/STAC Catalog",
    "text": "Connect to the LPCLOUD Provider/STAC Catalog\nFor this next step we need the provider title (e.g., LPCLOUD) from above. We will add the provider to the end of the CMR-STAC API URL (i.e., https://cmr.earthdata.nasa.gov/stac/) to connect to the LPCLOUD STAC Catalog.\n\n#\n\nSince we are using a dedicated client (i.e., pystac-client.Client) to connect to our STAC Provider Catalog, we will have access to some useful internal methods and functions (e.g., get_children() or get_all_items()) we can use to get information from these objects."
  },
  {
    "objectID": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#list-stac-collections",
    "href": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#list-stac-collections",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "List STAC Collections",
    "text": "List STAC Collections\nWe’ll create a products variable to view deeper in the STAC Catalog.\n\n#\n\n\nPrint one of the STAC Collection records\nTo view the products variable we just created, let’s look at one entry as a dictionary.\n\n#\n\n\n\nPrint the STAC Collection ids with their title\nIn the above output, id and title are two elements of interest that we can print for all products using a for loop.\n\n#"
  },
  {
    "objectID": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#search-for-granulesstac-items---set-up-query-parameters-to-submit-to-the-cmr-stac-api",
    "href": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#search-for-granulesstac-items---set-up-query-parameters-to-submit-to-the-cmr-stac-api",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "Search for Granules/STAC Items - Set up query parameters to submit to the CMR-STAC API",
    "text": "Search for Granules/STAC Items - Set up query parameters to submit to the CMR-STAC API\nWe will define our ROI using a geojson file containing a small polygon feature in western Nebraska, USA. The geojson file is found in the ~/data directory. We’ll also specify the data collections and a time range for our example.\n\nRead in a geojson file\nReading in a geojson file with geopandas will return the geometry of our polygon (our ROI).\nNOTE: If you are running the notebook from the tutorials-templates directory, please use the following path to connect to the geojson file: “../tutorials/data/ne_w_agfields.geojson”\n\n#\n\n\n\nVisualize contents of geojson file\nWe can use that geometry to visualize the polygon: here, a square. But wait for it –\n\n#\n\nWe can plot the polygon using the geoviews package that we imported as gv with ‘bokeh’ and ‘matplotlib’ extensions. The following has reasonable width, height, color, and line widths to view our polygon when it is overlayed on a base tile map.\n\n#\n\nWe will now start to specify the search criteria we are interested in, i.e, the date range, the ROI, and the data collections, that we will pass to the STAC API.\n\n\nExtract the coordinates for the ROI\n\n#\n\nSo, what just happen there? Let’s take a quick detour to break it down.\n\n\n\nSpecify date range\nNext up is to specify our date range using ISO_8601 date formatting.\n\n#\n\n\n\nSpecify the STAC Collections\nSTAC Collection is synonomous with what we usually consider a NASA data product. Desired STAC Collections are submitted to the search API as a list containing the collection id. We can use the ids that we printed from our products for loop above. Let’s focus on S30 and L30 collections.\n\n#"
  },
  {
    "objectID": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#search-the-cmr-stac-api-with-our-search-criteria",
    "href": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#search-the-cmr-stac-api-with-our-search-criteria",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "Search the CMR-STAC API with our search criteria",
    "text": "Search the CMR-STAC API with our search criteria\nNow we can put all our search criteria together using catalog.search from the pystac_client package.\n\n#\n\n\nPrint out how many STAC Items match our search query\n\n#\n\nWe now have a search object containing the STAC Items that matched our query. Now, let’s pull out all of the STAC Items (as a PySTAC ItemCollection object) and explore the contents (i.e., the STAC Items)\n\n#\n\nLet’s list some of the Items from our pystac item_collection:\n\n#\n\nWe can view a single Item as a dictionary, as we did above with STAC Collections/products.\n\n#"
  },
  {
    "objectID": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#filtering-stac-items",
    "href": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#filtering-stac-items",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "Filtering STAC Items",
    "text": "Filtering STAC Items\nWhile the CMR-STAC API is a powerful search and discovery utility, it is still maturing and currently does not have the full gamut of filtering capabilities that the STAC API specification allows for. Hence, additional filtering is required if we want to filter by a property, for example cloud cover. Below we will loop through and filter the item_collection by a specified cloud cover as well as extract the band we’d need to do an Enhanced Vegetation Index (EVI) calculation for a future analysis.\nWe’ll make a cloudcover variable where we will set the maximum allowable cloud cover and extract the band links for those Items that match or are less than the max cloud cover.\n\n#\n\nWe will also specify the STAC Assets (i.e., bands/layers) of interest for both the S30 and L30 collections (also in our collections variable above).\nIn this hypothetical workflow, we’ll extract the bands needed to calculate an enhanced vegetation index (EVI). Thus, the band needed include red, near infrared (NIR), and blue. We’ll also extract a quality band (i.e., Fmask) that we’d eventually use to perform per-pixel quality filtering.\nNotice that the band ids are in some case not one-to-one between the S30 and the L30 product. This is evident in the NIR band for each product where S30’s NIR band id is B8A and L30’s is B05. Note, the S30 product has an additional NIR band with a band id of B08, but the spectral ranges between B8A and B05 are more closely aligned. Visit the HLS Overview page to learn more about HLS spectral bands.\n\n#\n\nAnd now to loop through and filter the item_collection by cloud cover and bands:\n\n#\n\nThe filtering done in the previous steps produces a list of links to STAC Assets. Let’s print out the first ten links.\n\n#\n\nNOTE that HLS data is mapped to the Universal Transverse Mercator (UTM) projection and is tiled using the Sentinel-2 Military Grid Reference System (MGRS) UTM grid. Notice that in the list of links we have multiple tiles, i.e. T14TKL & T13TGF, that intersect with our region of interest. In this case, these two tiles represent neighboring UTM zones. The tiles can be discern from the file name, which is the last element in a link (far right) following the last forward slash (/) - e.g., HLS.L30.T14TKL.2021133T172406.v1.5.B04.tif. The figure below explains where to find the tile/UTM zone from the file name.\n\nWe will now split the list of links into separate logical sub-lists."
  },
  {
    "objectID": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#split-data-links-list-into-logical-groupings",
    "href": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#split-data-links-list-into-logical-groupings",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "Split Data Links List into Logical Groupings",
    "text": "Split Data Links List into Logical Groupings\nWe have a list of links to data assets that meet our search and filtering criteria. Below we’ll split our list from above into lists first by tile/UTM zone and then further by individual bands bands. The commands that follow will do the splitting with python routines.\n\nSplit by UTM tile specified in the file name (e.g., T14TKL & T13TGF)\n\n#\n\n\n#\n\n\nPrint dictionary keys and values, i.e. the data links\n\n#\n\n\n#\n\nNow we will create a separate list of data links for each tile\n\n#\n\n\n\nPrint band/layer links for HLS tile T13TGF\n\n#\n\n\n\n\nSplit the links by band\n\n#\n\n\n#\n\n\n#\n\n\n#"
  },
  {
    "objectID": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#save-links-to-a-text-file",
    "href": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#save-links-to-a-text-file",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "Save links to a text file",
    "text": "Save links to a text file\nTo complete this exercise, we will save the individual link lists as separate text files with descriptive names.\nNOTE: If you are running the notebook from the tutorials-templates directory, please use the following path to write to the data directory: “../tutorials/data/{name}”\n\nWrite links from CMR-STAC API to a file\n\n#\n\n\n\nWrite links to file for S3 access\n\n#"
  },
  {
    "objectID": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#resources",
    "href": "tutorials-templates/02_Data_Discovery_CMR-STAC_API.html#resources",
    "title": "02. Data Discovery with CMR-STAC API",
    "section": "Resources",
    "text": "Resources\n\nSTAC Specification Webpage\nSTAC API Documentation\nCMR-STAC API Github\nPySTAC Client Documentation\nhttps://stackoverflow.com/questions/26367812/appending-to-list-in-python-dictionary\nGeopandas\nHLS Overview"
  },
  {
    "objectID": "tutorials-templates/04_NASA_Earthdata_Authentication.html",
    "href": "tutorials-templates/04_NASA_Earthdata_Authentication.html",
    "title": "04. Authentication for NASA Earthdata",
    "section": "",
    "text": "This notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin &lt;USERNAME&gt;\npassword &lt;PASSWORD&gt;\n&lt;USERNAME&gt; and &lt;PASSWORD&gt; would be replaced by your actual Earthdata Login username and password respectively."
  },
  {
    "objectID": "tutorials-templates/04_NASA_Earthdata_Authentication.html#summary",
    "href": "tutorials-templates/04_NASA_Earthdata_Authentication.html#summary",
    "title": "04. Authentication for NASA Earthdata",
    "section": "",
    "text": "This notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin &lt;USERNAME&gt;\npassword &lt;PASSWORD&gt;\n&lt;USERNAME&gt; and &lt;PASSWORD&gt; would be replaced by your actual Earthdata Login username and password respectively."
  },
  {
    "objectID": "tutorials-templates/04_NASA_Earthdata_Authentication.html#import-required-packages",
    "href": "tutorials-templates/04_NASA_Earthdata_Authentication.html#import-required-packages",
    "title": "04. Authentication for NASA Earthdata",
    "section": "Import Required Packages",
    "text": "Import Required Packages\n\n#\n\nThe code below will:\n\ncheck what operating system (OS) is being used to determine which netrc file to check for/create (.netrc or _netrc)\ncheck if you have an netrc file, and if so, varify if those credentials are for the Earthdata endpoint\ncreate a netrc file if a netrc file is not present.\n\n\n#\n\n\nSee if the file was created\nIf the file was created, we’ll see a .netrc file (_netrc for Window OS) in the list printed below. To view the contents from a Jupyter environment, click File on the top toolbar, select Open from Path…, type .netrc, and click Open. The .netrc file will open within the text editor.\n\n!!! Beware, your password will be visible if the .netrc file is opened in the text editor.\n\n\n#"
  },
  {
    "objectID": "tutorials-templates/03_Xarray.html",
    "href": "tutorials-templates/03_Xarray.html",
    "title": "03. Introduction to xarray",
    "section": "",
    "text": "As Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g. zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors."
  },
  {
    "objectID": "tutorials-templates/03_Xarray.html#why-do-we-need-xarray",
    "href": "tutorials-templates/03_Xarray.html#why-do-we-need-xarray",
    "title": "03. Introduction to xarray",
    "section": "",
    "text": "As Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g. zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors."
  },
  {
    "objectID": "tutorials-templates/03_Xarray.html#what-is-xarray",
    "href": "tutorials-templates/03_Xarray.html#what-is-xarray",
    "title": "03. Introduction to xarray",
    "section": "What is xarray",
    "text": "What is xarray\nxarray is an open-source project and python package to work with labelled multi-dimensional arrays. It is leverages numpy, pandas, matplotlib and dask to build Dataset and DataArray objects with built-in methods to subset, analyze, interpolate, and plot multi-dimensional data. It makes working with multi-dimensional data cubes efficient and fun. It will change your life for the better. You’ll be more attractive, more interesting, and better equiped to take on lifes challenges."
  },
  {
    "objectID": "tutorials-templates/03_Xarray.html#what-you-will-learn-from-this-tutorial",
    "href": "tutorials-templates/03_Xarray.html#what-you-will-learn-from-this-tutorial",
    "title": "03. Introduction to xarray",
    "section": "What you will learn from this tutorial",
    "text": "What you will learn from this tutorial\nIn this tutorial you will learn how to:\n\nload a netcdf file into xarray\ninterrogate the Dataset and understand the difference between DataArray and Dataset\nsubset a Dataset\ncalculate annual and monthly mean fields\ncalculate a time series of zonal means\nplot these results\n\nAs always, we’ll start by importing xarray. We’ll follow convention by giving the module the shortname xr\n\n#\n\nI’m going to use one of xarray’s tutorial datasets. In this case, air temperature from the NCEP reanalysis. I’ll assign the result of the open_dataset to ds. I may change this to access a dataset directly\n\n#\n\nAs we are in an interactive environment, we can just type ds to see what we have.\n\n#\n\nFirst thing to notice is that ds is an xarray.Dataset object. It has dimensions, lat, lon, and time. It also has coordinate variables with the same names as these dimensions. These coordinate variables are 1-dimensional. This is a NetCDF convention. The Dataset contains one data variable, air. This has dimensions (time, lat, lon).\nClicking on the document icon reveals attributes for each variable. Clicking on the disk icon reveals a representation of the data.\nEach of the data and coordinate variables can be accessed and examined using the variable name as a key.\n\n#\n\n\n#\n\nThese are xarray.DataArray objects. This is the basic building block for xarray.\nVariables can also be accessed as attributes of ds.\n\n#\n\nA major difference between accessing a variable as an attribute versus using a key is that the attribute is read-only but the key method can be used to update the variable. For example, if I want to convert the units of air from Kelvin to degrees Celsius.\n\n#\n\nThis approach can also be used to add new variables\n\n#\n\nIt is helpful to update attributes such as units, this saves time, confusion and mistakes, especially when you save the dataset.\n\n#\n\n\n#"
  },
  {
    "objectID": "tutorials-templates/03_Xarray.html#subsetting-and-indexing",
    "href": "tutorials-templates/03_Xarray.html#subsetting-and-indexing",
    "title": "03. Introduction to xarray",
    "section": "Subsetting and Indexing",
    "text": "Subsetting and Indexing\nSubsetting and indexing methods depend on whether you are working with a Dataset or DataArray. A DataArray can be accessed using positional indexing just like a numpy array. To access the temperature field for the first time step, you do the following.\n\n#\n\nNote this returns a DataArray with coordinates but not attributes.\nHowever, the real power is being able to access variables using coordinate variables. I can get the same subset using the following. (It’s also more explicit about what is being selected and robust in case I modify the DataArray and expect the same output.)\n\n#\n\n\n#\n\nI can also do slices. I’ll extract temperatures for the state of Colorado. The bounding box for the state is [-109 E, -102 E, 37 N, 41 N].\nIn the code below, pay attention to both the order of the coordinates and the range of values. The first value of the lat coordinate variable is 41 N, the second value is 37 N. Unfortunately, xarray expects slices of coordinates to be in the same order as the coordinates. Note lon is 0 to 360 not -180 to 180, and I let python calculate it for me within the slice.\n\n#\n\nWhat if we want temperature for a point, for example Denver, CO (39.72510678889283 N, -104.98785545855408 E). xarray can handle this! If we just want data from the nearest grid point, we can use sel and specify the method as “nearest”.\n\n#\n\n\n#\n\nIf we want to interpolate, we can use interp(). In this case I use linear or bilinear interpolation.\ninterp() can also be used to resample data to a new grid and even reproject data\n\n#\n\nsel() and interp() can also be used on Dataset objects.\n\n#\n\n\n#"
  },
  {
    "objectID": "tutorials-templates/03_Xarray.html#analysis",
    "href": "tutorials-templates/03_Xarray.html#analysis",
    "title": "03. Introduction to xarray",
    "section": "Analysis",
    "text": "Analysis\nAs a simple example, let’s try to calculate a mean field for the whole time range.\n\n#\n\nWe can also calculate a zonal mean (averaging over longitude)\n\n#\n\nOther aggregation methods include min(), max(), std(), along with others.\n\n#\n\nThe data we have are in 6h timesteps. This can be resampled to daily or monthly. If you are familiar with pandas, xarray uses the same methods.\n\n#\n\n\n#\n\nThis is a really short time series but as an example, let’s calculate a monthly climatology (at least for 2 months). For this we can use groupby()\n\n#"
  },
  {
    "objectID": "tutorials-templates/03_Xarray.html#plot-results",
    "href": "tutorials-templates/03_Xarray.html#plot-results",
    "title": "03. Introduction to xarray",
    "section": "Plot results",
    "text": "Plot results\nFinally, let’s plot the results! This will plot the lat/lon axes of the original ds DataArray.\n\n#"
  }
]